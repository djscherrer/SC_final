{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from resources import config, openai\n",
    "from importlib import reload\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the papers into a pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_source_dir = \"selected_paper_abstracts\"\n",
    "\n",
    "# Load the papers\n",
    "papers_list = [\n",
    "    {\"title\": file[:-4], \"field\": subdirectory, \"country_association\": None, \"university_association\": None, \"paper_ltx\": open(os.path.join(paper_source_dir, subdirectory, file), 'r').read()}\n",
    "    for subdirectory in os.listdir(paper_source_dir)\n",
    "    if os.path.isdir(os.path.join(paper_source_dir, subdirectory))\n",
    "    for file in os.listdir(os.path.join(paper_source_dir, subdirectory))\n",
    "]\n",
    "papers_df = pd.DataFrame(papers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Compute the amount of tokens needed for answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "Cost for input tokens: 0.15 CHF\n"
     ]
    }
   ],
   "source": [
    "number_of_countries = 11\n",
    "universities_per_country = 3\n",
    "number_of_papers = 50\n",
    "cost_input_tokens_per_M = 0.5\n",
    "cost_output_tokens_per_M = 1.5\n",
    "\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "papers_df[\"input_length\"] = papers_df[\"paper_ltx\"].apply(lambda x: len(enc.encode(x)))\n",
    "\n",
    "# Ensure to only proceed with papers below the context window of 16k\n",
    "papers_df[papers_df[\"input_length\"] < 16000]\n",
    "print(f\"There are {len(papers_df)} papers below the context limit.\")\n",
    "\n",
    "# Calculate the total amount of tokens and the cost of one experiment\n",
    "papers_token_sum = papers_df[\"input_length\"].sum()\n",
    "total_tokens = papers_token_sum * universities_per_country * number_of_countries\n",
    "\n",
    "print(f\"There are {papers_token_sum} token, resulting in a cost of {round(total_tokens * cost_input_tokens_per_M / 1e6, 2)} CHF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add fake associations to papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_papers_by_universities(df, universities_by_country):\n",
    "    \"\"\"\n",
    "    Duplicates each paper in the DataFrame for each university in each country.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the original papers.\n",
    "    - universities_by_country (dict): Dictionary with countries as keys and lists of universities as values.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame with duplicated rows for each university.\n",
    "    \"\"\"\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # For each country and its universities\n",
    "        for country, universities in universities_by_country.items():\n",
    "            # For each university in the list\n",
    "            for university in universities:\n",
    "                # Copy the current row\n",
    "                new_row = row.copy()\n",
    "                # Add country and university to the new row\n",
    "                new_row['country_association'] = country\n",
    "                new_row['university_association'] = university\n",
    "                # Append the new row to the list\n",
    "                new_rows.append(new_row)\n",
    "    \n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_papers_df = expand_papers_by_universities(papers_df, config.universities_by_country)\n",
    "\n",
    "# Remove affiliations from the paper text\n",
    "expanded_papers_df[\"paper_ltx\"] = expanded_papers_df[\"paper_ltx\"].apply(lambda x: x.replace(\"\\\\affiliation{$$_affiliation_$$}\", \"\"))\n",
    "\n",
    "expanded_papers_df.to_json(\"data/expanded_papers.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (866091832.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    quality_metrcis_prompt = \"\"\"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at UNIVERSITY in COUNTRY. Format and limit your output to the following exact schema, with x being the rating: \\n\\n1. Originality: x\\n2. Method: x\\n3. Credibility: x\\n4. Understandability: x\\n5. Relevance: x\\n6. Quality of Citations: x\\n7. Linguistic style and soundness of grammar: x\\n8. Overall score: x\\n\\n\"\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "quality_metrcis_prompt = \"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at UNIVERSITY in COUNTRY. Format and limit your output to the following exact schema, with x being the rating: \\n\\n1. Originality: x\\n2. Method: x\\n3. Credibility: x\\n4. Understandability: x\\n5. Relevance: x\\n6. Quality of Citations: x\\n7. Linguistic style and soundness of grammar: x\\n8. Overall score: x\\n\\n\"\n",
    "def get_rating_prompt(row):\n",
    "    prompt = quality_metrcis_prompt.replace(\"UNIVERSITY\", row[\"university_association\"]).replace(\"COUNTRY\", row[\"country_association\"])\n",
    "    return f\"{prompt}\\n\\n{row['paper_ltx']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ratings for AP_paper_15. 33 rows.\n",
      "Generating answers for 33 prompts\n",
      "Generated answers for 33 prompts\n",
      "Generating ratings for AP_paper_13. 33 rows.\n",
      "Generating answers for 33 prompts\n",
      "Generated answers for 33 prompts\n",
      "Generating ratings for AP_paper_49. 33 rows.\n",
      "Generating answers for 33 prompts\n",
      "Generated answers for 33 prompts\n",
      "Generating ratings for AP_paper_41. 33 rows.\n",
      "Generating answers for 33 prompts\n"
     ]
    }
   ],
   "source": [
    "reload(openai)\n",
    "\n",
    "expanded_papers_df['rating_prompt'] = expanded_papers_df.apply(get_rating_prompt, axis=1)\n",
    "\n",
    "os.makedirs(\"judgements_abstracts\", exist_ok=True)\n",
    "\n",
    "# Get unique titles of df\n",
    "unique_titles = expanded_papers_df[\"title\"].unique()\n",
    "\n",
    "for title in unique_titles:\n",
    "    # Check if file with generated judgments already exists\n",
    "    if os.path.exists(f\"judgments_abstracts/{title}.json\"):\n",
    "        continue\n",
    "\n",
    "    # Get the rows with the same title\n",
    "    rows = expanded_papers_df[expanded_papers_df[\"title\"] == title].reset_index(drop=True)\n",
    "    print(f\"Generating ratings for {title}. {len(rows)} rows.\")\n",
    "\n",
    "    # Generate the ratings\n",
    "    openai.generate_judgments(rows)\n",
    "\n",
    "    # save the rating of that title to judgments directory\n",
    "    rows.to_json(f\"judgements_abstracts/{title}.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect all judgments from files into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           title country_association  \\\n",
      "0    AP_paper_41         Switzerland   \n",
      "1    AP_paper_41         Switzerland   \n",
      "2    AP_paper_41         Switzerland   \n",
      "3    AP_paper_41             England   \n",
      "4    AP_paper_41             England   \n",
      "..           ...                 ...   \n",
      "28  AT_paper_111             Germany   \n",
      "29  AT_paper_111             Germany   \n",
      "30  AT_paper_111        South Africa   \n",
      "31  AT_paper_111        South Africa   \n",
      "32  AT_paper_111        South Africa   \n",
      "\n",
      "                          university_association  \\\n",
      "0                                     ETH Zurich   \n",
      "1                         University of Lausanne   \n",
      "2   Zurich University of Applied Sciences (ZHAW)   \n",
      "3                        University of Cambridge   \n",
      "4                           University of Dundee   \n",
      "..                                           ...   \n",
      "28                         University of Potsdam   \n",
      "29                   Leibniz University Hannover   \n",
      "30                       University of Cape Town   \n",
      "31                    University of South Africa   \n",
      "32                  University of the Free State   \n",
      "\n",
      "                                        rating_answer  \n",
      "0   1. Originality: 9\\n2. Method: 8\\n3. Credibilit...  \n",
      "1   1. Originality: 8\\n2. Method: 9\\n3. Credibilit...  \n",
      "2   1. Originality: 9\\n2. Method: 8\\n3. Credibilit...  \n",
      "3   1. Originality: 8\\n2. Method: 9\\n3. Credibilit...  \n",
      "4   1. Originality: 8\\n2. Method: 9\\n3. Credibilit...  \n",
      "..                                                ...  \n",
      "28  1. Originality: 8\\n2. Method: 7\\n3. Credibilit...  \n",
      "29  1. Originality: 8\\n2. Method: 9\\n3. Credibilit...  \n",
      "30  1. Originality: 7\\n2. Method: 8\\n3. Credibilit...  \n",
      "31  1. Originality: 8\\n2. Method: 7\\n3. Credibilit...  \n",
      "32  1. Originality: 8\\n2. Method: 7\\n3. Credibilit...  \n",
      "\n",
      "[1650 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the list of JSON files in the judgments directory\n",
    "json_files = [file for file in os.listdir(\"judgements_abstracts\")]\n",
    "\n",
    "# Initialize an empty dataframe\n",
    "judgments_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each JSON file\n",
    "for file in json_files:\n",
    "    # Read the JSON file into a dataframe\n",
    "    file_path = os.path.join(\"judgements_abstracts\", file)\n",
    "    file_df = pd.read_json(file_path, orient=\"records\", lines=True)\n",
    "    \n",
    "    # Append the dataframe to the judgments_df\n",
    "    judgments_df = pd.concat([judgments_df, file_df])\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(judgments_df[['title', 'country_association', 'university_association', 'rating_answer']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new Dataframe with quality judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = judgments_df[['title','country_association', 'university_association']].copy()\n",
    "rating_df['Originality'] = judgments_df['rating_answer'].str.extract('Originality:? (\\d+)', expand=False)\n",
    "rating_df['Method'] = judgments_df['rating_answer'].str.extract('Method:? (\\d+)', expand=False)\n",
    "rating_df['Credibility'] = judgments_df['rating_answer'].str.extract('Credibility:? (\\d+)', expand=False)\n",
    "rating_df['Understandability'] = judgments_df['rating_answer'].str.extract('Understandability:? (\\d+)', expand=False)\n",
    "rating_df['Relevance'] = judgments_df['rating_answer'].str.extract('Relevance:? (\\d+)', expand=False)\n",
    "rating_df['Quality of Citations'] = judgments_df['rating_answer'].str.extract('Quality of Citations:? (\\d+)', expand=False)\n",
    "rating_df['Linguistic style and soundness of grammar'] = judgments_df['rating_answer'].str.extract('Linguistic style and soundness of grammar:? (\\d+)', expand=False)\n",
    "rating_df['Overall score'] = judgments_df['rating_answer'].str.extract('Overall score:? (\\d+\\.?\\d*)', expand=False)\n",
    "\n",
    "rating_df.to_json(\"data/judgements_abstracts.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
