\begin{document}
\affiliation{$$_affiliation_$$}
\title{Strategies for Rare Population Detection and Sampling: A Methodological Approach in Liguria}
\maketitle
\begin{abstract}
Economic policy sciences are constantly investigating the quality of well-being of broad sections of the population in order to describe the current interdependence between unequal living conditions, low levels of education and a lack of integration into society.
Such studies are often carried out in the form of surveys, e.g. as part of the EU-SILC program.
If the survey is designed at national or international level, the results of the study are often used as a reference by a broad range of public institutions.
However, the sampling strategy per se may not capture enough information to provide an accurate representation of all population strata.
Problems might arise from rare, or hard-to-sample, populations and the conclusion of the study may be compromised or unrealistic.
We propose here a two-phase methodology to identify rare, poorly sampled populations and then resample the hard-to-sample strata.
We focused our attention on 2019 EU-SILC section concerning
the Italian region of Liguria.
Methods based on dispersion indices or deep learning were used to detect rare populations.
A multi-frame survey was proposed as the sampling design.
The results showed that factors such as citizenship, material deprivation and large families are still fundamental characteristics that are difficult to capture. 
\end{abstract}


\section{Introduction}
As our world becomes increasingly interconnected, the role of poverty and inequality in wealthy societies becomes ever more important.
To capture and monitor the complex intricate evolution of social dynamics, many European countries participate in the \gls{eusilc} program \cite[]{EUSILC:2019}.
The task of this instrument is to examine income and living conditions across the European continent regularly. 
The most important aspects of \gls{eusilc} focus on the links between poverty, well-being and inequality.

About three decades ago, poverty was described as widespread as in the past when prosperity and technology were much less diffused \cite[]{atkinson1998world}.  
Despite the proliferation of new technologies, poverty and inequality persist in European society, and the anticipated positive effects of technological advancement on these socioeconomic issues have not materialized as expected \cite[]{longford2014statistical}
In opposition to this, the role of education in contrasting poverty and inequality has been largely proven so far\cite[]{hofmarcher2021effect, raffo2007education}. 
Among affluent countries, low-level education has correlated with a gap in income with a relevant impact on children's educational outcomes \cite[]{ferguson2007impact}. 

When considering the \gls{eusilc} data to monitor the current living conditions in Europe, it has recently been shown that the \gls{eusilc} microdata could substantially differ in describing the income-related variables among the European countries \cite[]{trindade2020comparability}.  
Further problems regarding the quality of the \gls{eusilc} samples were already reported for the German section \cite[]{hauser2008problems}.
Major problems might also arise when such an open-access source of data is utilized to promote local policies.
On the other hand, given the exponential growth of new emergent methods to analyse ever larger datasets, in the last decades the \gls{eusilc} instrument has been completely revised to provide a more reliable and consistent source of data \cite[]{wirth2022european}.

We propose in this work a novel methodology to investigate the \gls{eusilc} microdata with special attention on rare populations.
The problem of finding hard-to-sample populations is connected with finding those sections of the \gls{eusilc} subjects that are poorly represented within the sampling population.
Speaking more in general, we refer to rare populations as those sampling sub-populations that are widely dispersed among the reference sampling population \cite[]{lohr2021sampling}.
Thus, we addressed the problem under two complementary aspects: proposing a methodology to detect and sample rare population strata.

To illustrate the methodology we utilized the 2019 \gls{eusilc} dataset concerning the Liguria region.
We are motivated to focus on this specific sub-population of the \gls{eusilc} dataset because the government of the Liguria region has been developing a rich plan of assistance services for the strata of residents frail at most, i.e., those that are more exposed to inequality and poverty factors.
This work was therefore aimed at devising an investigation parallel to \gls{eusilc} in order to detect and newly sample the frail strata that are not well-represented in the \gls{eusilc} data. 

Thus, the detection phase, which is the first phase of this methodology, was accomplished through different univariate dispersion indices-based analyses and \gls{ann}-based methods.
The high potentiality revealed by \gls{ann} in deeply elaborating the \gls{eusilc} data was already taken into account \cite[]{veljkovic2023heating, pisati2010mapping}.
When designing the sampling strategy, which is the second phase of our methodology, we opted for the \gls{mfs} \cite{lohr2006multiple, lohr2007recent, lohr2011alternative}.
Given its robustness and the consequent reduction of the under coverage that could occur if only one sampling frame was used \cite{lohr2006multiple}, such a sampling strategy turned out to be the most appropriate for the problem under the exam.

The rest of this work is thus organized as follows: In section \ref{sec: data} we reported a concise description of the 2019 \gls{eusilc} data we utilized throughout the paper.
Section \ref{sec: methodology} was dedicated to describing the three methods we implemented to detect rare populations within the 2019 \gls{eusilc} dataset.
In addition, we also described the methodology based on the \gls{mfs} approach.
The results of our two-phase methodology are then reported in section \ref{sec: results}.
In particular, we ran a simulation study to investigate the effective benefits of using an \gls{mfs}-based approach for reducing survey costs.
Finally, we let a summary discussion in section \ref{sec: discussion}.

\section{Data}\label{sec: data}
In this section, we present the datasets we used to conduct our analysis.
We are interested in the 2019 \gls{eusilc} data for the Ligurian area.
The analysis of these data was combined with the \textit{2019 tax return data}; a complete ensemble of the annual declared income and economic situation of most of the households in Liguria. 
This tax dataset has the scope to provide a good measure of comparison for investigating the critical aspects of the \gls{eusilc} data. This analysis was commissioned by the Bureaux of Financial programing and Statistics of the Liguria Region. 

\subsection{2019 EU-SILC data}\label{sec: data_eusilc}
The~\gls{eusilc} is a European program with the scope of collecting and comparing cross-sectional and longitudinal multidimensional microdata on income, poverty, social exclusion and living conditions~\cite[]{EUSILC:2019}.
The collection of data is scheduled annually and involves households selected from all countries in Europe.
The acquisition of data occurs after submitting a questionnaire to the selected households; this questionnaire helps explore the living conditions and social exclusion in Europe.

We considered the data available for the year 2019; in particular, we brought our attention to the Italian scenario with a special emphasis on the Ligurian region. 
The sub-dataset of the 2019~\gls{eusilc} for Liguria amounts to 707 units.
The~\gls{eusilc} database presents a huge variety of information of different kinds. 
We selected from the questionnaire a group of questions concerning the living conditions and social exclusion of the population in Liguria.
These are 36 variables and all are categorical; a full description of these variables is in Appendix~\ref{apx: additional_info_data}.
In addition, an in-depth investigation of the sampling distribution of these variables helped detect those strata of the Ligurian population which are rare to track.


All variables presented a rate of missing values lower than 10\In combination with the Ligurian dataset, we considered the 2019 Italian~\gls{eusilc} dataset as well.
Likewise the Ligurian case, we extracted the same set of 36 categorical variables.
We collected information for a total amount of 51981 subjects. 
Missing values, found with an incidence lower than 10\
\subsection{2019 tax declaration dataset}\label{sec: data_taxt_decalration}
The 2019 tax collection dataset is a broad collection of data available from the tax declaration in Liguria.
The majority of the information in this collection concern the income and the economic situation of most the households in Liguria, i.e., a total amount of 1.216.747 declarations; information available for 731.537 households.
The Ligurian population is therefore represented through the following economic and demographic features, namely
\begin{itemize}
    \item Sex
    \item Age
    \item Employment
    \item Income
    \item Number of children
    \item Number of dependent disabled children.
\end{itemize}
In Section~\ref{sec: discussion} we compare the reliability of the 2019~\gls{eusilc} dataset for the Liguria reality with this source of administrative data.

\section{Detecting Rare Populations}\label{sec: methodology}
In this section we propose some tools to identify poorly represented or unrepresented sub-populations of \gls{eusilc} data. 
The problem of determining whether a sub-population is under- or un-represented in a given dataset is related to the issue of \emph{representation bias}~\cite[]{shahbazi2022survey}. 
Briefly, representation bias might arise from an intrinsic preexisting problematic nature of the data under exam~\cite[]{mehrabi2021survey}; in many other cases, the source of data from which the occurrences are taken might be lacking in a few of the sub-populations of interest~\cite[]{shahbazi2022survey}.
Another potential source of representation bias is given by the sampling scheme~\cite[]{shahbazi2022survey}.

In the past years, \emph{data coverage} for multicategorical attributes has broadly been considered to quantify representation bias~\cite[]{asudeh2019assessing, asudeh2021identifying, jin2020mithracoverage}.
However, such a metric does not apply to the identification of relevant, unrepresented population strata.
Furthermore, the problem of outlining a definition of rare or hard-to-reach population groups should go beyond the broad statement of "minority groups not involved in survey participation"~\cite[]{brackertz2007hard}. 
The hardest open issue remains the fact that the detection of unrepresented minority groups of data does not represent always a well-defined rare target population. 

Based on these considerations, we propose three methods to detect rare and unrepresented subpopulations within the \gls{eusilc} data.
The central idea is to look for the less informative minority groups in~\gls{eusilc} data by means of anomaly detection techniques~\cite[]{chandola2009anomaly} and entropy-based indices.

\subsection{Entropy-based indices}\label{sec: Entropy-Based indices}

For entropy-based measures, we expected that 
the variables with the lowest entropy were likely to have a high prevalence in a few classes; i.e., where we could identify a rare population of interest.
Accordingly, we examine one by one the categorical variables that are far from showing a uniform occurrence in their classes.
In other words, we focus our attention on the categorical variables that are not informative in terms of entropy. 

We recall here a few properties of Shannon entropy.
Given the generic probability mass function $P$, defined over the sample space $\Omega$, we define the \emph{Shannon Entropy} (or simply entropy) as 
\begin{equation}\label{eq: Entropy_discrete}
    H[P] = -\sum_{x\in \Omega} P(x) \log{P(x)}.
\end{equation}
By construction, $H[P] \ge 0$ and is a concave functional~\cite[]{bishop2006pattern}.  
Values of $H[P]$ close (or equal to zero) represent the case of a discrete distribution with most of (or all) the probability mass over one class.
In contrast, the maximum entropy is reached when all classes are equally probable; in this case, the entropy reaches the value $\log{N}$ with $N$ the total number of categories of $P$. 
The entropy $H[P]$ is often proposed as a measure of the overall \emph{informativeness} of a source of random events following the distribution $P$.

This peculiarity of the entropy functional makes it suitable for identifying all those categories of the sampling population that are poorly represented or rare.
In particular, due to the property of being a direct measure of the informativeness of a source of events, entropy was proposed in the last decades as an automated selection method of rare sample~\cite{daneshpazhouh2014entropy, berezinski2015entropy}.

However, using Shannon entropy alone can only help to identify a type of subjects rarely interviewed; 
i.e., outliers and interesting rare populations, both of which present an intrinsic difficulty to be interviewed.
Simply put, entropy can help uncover either some subjects who cannot be included in a particular category of the sampling population or the rare strata of the population that we want to detect.

To overcome the issue that arises from utilizing the sole entropy, we have opted for an additional entropy-based measure such as the \gls{kld} (we will also refer to it as \emph{relative entropy}).
Given two discrete distributions $P$ and $Q$ defined over the same space $\Omega$, the relative entropy (of $P$ with respect to $Q$) $D[P||Q]$ is
\begin{equation}\label{eq: KL_divergence_discrete}
    D[P||Q] = \sum_{x\in \Omega} P(x) \log{\frac{P(x)}{Q(x)}}.
\end{equation}

Relative entropy is proposed in many statistical learning contexts to quantify the \emph{distance between two distributions} \cite[]{bishop2006pattern}. 
For example, it is often employed as a loss function in many classification and regression tasks.
Similar to the entropy, the relative entropy is non-negative and concave \cite[]{bishop2006pattern}. 
Values close to zero express a small discrepancy between the analysed distributions while diverging positive values show that both distributions are far from being similar or equal.
In the context of univariate variable selection, we utilized relative entropy in combination with the Shannon entropy.
Specifically, we utilized relative entropy to compare the similarity between the categorical variables extracted from two different EU-SILC 2019 datasets, such as the dataset of the Ligurian region and the dataset of Italy.
To avoid redundancy, we removed the data of the Liguria dataset in the one of Italy. 
A high relative entropy of a variable, combined with a low Shannon entropy, suggests that the variable may represent a category that is rare or difficult to sample in Liguria. 
This peculiarity diverges from the situation represented in the rest of the country.
In fact, a low Shannon entropy denotes that one or more categories are undercovered, while a high relative entropy suggests a divergence between Liguria and Italy (except Liguria).
From the analysis of the relative entropy values, we could therefore detect the populations poorly represented in the Ligurian EU-SILC dataset and in the Italian scenario.

The \gls{kld} has the drawback of being asymmetric, i.e., $D[P || Q] \neq D[Q || P]$.
The Jensen-Shannon divergence is symmetric version of the relative entropy\gls{jsd}~\cite[]{nielsen2021variational} and is defined as 
\begin{equation}\label{eq: JS_divergence}
    JS[P||Q] = \frac{D[P||M]+D[Q||M]}{2}. 
\end{equation} 
with $M = \displaystyle \frac{P+Q}{2}$.
The Jensen-Shannon divergence is bounded by both sides. 
As a lower bound it has the null value that represents the case where both $P$ and $Q$ are equal.
The upper bound is the value $\log_{2}{N}$, with $N$ the number of categories of both $P$ and $Q$.

Another entropy-based dispersion measure is defined as $h = \sum_{j \in \Omega} P^{2}(x) $ and it holds $-\log{h} = H_{2}(X)$ with $H_{2}$ the \emph{R\`envi entropy} of order 2 (a.k.a \emph{collision entropy}). 
The R\`envi entropy \cite[]{renyi1961measures} for a generic order $\alpha \in \mathrm{R}^{+}$ is given as
\begin{equation}\label{eq: Renvi_Entropy}
    H_{\alpha} = \lim_{\gamma \to \alpha} \frac{1}{1-\gamma}\log{\left(\sum_{x\in\Omega} P^{\gamma}(x)\right)}.
\end{equation}
Note also $H_{1}$ coincides with the Shannon Entropy defined in~\eqref{eq: Entropy_discrete}.
The sum of squares of $P$ is also bounded; lower and upper bounds are respectively $1/N$ and $1$; with $N$ the number of categories of $P$.
This makes $h$ suitable to be employed as an index over the informativeness and dispersion of $P$. 

To summarise, we considered three entropy-based indices to describe the information content and redundancy of each variable.
Shannon entropy and collision entropy have to do with the dispersion of occurrences in the categories that make up the variable.
Conversely, the \gls{jsd} helps show a contrast in the informativeness of some variables between the Ligurian region and the rest of Italy.
All these indices are bounded, and we normalized them to reach values in the range $[0, 1]$.
For simplicity, we have considered complementary quantities, such as the \emph{complementary Shannon entropy} $1-H$ and the \emph{complementary collision entropy} $1-H_{2}.$ 
For each of the 36 observed variables in the Ligurian region, say $P$, and its equivalent in Italy (excluding Liguria), say $Q$, we consider the point $ent=(1-H[P], JS[P, Q], 1-H_2 )$ in $[0,1]^3$. 
A point close to $(1, 1, 1)$ will be associated with a poorly represented variable, a point close to $(0, 0, 0)$ the opposite.

The evaluation of the entropy-based indices themselves does not provide any information on whether a single variable has some poorly sampled categories or not.
Therefore, we considered an unsupervised classification problem to propose an automatic procedure to better discriminate those variables that might have some poorly sampled categories.
We opted for a \emph{K-means} clustering with two clusters. 
Avoiding complex analyses, we were interested in finding a net discrimination to distinguish the poorly represented categories from those that do not exhibit this problem.
We opted to proceed in this manner for pragmatic reasons.
First, this method combines the distances between different variables, expressed as the Euclidean distances between the three entropy-based indices. 
Secondly, the interpretability of the results comes from the fact of using interpretable entropy-based indices.
Simply put, the cluster of variables whose values of $ent$ are closer to $(1,1,1)$ represents that cluster of variables poorly represented in some of their categories.

To evaluate the goodness of the K-means clustering we relied on the \emph{Silhuhette score} metric \cite[]{rousseeuw1987silhouettes}.
We readapted the silhouette score for the scenario of searching for two clusters $\Gamma_{0}$ (i.e., rare or poorly represented variables) and $\Gamma_{1}$ (i.e., variables that do not suffer from undercoverage).
Let us consider the generic $i$-th variable among the 36 aforementioned.
As introduced, the point $ent_{i} = (1-H[P_i],JS[P_i,Q_i], 1-H_2[P_i] )$ describe the informativeness of the $i$-th variable, with $P_i$ and $Q_i$ the distributions of Liguria and Italy, respectively. 
To illustrate the method, we shall assume that $e_i$ is associated with the cluster $\Gamma_{0}$.
Thus, we define the \emph{cohesion coefficient}, namely
\begin{equation}
    \notag
    \Xi_{i} = \frac{\sum_{ent_{k} \in \Gamma_0, ent_{k} \neq ent_{i}}d({ent}_k, {ent}_i)}{|\Gamma_0|-1}, 
\end{equation}
with $d({ent}_k, {ent}_i)$ denote the distance between the instances ${ent}_k$ and ${ent}_i$ (e.g., the Euclidean distance); $|\Gamma_0|$ denotes the size of $\Gamma_0$, i.e., the total number of items it comprises.
Also, we take into account the \emph{separability coefficient} of the $i$-th variable, namely  
\begin{equation}
    \notag
    \Omega_{i} = \frac{\sum_{{ent}_k \in \Gamma_j, {ent}_k \neq {ent}_i}d({ent}_k, {ent}_i)}{|\Gamma_1|}.
\end{equation}
Combining the two coefficients, we get the  Silhouette Coefficient for the $i$-th variable, namely
\begin{equation}
    \label{def::Silhouette_Coefficient}
    \Sigma_{i} = \frac{\Xi_i-\Omega_i}{\max{(\Xi_i, \Omega_i)}}.
\end{equation}
In essence, one considers the discrepancy between the mean distance of the $i$-th instance concerning all other instances labelled in the same cluster (i.e., cohesion coefficient) and the mean distance between the $i$-th instance with respect to the elements of its closest cluster (i.e., the separability coefficient).


The values attained by $\Sigma_{i}$ lay on the range [-1, 1]; values close to unity mean a net separation between the clusters; the opposite is represented by negative values.
Values about 0 mean an overlapping of different clusters.
To give an overall measure we utilized the jackknife cross-validation \cite[]{efron1981jackknife} to estimate the mean silhouette score and its 95\
\subsection{Higher-order central moments-based indices}\label{sec: Higher-order central moments-based indices}
We propose here another univariate analysis to detect variables with some misrepresentation in the categories of the probability mass function.
More specifically, we adapt the central moments of the distributions of these variables to provide some indices denoting the presence of an equal amount of occurrences among all the categories in question.
Given the probability mass function $P$ over the sample space $\Omega$, the $k$-th central moment $\mu_{k}$ of $P$ is defined as 
\begin{equation}\label{eq: central_moment_definition}
    \mu_{k} = \sum_{x \in \Omega} P(x)(x-\mathbb{E}(x))^{k}.
\end{equation}
It holds $\mu_0 = 1$ and $\mu_{1} = 0$ for all $P$; this implies that higher-order moments (say, $k\ge2$) need to be taken into account.
Moreover, the central moments of orders 2, 3, 4, 5, and 6 have a direct connection with some tendency of the distribution $P$, such as the deviation from the mean, the skewness about the mean value, the skewness and the fluctuation of outliers and extreme values about the mean value \cite[]{kamath2021explainable}.
Using central moments for anomaly detection has been suggested recently \cite[]{renganathan2021higher}, considering their property describes how data are spread out.
The use of central moments has recently been proposed in the context of anomaly detection \cite[]{renganathan2021higher}; the formulation of such a methodology was focused on time-discrete linear systems that undergo noise not necessarily Gaussian and possibly heavy-tailed.

Thus, starting from \eqref{eq: central_moment_definition}, we can construct the desired indices from the ratio
\begin{equation}\label{eq: central_moments_index_raw_version}
    \frac{\Tilde{\mu_{k}}}{\sum_{x \in \Omega} P^{2}(x)\sum_{x\in\Omega}(x-\mathbb{E}(x))^{2k}};
\end{equation}
with $\Tilde{\mu_{k}}$ the unbiased estimator of the $k$-th order central moment.
The heuristic of this approach is to consider \eqref{eq: central_moment_definition} as the scalar product; then we consider the cosine similarity between $P$ and $(x-\mathbb{E}(x))$ to produce the desired index.
For odd orders, we took \eqref{eq: central_moments_index_raw_version} as it is, since values close to unity indicate a skewness or an asymmetry of tails about the central value.
In contrast, for even orders, values close to unity tend to represent a well-spread mass of probability.
Hence, we can define the index of $k$-th order as
\begin{equation}
\gamma_{k} =
\begin{cases}
    1-\frac{\Tilde{\mu_{k}}}{\sum_{x \in \Omega} P^{2}(x)\sum_{x\in\Omega}(x-\mathbb{E}(x))^{2k}}~\text{if }k=2, 4, 6;\\
    \frac{|\Tilde{\mu_{k}}|}{\sum_{x \in \Omega} P^{2}(x)\sum_{x\in\Omega}(x-\mathbb{E}(x))^{2k}}~\text{if }k=3, 5.\\
\end{cases}
    \end{equation}

To identify which variables show some misrepresentation in their categories we reutilize the same approach proposed in section \ref{sec: Entropy-Based indices}.
That is, we consider an unsupervised classification problem to propose an automatic procedure based on the central moment-based indices to distinguish variables that suffer from undercoverage from the others.
Hence, we again opted for a \emph{K-means} clustering with 2 clusters; the quality of the obtained clustering was evaluated through the silhouette score.

\subsection{Auto-Encoder Reconstruction Error-based index}\label{sec: Auto-Encoder Reconstruction Error-based index}
So far we have considered univariate approaches.
Both methods of sections \ref{sec: Entropy-Based indices} and \ref{sec: Higher-order central moments-based indices} are based on the univariate extraction of features concerning the dispersion of a univariate discrete probability distribution.
In contrast to this kind of approach, \gls{ann} have been proposed in recent years as a successful strategy to address anomaly detection in a multivariate setting \cite[]{an2015variational, zhou2017anomaly}.
In particular, we focused on \gls{vae}; a type of \gls{ae} broadly employed in anomaly detection problems.
Generally speaking, the idea behind the \gls{ae} is to learn simultaneously two distinct transformations, an \emph{encoder} $\mathcal{E}$ and a \emph{decoder} $\mathcal{D}$.
The encoder has the scope to give the input data an efficient representation in a low-dimensional space; 
while the decoder attempts to reconstruct the input data from the encoding provided by the encoder.
Usually, the decoder is placed just underneath the decoder and returns the output reconstruction.
In other words, if we denote with $\mathcal{X}$ and $\mathcal{Z}$ the space of the input and encoded data, we can define the encoder as $\mathcal{X}:\mathcal{E} \to \mathcal{Z}$ and the decoder as $\mathcal{Z}:\mathcal{D} \to \mathcal{X}$.
Accordingly, we can (informally) sketch it as $\mathcal{X}:\mathcal{E} \to \mathcal{Z} : \mathcal{D} \to \mathcal{X}$

In many applications for anomaly detection, \gls{ae} are usually utilized to learn all the characteristics of one determined class of data.
Unlike standard \gls{ae} algorithms, \gls{vae} has the peculiarity of constraining the structure of the hidden units with a specific probabilistic structure.
Usually, each output of the encoder is drawn from a Normal distribution whose parameters are learned from the input data; see figure \ref{fig: scheme_vae}.  
Such a structure leads to better generalization during the learning phase; the input data are not mapped into a single point but in a stochastic region of a reduced dimensionality space.
As a result, small changes in the encoder output cannot affect largely the reconstruction through the decoder.
Since this algorithm is devised to learn the interconnections among the features of training data, we expect that the propagation of a similar instance through this model will be encoded and then reconstructed with a very minimal loss.
Conversely, when propagating an instance with different features from the ones utilized in the learning phase, the model will no longer be able to reconstruct the input data efficiently.
When the last scenario occurs, one identifies that instance as an outlier or an anomalous instance; more in general, as a rare item that significantly differs from the majority of the training data. 
\begin{figure}[!]
    \centering
\includegraphics[width=.8\textwidth]{imgs/VAE_example_1_.png}
    \caption{Schematic illustration of a \gls{vae}}
    \label{fig: scheme_vae}
\end{figure}

Thus, we implemented a \gls{vae} to detect the set of categorical variables showing some misrepresentation in their categories.
This model was implemented with the following encoder structure:
\begin{enumerate}
    \item The input data are propagated through a single dense layer with  32 outputs \emph{softplus} activation function (i.e., $f(x) = \log{(1+\exp{x})}$.)

    \item The 32-dimension description of input data is then propagated through two distinct dense layers with 16 output units Rational Activation Function \cite[]{boulle2020rational} (i.e., a parametric rational function whose parameters are learnt during the training phase).
    The first of the two dense layers provides the mean, while the second one the logarithm of the standard deviation, of the normally sampled outputs of the decoder.

    \item Once each couple of parameters is evaluated, the output of each node of the encoder is drawn from the corresponding normal distribution. 
\end{enumerate}
For the decoder, we used this architecture:
\begin{enumerate}
    \item The encoder's output is sent to the dense layers with 32 output nodes and a softplus activation function.

    \item A further dense layer with a number of output nodes equal to the features of the input data is then placed underneath. The activation function is still a softplus function.
    Implementing such a number of outputs is necessary to ensure the input data reconstruction.
\end{enumerate}

The reconstruction was accomplished after minimizing the \emph{mean square error} loss function; that is, denoted with $X_1, \dots X_N$ and $\Tilde{X}_1 \dots \Tilde{X}_N$ the input data and their reconstruction, respectively, we have
\begin{equation}
    \Lambda = \frac{1}{N} \sum_{i = 1}^{N} (X_{i}-\Tilde{X_{i}})^{2}.
\end{equation}
To optimise the loss function use the \emph{ADAM} algorithm \cite[]{kingma2014adam}.

The validation of this model was mainly focused on the ability of the \gls{vae} to provide a good reconstruction of input data.
To do so, we exploited the fact that the input data are categorical variables. 
Thus, for the $i$-th instance, we constructed the following metrics
\begin{equation}
    \tau_0(X_{i}, \Tilde{X}_{i}) = 
    \begin{cases}
        1 ~\text{if~} X_{i}-\lfloor{\Tilde{X}_{i}}\rfloor = 0\\

        0 ~\text{if~} X_{i}-\lfloor{\Tilde{X}_{i}}\rfloor \neq 0\\
    \end{cases}
\end{equation}

\begin{equation}
    \tau_1(X, \Tilde{X}) = \frac{1}{N}\sum_{i = 1}^{N} \tau_0(X_{i}, \Tilde{X}_{i}).
\end{equation}
where $\lfloor{a}\rfloor$ is the largest integer number smaller than $a$. 
We then used the \emph{binomial test} to control whether at least 90\Thus, we claimed the null hypothesis $H_0 : \tau_1 = 0.9$ with the alternative hypothesis $H_1 : \tau_1 < 0.9$.
This approach was combined with a $K$-fold cross-validation; thus, we repeated the binomial test for each one of the $K$ folds.

During the learning phase, we utilized $K-1$ of the $K$ folds, while the leftover was used to test the reconstruction ability of the model.
To be sure that each of the validation folds does not significantly differ (in statistical terms) from the training ones, we evaluated the \gls{kld} between each couple of folds.
Taking the total number of folds we reported
the mean and the 95\The estimation of these quantities was accomplished through the jackknife resampling method.

To investigate which categorical variables were the most misrepresented through the \gls{vae}, we took inspiration from the approach described in \cite{torabi2023practical}.
That is, we set a vectorised threshold based on the reconstruction error with the scope of pointing out anomalous instances.

More specifically, after training the model, the validation set was investigated to search for anomalous items.
Thus, we propagated the training items through the model and for each one of them, we evaluated the \emph{absolute reconstruction error} per each feature.
Given the generic $i$-th item, the absolute reconstruction error is 
\begin{equation}
    T_0(X_i, \Tilde{X_i}) = \left(|X^{(1)}_{i}-\Tilde{X}^{(1)}_{i}|,\dots, |X^{(N)}_{i}-\Tilde{X}^{(N)}_{i}| \right).
\end{equation}
Note that $T_0$ is a vector with dimension equal to the dimension of item $X_{i}$.
With regard to the data in our possession, the generic instance $X_i$ is indeed an array of length 36 (since the aforementioned variables are 36 in total).
By construction, $\Tilde{X}_i$ is also an array with length 36. 
It follows that $T_0$ has length 36 as well. 
We denoted with $T_0^{j}(X_i, \Tilde{X_i})$ the absolute reconstruction error relative to the $j$-th feature of the $i$-th item.
Next, we constructed the desired vectorized threshold; the generic $j$-th component was given by
\begin{equation}
    \theta^{(j)} = \max_{x \in \{X_1, \dots X_{N}\}} \{T_0^{(j)}(x, \Tilde{x})\}.
\end{equation}

Thus, when propagating one item of the validation set $Y$ throughout the model, if the absolute reconstruction $T_0(Y, \Tilde{Y})$ met $T_{0}^{(j)}(Y, \Tilde{Y}) > \theta^{(j)}$ for any $j$-th component, the item $Y$ was then flagged as anomalous.
This procedure allowed us to detect which items needed more attention.

Finally, to identify which feature is more responsible for the anomalous feature of these outliers, we resorted to the \emph{Feature Importance} algorithm \cite[]{breiman2001random}.
The basic idea of this method is to break the correlations among the input variables to observe the resilience of the model in being able to make good predictions.
During the validation phase, the variables of the validation data are shuffled, one at a time. 
The data with one shuffled variable are then propagated through the model; the importance of that shuffle feature, that is the prevalence of the anomalous items, is finally collected.
The reshuffle of each variable was performed 30 times.
The mean importance of each variable and the corresponding 95\

\subsection{An illustrative example with the Iris Dataset}

The indices above are tested on the Iris Dataset \cite[]{fisher1936use}, which consists of 150 instances of three types of irises (i.e., Setosa, Versicolour, and Virginica). For each instance, four features are reported: sepal length, petal length, sepal width, and petal width.
The Setosa type differs from the others because of its short and tight petals.
Based on this fact, we made a stratified sampling with a special focus on the Setosa type.
Regarding the data sampling, we intentionally introduced an imbalance, particularly in sampling petal features. 
More specifically, we sampled a total of 80 samples; 40 samples from the Setosa population and 20 from each one of the remaining populations.
A visualization of the pair plot of the sampled population is shown in Figure \ref{fig:toy_example_marginals}.

A summary with the results for all three methods is shown in Table \ref{tab: summary_table_indices_toy_example}.
All methods showed that the sampled data revealed a shortage of information with the feature \emph{Petal width}.
In particular, the Entropy-based index and the \gls{vae} revealed a lack in both the petal characteristics, as expected.
In opposition to this, the Moment-based index method indicated the \emph{Sepal length} as a feature poorly represented together with the \emph{Petal length} and \emph{Petal width}.
We observed that this group is characterized by high-scored indices $\gamma_2$, $\gamma_4$, $\gamma_6$, and low-scored indices $\gamma_3$, $\gamma_5$ (see Table \ref{tab: summary_table_indices_toy_example}), denoting the presence of an important sharp peak.
This can be explained through a comparison with Figure \ref{fig:toy_example_marginals}; the method detected the shaped highly-tailed mass of Setosa items across all the three features under the exam.
For completeness, we reported in Table \ref{tab: execution_times} the execution times for all three methods.
As expected, the \gls{vae}-based approach requires a larger amount of time compared to the others.
This is due to the large
number of parameters of \gls{vae} that need to be optimized.

\begin{figure}
    \centering
    \includegraphics[width= \textwidth]{imgs/iris_pirplot_sampled.pdf}
    \caption{Pair plot for all the four features. Along the diagonal the Kernel Density Estimation, stratified per class, per each of the four features}
    \label{fig:toy_example_marginals}
\end{figure}
\begin{table}
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Method} & \textbf{Duration (sec.)} \\
    \hline
     Entropy-based indexes    &  1.01\\
     Central Moment-based indexes    & 1.04\\
     VAE Reconstruction Error-based index & 40.33\\
     \hline
    \end{tabular}
    \caption{Execution times of all methods when applied to the Iris Dataset}
    \label{tab: execution_times}
\end{table}
\begin{footnotesize}
\begin{longtable}{|l|l|l|l|r|r|r|r|r|r|r|r|r|}
    \hline
    \textbf{Variable}& \textbf{Entropy} & \textbf{Central} & \textbf{AE Rec} &     $1-H_1$ & JS & $1-H_2$ &  $\gamma_2$ &  $\gamma_3$ &  $\gamma_4$ &  $\gamma_5$ &  $\gamma_6$ &  AE Rec  \\
     & \textbf{Cluster} & \textbf{Moment} & \textbf{Cluster} & & &  & &  & &  &  & Impor \\
     &  & \textbf{Cluster} & & & &  & &  & &  &  & tance \\
    \hline
    Sepal length &          Common &                   Rare &           Common &  0.11 &  0.02 &  0.21 &    0.97 &    0.01 &    0.99 &    0.01 &    0.99 &           0.28 \\
    Sepal width &          Common &                   Rare &           Common &  0.07&  0.02&  0.12&  0.97 &   -0.01 &    0.99 &  -0.01 &    0.99 &           0.19 \\
    Petal length &            Rare &                 Common &           Rare &  0.29 &  0.04 &  0.43 &    0.66 &    0.05&    0.84 &    0.06 &    0.91 &           0.48\\
    Petal width &            Rare &                   Rare &           Rare &  0.23 &  0.03 &  0.37 &    0.97&    0.01&    0.99&    0.01&    0.99&           0.37 \\
    \hline
    \caption{Summary Table with the indices and results for the toy example. Results for all three methods were reported (Entropy-based indices, Central moment-based indices, and VAE reconstruction error-based indices). 
    The first column shows the name of each categorical variable. Three columns show whether a variable was considered \emph{rare} (i.e., there might exist some rare samples in it) or \emph{common} (i.e., the opposite). The leftovers show the value of the indices of the three methods.
    }
    \label{tab: summary_table_indices_toy_example}
\end{longtable}
\end{footnotesize}

\section{Sampling Rare Populations}

We shall present here in detail the sampling strategy we developed. 
To illustrate our work, we shall first give a motivation for the benefits of using the \gls{mfs} in sampling rare populations.
After that, we will focus on applying this sampling strategy through a simulation study.

The Multi Frame Scheme (\gls{mfs}) is a sampling technique pioneered by \cite{hartley1962sampling}.
Its basic scope consists of aggregating, without duplicates, various incomplete sampling frames; each one containing a high prevalence of the rare population of interest, in order to obtain a larger sampling framework including all the rare populations we would like to investigate.
When formulating an estimator of the expected value of the response variable $Y$, the following requirements should be met along the complete sampling framework \cite[]{lohr2011alternative}
\begin{itemize}
    \item The estimator must be unbiased.

    \item The estimator must be \emph{internally consistent}. 
    That is, if we consider only two incomplete frames $A$ and $B$, the relation $Y = Y_a + Y_{ab} + Y_{b}$ must be met.
    We denoted with $Y_a$, $Y_{ab}$, $Y_{b}$ represent the number of subjects, without considering intersections, in $A$, $B$, and $A\cap B.$ 
    By omitting duplicates, the variable of response $Y$ must be conserved along all the incomplete frames. 

    \item The estimator should be efficient, with low variance

    \item The estimator should be as robust as possible to control possible non-sampling errors from multiple frame surveys.
\end{itemize}

For the two-frame case, the method proposed by \cite{hartley1962sampling} was based on the reweighting of the repose variable \cite{skinner1996estimation}.
The optimal choice of the reweighted parameters was exploited to ask the efficiency of the estimator.
Such an approach, denoted as \emph{optimum} approach, is based on the optimization of the estimator variance and has been largely studied to obtain an estimator in the two-frame case \cite{hartley1962sampling, fuller1972estimators, bankier1986estimators}. 
A theoretical and general solution in the case involving an arbitrary number of frames (larger than 2) has already been proposed by \cite{lohr2006multiple}.
Although the optimal approach gives the estimator some desired theoretical properties, it might often lead to an unstable solution when involving large sampling frameworks.
In such a case, the method proposed by \cite{mecatti2007single} overcomes the issue brought by the optimal approach.
As in the optimization approach, this method accomplishes a reweighing of the response variable; in this case, the weights are fixed and specific for each subject.
Thus, the response variable of each subject is multiplied by the inverse of its multiplicity along the incomplete frames, i.e., the inverse of the number of frames that the subject belongs to.
In the work of \cite{mecatti2007single} an unbiased and consistent estimator for the population mean is given.
However, the efficiency is not guaranteed.
In addition, such an approach does not provide any indication about the sampling size of the survey.

To bypass this issue, we have proposed here a method based on a constrained minimization problem.
The idea consists of finding the number of subjects per frame minimizing the variance estimator (i.e., making the estimator the most efficient one) given the constraint on the total number of subjects to be included in the survey.
Since the optimal solutions cannot be expressed in a close form, we searched for them through a computational approach. 
Briefly, we took under consideration the unbiased \gls{mfs} variance estimator, namely
\begin{equation}
    \hat{v}_{mfs} = \sum_{q \in Q} \left(\frac{N_q}{n_q}-1\right) \frac{N_q}{n_q -1}\left[N_q\sum_{i\in \mathcal{S}_{q}} u_{i}^{2} -\frac{N_q}{n_q}\left(\sum_{i\in \mathcal{S}_{q}} u_{i}\right)^{2}\right], 
\end{equation}
with $N_{q}$ the total population of the $q$-th incomplete frame, $n_{q}$ the sample size of the $q$-th incomplete frame, and $m_{i}$ the multiplicity of the $i$-th subject.
Thus, we aimed to minimize the Lagrangian
\begin{equation}
    \mathcal{L}[\pi_{1}, \dots, \pi_{n}, \lambda] = \hat{v}_{mfs} + \lambda \left(\sum_{j \in \mathcal{Q}}\pi_{j} \nu_{j}-n \right);
\end{equation}
with $\pi_{1}, \dots, \pi_{n}$ the proportion of each population $N_{1}, \dots, N_{n}$ to allocate in each corresponding incomplete frame of $\mathcal{Q} = \{\mathcal{Q}_1 \dots \mathcal{Q}_n\}$; $\nu_{j} = \sum_{i = 1}^{N_{n}}\frac{1}{m_{i}}$.
$\lambda$ is a the Lagrange multiplier.

As a response variable, we utilized the \gls{ehi}.
Such a variable is widely used in the \gls{eusilc} context to describe the income situation of a household \cite[]{longford2014statistical}.
The \gls{ehi} can be evaluated by the formula
\begin{equation}
    \operatorname{EHI} = \frac{\omega}{1+0.5\psi+0.3\chi};
\end{equation}
with $\omega$ the total income of the household, $\psi$ the number of components of the household older than 18, and $\chi$ the number of children within the household older than 16 (and younger than 18). 
For more details see appendix \ref{apx: dettagli_matematici}.

To test our sampling scheme, we utilized data randomly generated from the rare populations investigated through the methods shown in sections \ref{sec: Entropy-Based indices}, \ref{sec: Higher-order central moments-based indices}, and \ref{sec: Auto-Encoder Reconstruction Error-based index}.
The data generation was accomplished through a Gaussian mixture model, with the optimal number of normal distributions sought after asking for the lowest Akaike information \cite[]{akaike1976canonical}.

We compared the results of our approach with a \gls{srs}-based survey; that is we repeated the sampling procedure for the \gls{srs} approach and then we compared the results.
We referenced the \emph{Design Effect}, or simply Deff.
Denoting with $\hat{v}_{mfs}$ and $\hat{v}_{srs}$, respectively, the estimator variance of both the \gls{mfs} and \gls{srs} approaches; the Deff is defined as
$$\Delta = \frac{\hat{v}_{mfs}}{\hat{v}_{srs}}.$$
The less the Deff, the more is the efficiency of \gls{mfs} with respect to the \gls{srs}.

\section{Results}\label{sec: results}
In this section, we shall present all the results obtained in the two steps of the analysis.
First, we shall show the rare populations that our methodology suggested.
Next, we shall present a simulation of the \gls{mfs}-based population sampling by using the EU-SILC 2019 data.

\subsection{Phase one: detection of rare populations}\label{sec: detecting_rare_populations}

A pairwise plot of all the entropy-based features is shown in figure \ref{fig: Entropy_Pairplot}.
In that figure, the distinction between \emph{rare} and \emph{common} (i.e., well-represented) variables is also reported.
For the sake of clarity, we meant here with the term \emph{rare} a variable whose categories might not be well represented in the sampling population under the exam; the term \emph{common} refers to the vice-versa.
\begin{figure}[!]
    \centering
    \includegraphics[width= .8\textwidth]{imgs/Cluster_Entropy_pairplot_cropped.pdf}
    \caption{Pairwise plot of all entropy-based indices. Each plot shows also the two-cluster classification provided by the K-mean algorithm; in blue the \emph{rare} items while in orange the others. The diagonal plots depict the Kernel Density Estimation for both classes.}
    \label{fig: Entropy_Pairplot}
\end{figure}
When validating the K-mean clustering model, we obtained an overall Silhouette coefficient of 0.66; the 95\
When considering the central moment-based indices, we followed the same procedure as above.
In figure \ref{fig: Central_Moment_Pairplot}, the pairwise plots of all indices are shown.
The plots show also the clustering of variables in rare and common variables.
The evaluation of the clustering guarantees an overall silhouette score of 0.52; the 95\\begin{figure}[!]
    \centering
    \includegraphics[width= \textwidth]{imgs/Central_Moment_Index_Cluster_pairplot_cropped.pdf}
    \caption{Pairwise plot of all central moment-based indices. Each plot shows also the two-cluster classification provided by the K-mean algorithm; in blue the \emph{rare} items while in orange the others. The diagonal plots depict the Kernel Density Estimation for both classes.}
    \label{fig: Central_Moment_Pairplot}
\end{figure}

The results for the \gls{vae}-based analysis are shown in figures \ref{fig: KL-divergence_validation} and \ref{fig: VAE_mean_importance}.
We examined plenty of different choices of folds, from two up to ten included, to validate the \gls{vae}.
We observed that using a 10-fold validation reduced at best the risk of letting the model be trained and tested on different sets of data; see figure \ref{fig: KL-divergence_validation}.
The results from the binomial test, for each one of the ten training phases, are shown in table \ref{tab: vae_binomial_tests}.
The analysis of the variables' importance helped reveal the set of variables mostly misrepresented; we took all variables whose mean importance has an absolute error larger than 100\We then identified variables as \textit{CITTADX} (whether the interviewed has Italian citizenship), \textit{FERIE} (whether the interviewed can spend a week of vacation abroad or far from home), \textit{ITALIA} (whether the intertwined was born in Italy), \textit{RAGA015} and \textit{RAGA117} (the number of children in the range of age, respectively, 0-15 and 1-17 who live in the household), \textit{SEV\_MAT\_DEPRIV} (whether the interviewed is exposed to severe material deprivation); see figure \ref{fig: VAE_mean_importance}.


In table \ref{tab: summary_table_indices}, a summary of all the results produced for each one of the three methods is reported.
In that table, we highlighted the populations indicated as the ones to include in the sampling stage.
Hence, the variables we selected are \textit{CITTADX}, \textit{ITALIA}, \textit{RAGA015} and \textit{RAGA117}, and \textit{SEV\_MAT\_DEPRIV}.
The rare populations are obtained after looking at the less frequent categories of these variables; the histograms of these variables are shown in figure \ref{fig: Hist_rare_catgs}.
In the end, the rare populations we took under consideration are 
\begin{itemize}
    \item Households who do not have the Italian citizenship
    \item Households whose members were not born in Italy
    \item Households with 3 or more children in the age range either [1, 15] or [1, 17].
    \item Households subjected to severe material deprivation.
\end{itemize}
\begin{table}[!]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
         \textbf{N. of fold} & \textbf{Statistic} ($\boldsymbol{\hat{p}}$) &  \textbf{p-value}\\
         \hline
         1 & 0.96 & 0.99\\
         \hline
         2 & 0.97 & 1.0\\
         \hline
         3 & 0.98 & 0.99\\
         \hline
         4 & 0.98 & 0.99\\
         \hline
         5 & 0.98 & 0.99\\
         \hline
         6 & 0.97 & 0.99\\
         \hline
         7 & 0.97 & 0.99\\
         \hline
         8 & 0.98 & 0.99\\
         \hline
         9 & 0.97 & 0.99\\
         \hline
         10 & 0.97 & 0.99\\
            \hline
    \end{tabular}
    \caption{Results of the binomial test to validate the reconstruction ability of the \gls{vae}. The first column reports the number of folds which was used as a validation test. The statistic is the test statistic, i.e., the prevalence of correctly reconstructed items. The p-values are shown in the last column. We recall the null hypothesis is $\hat{p} = 0.9$, versus the alternative hypothesis $\hat{p} < 0.9$.}
    \label{tab: vae_binomial_tests}
\end{table}
\begin{figure}[!]
    \centering
    \includegraphics[width= 0.8\textwidth]{imgs/KL_per_fold.pdf}
    \caption{Mean value of KL divergence per each pair of data folds. Data were split using a K-fold approach. The number of folds is shown on the x-axis. When appreciable, the 95\    \label{fig: KL-divergence_validation}
\end{figure}
\begin{figure}[!]
    \centering
    \includegraphics[width= 0.8 \textwidth]{imgs/VAE_based_Importance.pdf}
    \caption{Mean Importance per each variable. The light-coloured area denotes the 95\    The dashed black line denotes the mean importance when no shuffling is applied to any one of the variables.}
    \label{fig: VAE_mean_importance}
\end{figure}
\begin{tiny}
\begin{longtable}{|l|l|l|l|r|r|r|r|r|r|r|r|r|}
    \hline
   \textbf{Variable}& \textbf{Entropy} & \textbf{Central} & \textbf{AE Rec} &     $1-H_1$ & JS & $1-H_2$ &  $\gamma_2$ &  $\gamma_3$ &  $\gamma_4$ &  $\gamma_5$ &  $\gamma_6$ &  AE Rec  \\
     & \textbf{Cluster} & \textbf{Moment} & \textbf{Cluster} & & &  & &  & &  &  & Impor \\
     &  & \textbf{Cluster} & & & &  & &  & &  &  & tance \\
    \hline
    ACCORD &  Rare &                         Rare &         Common &          0.876 &   0.001 &                    0.951 &                  0.018 &                  0.983 &                  0.017 &                  0.983 &                  0.017 &              0.019 \\
    ARRETR &            Rare &                         Rare &         Common &          0.722 &   0.001 &                    0.861 &                  0.053 &                  0.950 &                  0.050 &                  0.950 &                  0.050 &              0.013 \\
    BAGNI2\_B &          Common &                       Common &         Common &          0.417 &   0.033 &                    0.481 &                  0.596 &                  0.673 &                  0.458 &                  0.601 &                  0.426 &              0.018 \\
    BORST &            Rare &                         Rare &         Common &          0.985 &   0.000 &                    0.996 &                  0.001 &                  0.999 &                  0.001 &                  0.999 &                  0.001 &              0.016 \\
    CAMBCOND &            Rare &                         Rare &         Common &          0.836 &   0.000 &                    0.931 &                  0.025 &                  0.975 &                  0.025 &                  0.975 &                  0.025 &              0.014 \\
    CAMB\_CASA &            Rare &                         Rare &         Common &          0.829 &   0.001 &                    0.927 &                  0.027 &                  0.974 &                  0.026 &                  0.974 &                  0.026 &              0.016 \\
    CASSAINT &            Rare &                         Rare &         Common &          0.591 &   0.001 &                    0.764 &                  0.097 &                  0.912 &                  0.089 &                  0.911 &                  0.089 &              0.014 \\
    CIBO &            Rare &                         Rare &         Common &          0.728 &   0.001 &                    0.866 &                  0.051 &                  0.951 &                  0.049 &                  0.951 &                  0.049 &              0.021 \\
    \rowcolor{Kol2023}
    CITTADX &            Rare &                         Rare &           Rare &          0.800 &   0.002 &                    0.910 &                  0.033 &                  0.968 &                  0.032 &                  0.968 &                  0.032 &              0.123 \\
    CONTRIB &            Rare &                         Rare &         Common &          0.920 &   0.000 &                    0.971 &                  0.010 &                  0.990 &                  0.010 &                  0.990 &                  0.010 &              0.019 \\
    CONTRPU &            Rare &                         Rare &         Common &          0.874 &   0.002 &                    0.950 &                  0.018 &                  0.982 &                  0.018 &                  0.982 &                  0.018 &              0.017 \\
    CORREG &            Rare &                         Rare &         Common &          0.960 &   0.000 &                    0.988 &                  0.004 &                  0.996 &                  0.004 &                  0.996 &                  0.004 &              0.016 \\
    CRIME &            Rare &                         Rare &         Common &          0.642 &   0.000 &                    0.805 &                  0.078 &                  0.928 &                  0.073 &                  0.927 &                  0.073 &              0.020 \\
    DIFCIB &            Rare &                         Rare &         Common &          0.818 &   0.002 &                    0.923 &                  0.030 &                  0.975 &                  0.022 &                  0.980 &                  0.019 &              0.024 \\
    DIFFRED &          Common &                         Rare &         Common &          0.335 &   0.001 &                    0.483 &                  0.151 &                  0.924 &                  0.074 &                  0.933 &                  0.065 &              0.024 \\
    DMEZZO &            Rare &                         Rare &         Common &          0.755 &   0.001 &                    0.883 &                  0.044 &                  0.958 &                  0.042 &                  0.958 &                  0.042 &              0.019 \\
    DOM &          Common &                       Common &         Common &          0.110 &   0.097 &                    0.206 &                  0.662 &                  0.884 &                  0.451 &                  0.810 &                  0.338 &              0.018 \\
    DON1555 &          Common &                         Rare &         Common &          0.433 &   0.003 &                    0.529 &                  0.064 &                  0.980 &                  0.015 &                  0.989 &                  0.009 &              0.021 \\
    FERIE &          Common &                       Common &           Rare &          0.197 &   0.010 &                    0.334 &                  0.406 &                  0.724 &                  0.319 &                  0.695 &                  0.309 &              0.037 \\
    FTPT &          Common &                       Common &         Common &          0.364 &   0.002 &                    0.546 &                  0.224 &                  0.819 &                  0.189 &                  0.812 &                  0.188 &              0.019 \\
    IMPREV &          Common &                       Common &         Common &          0.146 &   0.000 &                    0.258 &                  0.494 &                  0.694 &                  0.381 &                  0.648 &                  0.363 &              0.026 \\
    \rowcolor{Kol2023}
    ITALIA &            Rare &                         Rare &           Rare &          0.698 &   0.002 &                    0.845 &                  0.060 &                  0.943 &                  0.057 &                  0.943 &                  0.057 &              0.110 \\
    LIQUID &            Rare &                         Rare &         Common &          0.675 &   0.001 &                    0.829 &                  0.067 &                  0.937 &                  0.063 &                  0.937 &                  0.063 &              0.018 \\
    MALAT &            Rare &                         Rare &         Common &          0.618 &   0.000 &                    0.795 &                  0.086 &                  0.985 &                  0.084 &                  0.990 &                  0.082 &              0.016 \\
    NOCERCO\_I &            Rare &                         Rare &         Common &          0.582 &   0.905 &                    0.778 &                  0.071 &                  0.955 &                  0.051 &                  0.956 &                  0.045 &              0.020 \\
    PAGAFF &          Common &                       Common &         Common &          0.337 &   0.012 &                    0.515 &                  0.246 &                  0.805 &                  0.206 &                  0.796 &                  0.204 &              0.017 \\
    PAGAPP &          Common &                       Common &         Common &          0.139 &   0.010 &                    0.188 &                  0.359 &                  0.845 &                  0.242 &                  0.804 &                  0.216 &              0.023 \\
    PAGBOL &            Rare &                         Rare &         Common &          0.692 &   0.000 &                    0.841 &                  0.062 &                  0.942 &                  0.058 &                  0.942 &                  0.058 &              0.022 \\
    PASTO &            Rare &                         Rare &         Common &          0.631 &   0.000 &                    0.797 &                  0.082 &                  0.925 &                  0.076 &                  0.924 &                  0.076 &              0.015 \\
    PESTERO &            Rare &                         Rare &         Common &          0.939 &   0.000 &                    0.980 &                  0.007 &                  0.993 &                  0.007 &                  0.993 &                  0.007 &              0.016 \\
    \rowcolor{Kol2023}
    RAGA017 &            Rare &                         Rare &           Rare &          0.630 &   0.000 &                    0.786 &                  0.028 &                  0.987 &                  0.008 &                  0.995 &                  0.004 &              0.203 \\
    \rowcolor{Kol2023}
    RAGA115 &            Rare &                         Rare &           Rare &          0.667 &   0.001 &                    0.811 &                  0.023 &                  0.990 &                  0.006 &                  0.996 &                  0.003 &              0.203 \\
    RICMAN &            Rare &                         Rare &         Common &          0.876 &   0.000 &                    0.951 &                  0.018 &                  0.983 &                  0.017 &                  0.983 &                  0.017 &              0.016 \\
    \rowcolor{Kol2023}
    SEV\_MAT\_DEPRIV &            Rare &                         Rare &         Rare &          0.747 &   0.000 &                    0.878 &                  0.046 &                  0.956 &                  0.044 &                  0.956 &                  0.044 &              0.031 \\
    SMOG &          Common &                         Rare &         Common &          0.435 &   0.000 &                    0.622 &                  0.175 &                  0.852 &                  0.152 &                  0.849 &                  0.152 &              0.015 \\
    SPESCUOL &          Common &                         Rare &         Common &          0.384 &   0.003 &                    0.460 &                  0.117 &                  0.967 &                  0.026 &                  0.983 &                  0.015 &              0.018 \\
    \hline
    \caption{Summary Table with the results for all three methods (Entropy-based indices, Central moment-based indices, and VAE reconstruction error-based indices). 
    The first column shows the name of each categorical variable. Three columns report the detection of the \emph{rare} populations (according to the three methods). The leftovers show the value of the indices used in the three methods.
    The highlighted rows denote a rare population for all three methods.
    }
    \label{tab: summary_table_indices}
\end{longtable}
\end{tiny}
\begin{figure}[!]
    \centering
    \includegraphics[width= \textwidth]{imgs/Rare_pops_categs_hor.pdf}
    \caption{Histograms for the rare categorical variables (as indicated in table \ref{tab: summary_table_indices})}
    \label{fig: Hist_rare_catgs}
\end{figure}

\subsection{Phase two: detecting rare populations}\label{sec: sampling_rare_populations}
The analysis of section \ref{sec: detecting_rare_populations} brought to light five different rare sub-populations within the 2019 \gls{eusilc} dataset.
We designed a simulation study to analyse the benefits of opting for a \gls{mfs} to investigate these sub-populations.
We generated the characteristics of 1500000 subjects through a Gaussian Mixture model.
The Gaussian Mixture model was fitted on the 2019 \gls{eusilc} data.
According to the values of these characteristics, a single subject might be allocated to none, one or more than one rare sub-population.
In case none of the rare sub-populations is represented, the subjects are said to belong to the \emph{ground population}.
Note that the choice of generating 1500000 items was determined by aiming to accurately represent the Ligurian population during the year 2019 \cite[]{ISTAT:pop_liguria_2019}.
In figure \ref{fig: AIC_ggm} the \gls{aic} as a function of the number of normal distributions involved in the Gaussian Mixture model is shown; a single-Gaussian model proved most efficient for generating the synthetic population.

In figure \ref{fig: Deff} the values of the Design effect are shown for different orders of magnitude of the sampling population.
The \gls{mfs} approach became more accurate as larger populations were included.
With a sampling size of 100 (or 1000), the \gls{mfs} can be about $10^{5}$ (or $10^{7}$) times more efficient than a single (complete) frame-based survey with \gls{srs} sampling scheme. 

When focusing on the sample size per each incomplete frame, we can observe a clear difference in how both sampling methods include subjects in each subframe.
For large sample sizes ($>> 10^{3}$), the \gls{mfs} tend to assign subjects keeping almost fixed some proportions between the sub-populations; see figure \ref{fig: optimal_pi_MFS}.
Lower sample sizes are characterised by a similar proportion through all the incomplete frames, i.e. almost $10^{-2}.$
In opposition to this, the \gls{srs} turned out to be more focused on sampling the rare sub-population giving much less importance to the ground population; see figure \ref{fig: optimal_pi_SRS}.

\begin{figure}[!]
    \centering
    \includegraphics[width= .8\textwidth]{imgs/AIC_Simulated_Data.pdf}
    \caption{\gls{aic} as a function of the number of clusters in the Gaussian Mixture density model.}
    \label{fig: AIC_ggm}
\end{figure}
\begin{figure}[!]
    \centering
    \includegraphics[width= 0.8\textwidth]{imgs/Mecatti_DEFF_LigDat.pdf}
    \caption{Design effect as a function of the constrained population. Design effect between the schemes \gls{mfs} and \gls{srs}. 
    On the x-axis, the population size and on the y-axis the values of the design effect}
    \label{fig: Deff}
\end{figure}
\begin{figure}[!]
    \centering
    \includegraphics[width= .9\textwidth]{imgs/Mecatti_constrained_optimal_props_MFS_EUSILC_GGMIXgen.pdf}
    \caption{Optimal proportions for the \gls{mfs} variance estimator as a function of constrained population size. Error bars represent the 95\    \label{fig: optimal_pi_MFS}
\end{figure}
\begin{figure}[!]
    \centering
    \includegraphics[width= .9\textwidth]{imgs/Mecatti_constrained_optimal_props_SRS_EUSILC_GGMIXgen.pdf}
    \caption{Optimal proportions for the \gls{srs} variance estimator as a function of constrained population size. Error bars represent the 95\    \label{fig: optimal_pi_SRS}
\end{figure}

\subsection{Comparision with 2019 tax declaration data}
To provide further validation of our methodology, we compared the rare subpopulations identified using the aforementioned methods with the subpopulations of the \emph{}{2019 tax declaration dataset}. (see \ref{sec: data_taxt_decalration}).
The goal was to illustrate that the undercoverage of hard-to-reach subpopulations revealed by our methodology was also present when inspecting a comprehensive administrative data source. Particularly, we put the accent on searching for statistical differences in the sub-populations concerning the number of children (age less than 17) in households.
Note that such a subpopulation is the only one in which faithful data are available from both datasets (Ligurian section of 2019 \gls{eusilc} dataset and the 2019 tax declaration dataset).

To reveal significant differences, we utilized the \emph{2-sample Kolomogorov-Smirnov} test with a 5\Denoted with $\mathcal{F}_1$ and $\mathcal{F}_2$ the distribution of the number of children in the households for the two datasets, we tested the null hypothesis $H_0: \mathcal{F}_1 = \mathcal{F}_2$ versus the alternative $H_1: \mathcal{F}_1 \neq \mathcal{F}_2$.
The test's result showed that the null hypothesis had to be rejected (p-value of $7\cdot10^{-4}$).
The graphical analysis showed that the \gls{eusilc} data have scarcely represented all the households with a very large number of children (say, $>5$).
As a result, the 2019 \gls{eusilc} data of region Liguria appeared to be more focused on households with less than 3 children; see figure \ref{fig: Children_comparision}
\begin{figure}[!]
    \centering
    \includegraphics[width= .8\textwidth]{imgs/Children_comparision.pdf}
    \caption{Historograms for the number of children in households in Liguria. Blue: 2019 EU-SILC data, Red: Tax-declaration}
    \label{fig: Children_comparision}
\end{figure}

\section{Discussion and conclusion}\label{sec: discussion}

Our work was committed to inquiring about the representativeness of \gls{eusilc} data, for the Ligurian region in Italy, proposing a methodology to reveal hard-to-find sub-populations within the \gls{eusilc} dataset together with a sampling scheme to acquire \emph{ab novo} the information concerning the investigated sub-populations.

A comparison between the entropy and the central moment-based indexes revealed that the former was robust compared to the central moment-based one.
Both methods identified similar hard-to-sample variables, but the Silhouette score comparison and graphical inspection favoured the entropy measure-based approach for detecting potential misrepresentations among \gls{eusilc} variables.
Compared to the \gls{vae}-based approach, most variables were labelled as not rare, and the multivariate-based approach aligned with others only for a limited set of variables.

About the sampling design analysis, the \gls{mfs}  approach turned out to be more valid than \gls{srs}-based unique frame strategy.
The ability of \gls{mfs} to be more efficient than a standard \gls{srs} approach is due to a wiser selection of rare subjects to interview.
Consequently, the \gls{mfs} suggests sampling a much lower number of rare subjects without a loss in the representability of the target population. 
The comparison with administrative data, such as the tax-declaration dataset, revealed that the \gls{eusilc} dataset does not represent adequately rare strata of the sampled population, such as the numerous families.
Such a result sheds some light on the weak faithfulness of the \gls{eusilc} data when considering such a source of data to promote some policies for specific hard-to-reach strata of the population of interest.


It is important to stress, however, that the first phase of our methodology (i.e., the research of rare classes) was conducted on the sole \gls{eusilc} data and in the absence of any other similar source of administrative data that could represent ground truth.
This posed a challenge to developing our methodology; however, the comparison with the tax-declaration dataset showed a proof of concept regarding its practical potential.



Limitations in this study are certainly present; however, we remember we have just limited our goal to present a methodology that could delve into the application of advanced studies.
We tried to combine and compare different approaches, such as deep learning and traditional dispersion indices, to ameliorate the precision of rare population detection.
Additionally, longitudinal studies may provide a deeper understanding of the dynamics and evolution of rare populations over time.
In conclusion, this paper lays the groundwork for a nuanced (and rigorous) approach to detecting misrepresented rare populations within population datasets.
We investigated the issue of rare populations within the Ligurian area, offering a valuable contribution to the broader field of population research. 
As we continue to refine and expand our methodologies, we move closer to a more comprehensive and inclusive understanding of the diverse peculiarities of rare samples that are hard to include in large surveys.


\begin{appendices}
\noappendicestocpagenum
\addappheadtotoc

\section{Supplementary information on data}\label{apx: additional_info_data}

\begin{longtable}{ | m{4cm} | m{12cm}|}
    \hline
    \textbf{Name Variable} &  \textbf{Description}\\
    \hline
ACCORD &                Whether the subject has a written employment contract, oral employment agreement, or none of both\\ 
\hline
    ARRETR &                  Whether the subject has received arrears from employment\\
    \hline
    BAGNI2\_B &               Whether the dwelling has two or more toilets\\
    \hline
    BORST &                   Whether the sbject has received a scholarship\\
    \hline
    CAMBCOND &           Whether the subject changed profession during the year of reference (2019) \\
    \hline
    CAMB\_CASA &                 Whether the subject changed dweliing during the year of reference (2019) \\
    \hline
    CASSAINT &                 Whether the subject has enrolled in a reduncy program\\
    \hline
    CIBO &                  Whether the subject could not afford to get the food nedded during the year of reference (2019) \\
    \hline
    CITTADX &                 Whether the subject jas Italain citizenship \\
    \hline
    CONTRIB &                 Whether the subject has received public contributions in cash to cover all or part of the costs of housing \\
    \hline
    CONTRPU &                 Whether the subject has received public contributions in cash to cover all or part of the rental costs \\
    \hline
    CORREG &                 Whether the subject is actually enrolled in a training course organized and/or recognized by the Region, of duration equal to or greater than 6 months (or 600 hours) and which grants a professional qualification \\
    \hline
    CRIME &                 Whether the area where the subject currently lives  presents problems of crime, violence, and vandalism \\
    \hline
    DIFCIB &                 Whether the subject resorted to the help of someone who gave food, clothing or other essential goods to the family during the year of reference (2019)  \\
    \hline
    DIFFRED &                 Whether the income of the subject in 2019 was increased, decreased, or unchanged compared to the year 2018\\
    \hline
    DMEZZO &                 Whether the subject’s employer provided him/her a car, even for personal reasons \\
    \hline
    DOM &                 The population of the town hall where the subject lives: less than 5000, between 5000 and 10000, between 10000 and 20000, between 20000 and 50000, more than 50000 \\
    \hline
    DON1555 &                 Number of women with age in range 15-55 in the subject’s household. \\
    \hline
    FERIE &                 Whther the subject could afforf a vacation far from his/her dwelling \\
    \hline
    FTPT &                 Whether the subject has a full-time or part-time contract \\
    \hline
    IMPREV &              Whetehr the subbject would be able to cope, with own resources, with unforeseen expenditure of approximately EUR 800   \\
    \hline
    ITALIA &                 Whether the subject was born in Italy \\
    \hline
    LIQUID &                 whether the subject has received one or more severance payments \\
    \hline
    MALAT &                 Whether the subject expirienced times or periods when he/she did not have the money to pay for the expenses for diseases \\
    \hline
    NOCERCO\_I &                 The reason (when applicable) why the subject did not find a job in the last 4 weeks \\
    \hline
    PAGAFF &                 Whether the subject expirienced times or periods when he/she did not have the money to pay renting costs\\
    \hline
    PAGAPP &                 Whether the subject has utilized some apps to pay bills or transfer money \\
    \hline
    PAGBOL &                 Whether the subject expirienced times or periods when he/she could not pay the bills (e.g., gas, eletricity, water consumtion, and so on) \\
    \hline
    PASTO &                 Whther the subject could eat, meat, fish, or an equivalent vegetrain dish, at least once per two days \\
    \hline
    PESTERO &                 Whther the subject has recived one or more pensions from abroad  \\
    \hline
    RAGA017 &                 The number of children with age in the range 0-17 \\
    \hline
    RAGA115 &                 The number of children with age in the range 1-15 \\
    \hline
    RICMAN &                 Whether the subject has received regular maintenance or maintenance payments from the ex-spouse/ex-partner or the separated/divorced parent who is not a co-owner \\
    \hline
    SEV\_MAT\_DEPRIV &                 Whther the subject has been expirencing a severe material deprivation \\
    \hline
    SMOG &                 Whether the subject lives in an area presenting pollution, dirt or other environmental problems caused by traffic or industrial activities \\
    \hline
    SPESCUOL &               Whether the subject experienced times or periods when he/she could not  pay school fees, such as school books, school or university taxes \\
    \hline
    \caption{List with the description of the 36 categorical variables selected for the analysis.}
    \label{tab: descitpion eu-silc_data}
\end{longtable}

\section{Mathematical details on the multi-frame approach}\label{apx: dettagli_matematici}

\subsection{Population Evaluation}\label{apx: population_evaluation}

Let us consider a set of incomplete frames $\mathcal{R} = \{R_{1} \dots R_{n}\}$ and a the population $\mathcal{P} = \bigcup_{k= 1}^{n} R_k.$
Since the frames are incomplete, each one of them contains subjects sharing the same unique characteristics.
To evaluate the total population we can utilize the formula
\begin{equation}\label{eq: popolazione_mfs}
    t = \sum_{r \in \mathcal{R}}\sum_{k \in r} \frac{y_{k}}{m_{k}};
\end{equation}
with $y_{k}$ a binary response variable and $m_{k}$ its corresponding  \emph{multiplicity}.
With the term multiplicity, we refer to the number of times that a sample is included in one or more of the incomplete frames $R_{1} \dots R_{n}$.
Note that the incomplete frames might be somehow overlapped. 
Consequently, the use of \eqref{eq: popolazione_mfs} to count the total items of $\mathcal{P}$ takes into account the eventuality that one subject might be included in one or more incomplete frames.  

Dealing with the multiplicity can turn out to be unpractical.
Thus, we can consider the \emph{rescaled response variable} (i.e., ${u}_{k} = \frac{y_{k}}{m_{k}}.$).
When evaluating the total population, we can still resort to \eqref{eq: popolazione_mfs}.
However, we can notice that we are now legitimised to consider the incomplete frames as independent of each other.
For convenience, we shall denotes these independent sets with $Q_{1} \dots Q_{n}$ with $\mathcal{Q} = \{Q_{1}, \dots, Q_{n}\}$.
Thus,  we have $\cup_{k=1}^{n}{R_k} = \cup_{k=1}^{n}{Q_k}$, with ${Q_i}\cap{Q_j} = \emptyset ~\forall i\neq j$.
We  rewrite the \eqref{eq: popolazione_mfs} as
\begin{equation}\label{eq: popolazione_mfs_srs}
    t = \sum_{q \in \mathcal{Q}} \sum_{k \in q} u_{k}.
\end{equation}

Hence, when considering a muti-frame framework we can simplify the approach to population estimation by resorting to this sort of equivalence due to a repositioning of the response variable.
The mitigation of the overlapping over the incomplete frames allows us to study them separately.
We shall discuss it more in detail in the next sections.

\subsection{Population estimator}
Let us consider the set of incomplete frames $\mathcal{R} = \{R_{1} \dots R_{n}\}$ and a the population $\mathcal{P} = \bigcup_{k= 1}^{n} R_k.$
Our goal is the estimate of the total population adopting a \gls{mfs} scheme.

We can use the argumentation of section \ref{apx: population_evaluation} to consider the incomplete frames as independent.
As a result, adopting the notation of section \ref{apx: population_evaluation}, the \gls{mfs} scheme over the frames of $\mathcal{R} = \{R_{1} \dots R_{n}\}$ can be regarded as a \gls{srs} over the frames of $\mathcal{Q} = \{Q_{1} \dots Q_{n}\}.$

To get the population estimation we thus start from a \gls{srs}; the 
population estimator is given as
$$ \hat{t} = \sum_{q \in \mathcal{Q}}\sum_{k \in \mathcal{S}_q} u_{k}$$
with $\mathcal{S}_q$ a sample over the $q$-th frame.
We evaluate the expectation of $\hat{t}$, so we have
$$ \mathbf{E}\left(\sum_{q \in \mathcal{Q}}\sum_{k \in q} u_{k}\zeta^{(q)}_{k}\right) = \sum_{q \in \mathcal{Q}}\sum_{k \in q} u_{k} \mathbf{E}\left(\zeta^{(q)}_{k}\right) = \sum_{q \in \mathcal{Q}}\sum_{k \in q} u_{k}\pi^{(q)}_{k};$$ with $\zeta^{(q)}_{k}$ a random dichotomic variable (with fix probability $\pi^{(q)}_{k}$ which is the same per each sample of each frame).
As a result, the unbiased estimator is obtained after reweighting 
$u_{k}$ with the weight $\frac{1}{\pi^{(q)}_{k}}$, namely
\begin{equation}\label{eq:stimatore_popolazione_mfs}
    \hat{t}_{unbiased} = \sum_{q \in \mathcal{Q}}\sum_{k \in \mathcal{S}_q} \frac{u_{k}}{\pi^{(q)}_{k}}.
\end{equation} 
When all subjects are equally likely to be acquired into $\mathcal{S}_{q}$, the weight $1/\pi^{(q)}_{k}$ is determined by the ratio $\frac{N_{q}}{n_{q}}$, with $n_{q}$ the number of subjects sampled and $N_{q}$ the total number of elements within the $q$-th frame.

\subsection{Variance estimator}
Let us determine the variance estimator of \eqref{eq:stimatore_popolazione_mfs}.
Thus, we write down
$$\mathbf{V}(\hat{t}_{unbiased}) = \mathbf{V}\left(\sum_{q \in Q}\sum_{k \in \mathcal{S}_q} \frac{u_{k}}{\pi^{(q)}_{k}}\right).$$
Since the frames of $\mathcal{Q}$ are independent, we write down
$$\mathbf{V}(\hat{t}) = \sum_{q \in Q}\left[ \sum_{k \in q}\mathbf{V}\left( \frac{u_{k} \zeta^{(q)}_{k}}{\pi^{(q)}_{k}}\right) + \sum_{i \in q; j \in q} (1-\delta_{ij})\mathbf{Cov}\left(\frac{u_{i}}{\pi^{(q)}_{i}} \zeta^{(q)}_{i}; \frac{u_{j}}{\pi^{(q)}_{j}}\zeta^{(q)}_{j}\right) \right],$$
with $\delta_{ij}$ denoting the Kronecker's delta.

We recall that
$$\mathbf{V}(\zeta^{(q)}_{i}) = \frac{n_q}{N_q}\left(1-\frac{n_q}{N_q}\right);$$
$$\mathbf{Cov}(\zeta^{(q)}_{i}; \zeta^{(q)}_{j}) = -\frac{1}{N_q-1}\left(1-\frac{n_{q}}{N_{q}}\right)\left(\frac{n_q}{N_q}\right)$$
and 
$$\sum_{i\in q; j \in q} (1-\delta_{ij})u_{i}u_{j} = \left(\sum_{k \in q} u_{k} \right)^{2} - \sum_{k \in q} u^{2}_{k}.$$

By putting them all together, under the assumption that $\pi^{(q)}_{i} = \frac{n_q}{N_q}, \forall i\in Q_{q}$, we obtain the variance estimator
\begin{equation}\label{eq:variance_mfs_estimator}
    \mathbf{V}(\hat{t}) = \sum_{q\in Q}\frac{1}{N_q-1}\left(\frac{N_q}{n_q}-1\right) \left[N_q \sum_{k \in q} u^{2}_{k} -  \left(\sum_{k \in q} u_{k} \right)^{2}\right]. 
\end{equation}

Alternatively, we can write the last equation as 
$$\mathbf{V}(\hat{t}) = \sum_{q\in Q} N_{q}\left(\frac{N_q}{n_q}-1 \right)S_{q}^{2};$$
with 
$$S_{q}^{2} = \frac{1}{N_q -1}\sum_{k \in q}\left( u_k - \frac{1}{N_q} \sum_{l \in q} u_l \right)^{2} =  \frac{1}{N_q(N_q-1)}\left(N_{q} \sum_{k \in q} u^{2}_{k}-\left(\sum_{k \in q}u_k\right)^{2}\right).$$

To obtain the unbiased variance estimator, we can make first the following observation that in a \gls{srs}
is valid $\mathbf{E}(s_{q}^{2}) = S_{q}^{2}$; with 
\begin{equation}\notag
     s_{q}^{2} = \frac{1}{n_q -1}\left[N_q\sum_{i\in \mathcal{S}_{q}} u_{i}^{2} -\frac{N_q}{n_q}\left(\sum_{i\in \mathcal{S}_{q}} u_{i}\right)^{2}\right].
\end{equation}

Thus, the unbiased variance estimator is
\begin{equation}\notag
    \hat{v} = \sum_{q \in Q} N_q\left(\frac{N_q}{n_q}\right) s_{q}^{2};
\end{equation}
or equivalently
\begin{equation}\label{eq:variance_estimator_apx}
    \hat{v} = \sum_{q \in Q} \left(\frac{N_q}{n_q}-1\right) \frac{N_q}{n_q -1}\left[N_q\sum_{i\in \mathcal{S}_{q}} u_{i}^{2} -\frac{N_q}{n_q}\left(\sum_{i\in \mathcal{S}_{q}} u_{i}\right)^{2}\right].
\end{equation}
To make evident the dependence on $\pi^{(q)}$, we can rearrange the last equation as
\begin{equation}\label{eq:variance_estimator_apx}
    \hat{v} = \sum_{q \in Q} N_{q}\left(\frac{1-\pi^{(q)}}{\pi^{(q)}}\right) \frac{1}{N_{q}\pi^{(q)} -1}\left[N_q\sum_{i\in \mathcal{S}_{q}} u_{i}^{2} -\frac{1}{\pi^{(q)}}\left(\sum_{i\in \mathcal{S}_{q}} u_{i}\right)^{2}\right].
\end{equation}

\subsection{Sample size estimation} \label{sec:vincolo_popolazione}
To estimate the sample size we opted for solving an optimal problem.
We required the estimator \eqref{eq:variance_estimator_apx} to be minimal, given the constraint that the sample size must be equal to some fixed $n$.
For convenience, unlike the previous sections, we shall change the notation $\pi^{(q)}$ with $\pi_{q}.$ 

The Lagrangian for such a problem is the following:
\begin{equation}
    \mathcal{L}[\pi_{1}, \dots, \pi_{n}, \lambda] = \hat{v}_{mfs} + \lambda \left(\sum_{j \in \mathcal{Q}}\pi_{j} \nu_{j}-n \right);
\end{equation}
with $\nu_{j} = \sum_{i = 1}^{N_{j}}\frac{1}{m_{i}}$; $m_i$ the multiplicity of the $i$-th element with respect to the frames  $\mathcal{Q}_1 \dots \mathcal{Q}_n$.
$\lambda$ is a Lagrange multiplier.
$\nu_{j}$ describe the \emph{effective population} per frame, i.e., it holds
$\sum_{j \in \mathcal{Q}} \nu_{j} = N_{q}.$ 
Thus, we aim to find the optimal values of $\pi_1, \dots, \pi_n$ leading to the minimal variance estimator, given $n$ the sample size.

To find the stationary solution of the Lagrangian, we impose
\begin{equation}\label{eq:grad_lagrangian_pi}
\frac{\partial \mathcal{L}}{\partial \pi_{q}} = \frac{\partial \hat{v}_{mfs}}{\partial \pi_{q}} + \lambda \nu_q = 0
\end{equation}
\begin{equation}\label{eq:grad_lagrangian_multiplier}
    \frac{\partial \mathcal{L}}{\partial \lambda} = \sum_{j\in\mathcal{Q}_j}\pi_j \nu_j -n = 0.
\end{equation}

For convenience, we rewrite \eqref{eq:variance_estimator_apx} as 
$$\hat{v}_{mfs} = \sum_{q \in \mathcal{Q}} \Phi_q(\pi_q)\Psi_q(\pi_q).$$ 
With
$$\Phi(\pi_q)=  \frac{N_q}{\pi_{q}}\frac{(1-\pi_{q})}{N_{q}\pi_{q}-1}$$
and $$\Psi(\pi_q)= \pi_{q}N_q\sum_{i\in \mathcal{S}_{q}} u_{i}^{2} -\left(\sum_{i\in q} u_{i}\right)^{2}.$$
Accordingly, we formulate the gradient with respect to $\pi$ as
$$\partial_{\pi_q}\mathcal{L} = \partial_{\pi_q}\Phi_{q} \left[\pi_q A_q - B_q\right] + \Phi_q A_q +\lambda \nu_q.$$
with $$A_q = N_q\sum_{i\in \mathcal{S}_{q}} u_{i}^{2}$$
and
$$B_q = \left(\sum_{i\in q} u_{i}\right)^{2}.$$

We would like to solve \eqref{eq:grad_lagrangian_pi}; we try rewriting that equation as 
$$\partial_{\pi_q} (A_q \Phi_q \pi_q - \Phi_q B_q) = -\lambda\nu_q;$$
we integer both sides and get
$$A_q \Phi_q \pi_q - \Phi_q B_q \rvert_{\pi_q}^{1} = -\lambda \nu_q(1-\pi_q).$$
With $\pi_q \in [1/N_q, 1];$ we recall that $\Phi(1) = 0.$ 
When putting it all together we have
\begin{equation}\label{eq:sol_grad_pi}
    \Phi(\pi_q) = \frac{\lambda \nu_q (\pi_q-1)}{B_q-A_q\pi_q}.
\end{equation}

We attempt to make the form of the stationary solution more explicit, so we write 
\begin{equation*}
    \frac{N_q}{\pi_q}\frac{(1-\pi_{q})}{N_{q}\pi_{q}-1} = \frac{\lambda \nu_q (\pi_q-1)}{B_q-A_q\pi_q}
\end{equation*}
or equivalently,  
\begin{equation*}
    \frac{N_q}{\pi_q}\frac{1}{N_{q}\pi_{q}-1} = \frac{\lambda \nu_q }{A_q\pi_q-B_q}.
\end{equation*}


Thus, we obtain the optimal solution
\begin{equation}\label{eq: optimal_solution_pi}
    \pi_{q} = \frac{\left(\frac{A_q N_q}{\lambda \nu_q} -1 \right) + \sqrt{\left(\frac{A_q N_q}{\lambda \nu_q} -1 \right)^{2}+4\frac{B_q N_{q}}{\lambda \nu_q}}}{2 N_q}
\end{equation}

However, the optimal solution  \eqref{eq: optimal_solution_pi} is still dependent on $\lambda$.
To determine $\lambda$ we need to solve
\begin{equation}\label{eq:grad_lagrangian_multiplier}
    \sum_{j \in \mathcal{Q}_{j}} \frac{\left(A_{j} N_{j} -\nu_{j} \right) + \sqrt{\left(A_{j} N_{j} -\nu_{j} \right)^{2}+4 B_{j} N^{2}_{j} \nu_{j} \lambda}}{2 N_{j} \nu_{j} \lambda} = n.
\end{equation}

The solution of \eqref{eq:grad_lagrangian_multiplier} together with \eqref{eq: optimal_solution_pi} provide the desired optimal solution.
As shown, it is not possible to find a closed-form solution.



\end{appendices}

\section*{Acknowledgements}

The authors acknowledge the Bureaux of Financial Programming and the Statistic Study of Regione Liguria for the financial and technical support provided during the elaboration of this work. 
This research was supported in part by the MIUR Excellence Department Project was awarded to Dipartimento di Matematica, Università degli Studi di Genova, CUP D33C23001110001.

\section*{Code Avaliability}
Python code available at \url{https://github.com/glancia93/-Population-Detection-and-Sampling-in-Liguria/tree/main}

\bibliographystyle{apalike}  
\bibliography{references}  

\end{document}
