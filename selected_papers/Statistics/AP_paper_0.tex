\begin{document}
\affiliation{$$_affiliation_$$}
\title{A mixed effects cosinor modelling framework for circadian gene expression}
\maketitle
\begin{abstract}
\noindent The cosinor model is frequently used to represent gene expression given the 24 hour day-night cycle time at which a corresponding tissue sample is collected. However, the timing of many biological processes are based on individual-specific internal timing systems that are offset relative to day-night cycle time. When these offsets are unknown, they pose a challenge in performing statistical analyses with a cosinor model. To clarify, when sample collection times are mis-recorded, cosinor regression can yield attenuated parameter estimates, which would also attenuate test statistics. This attenuation bias would inflate type II error rates in identifying genes with oscillatory behavior. This paper proposes a heuristic method to account for unknown offsets when tissue samples are collected in a longitudinal design. Specifically, this method involves first estimating individual-specific cosinor models for each gene. The times of sample collection for that individual are then translated based on the estimated phase-shifts across every gene. Simulation studies confirm that this method mitigates bias in estimation and inference. Illustrations with real data from three circadian biology studies highlight that this method produces parameter estimates and inferences akin to those obtained when each individual's offset is known.

 \end{abstract}

{\bf Keywords: } Circadian biology; Dim-light melatonin onset;  Measurement error; Mixed effects models; Two-stage methods

\section{Introduction} \label{sec:1}


Human physiology, metabolism, and behavior all exhibit diurnal variations consistent with a circadian rhythm \citep{Marcheva2013}. These rhythms are generated by an internal self-sustained oscillator located in the hypothalamic suprachiasmatic nucleus, which is often referred to as a circadian clock \citep{Bae2001, Hastings2018, Herzog2017, Yamaguchi2003}. An understanding of a patient's circadian clock has implications in maximizing their quality of care, as survival rates for open-heart surgery \citep{Montaigne2018}, effectiveness of chemotherapy \citep{Dallmann2016}, and response to a vaccine \citep{Long2016} each vary based on the time of day that treatment is administered. However, there is currently limited use of the circadian clock in the timing of treatment administration \citep{Dallmann2014, Wittenbrink2018}.

One challenge in integrating the circadian clock into clinical decision-making is that an individual's clock time (internal circadian time, or ICT) is often offset relative to day-night cycle time (Zeitgeber time, or ZT) \citep{Duffy2011, Lewy1999, Wittenbrink2018}. Laboratory tests can determine the time at which melatonin onset occurs under dim-light conditions, or DLMO time, which is the gold-standard marker of this offset \citep{Hughey2017, Kennaway2023, Lewy1999, Ruiz2020, Wittenbrink2018}. However, these laboratory tests require that multiple tissue samples are collected from an individual under controlled conditions and analyzed \citep{Kantermann2015, Kennaway2019, Kennaway2020, Reid2019}. This process places a costly and labor-intensive burden on investigators who aim to identify treatment strategies based on the circadian clock. These tests can also fail to produce precise DLMO time estimates depending on the devices used in its determination \citep{Kennaway2019, Kennaway2020}. However, the use of ZT instead of ICT in these research efforts can introduce bias in the parameter estimates of regression models commonly used to represent biological phenomena over time, which would bias statistical inferences \citep{Gorczyca2023, Gorczyca2024, Sollberger1962, Weaver1995}.

This paper is motivated by the challenge of determining DLMO time, and considers a scenario where a longitudinal design is utilized to record biological phenomena from each individual. Initially, we assess the suitability of a linear mixed effects cosinor model, commonly used to represent gene expression over time, in accounting for the offset of ICT relative to ZT when the offset is unknown \citep{Archer2014, delolmo2022, Fontana2012, Hou2021, MllerLevet2013}. This assessment reveals that fixed parameter estimates for this model and test statistics computed from it are biased towards zero, or suffer from attenuation bias, when each individual possesses a unique offset. To remove this bias without performing laboratory tests, this paper proposes a method to identify an individual's offset prior to regression. Specifically, given multiple samples collected from each individual over time, this method involves first estimating a cosinor model from each individual's data, and then adjusting the sample collection times for that individual based on their individual-specific phase-shift estimate. 

The remainder of this paper is organized as follows. In Section \ref{sec:2}, background on the mixed effects cosinor model, motivating results, and an overview of the proposed method are presented. In Section \ref{sec:3}, Monte Carlo simulation studies are performed to assess the utility of the proposed method. In Section \ref{sec:4}, the proposed method is applied on real data from three circadian biology studies, where it is found to produce parameter estimates and inferences comparable to those where each individual's DLMO time has been determined from laboratory tests. Finally, in Section \ref{sec:5}, the proposed method and directions for future work are discussed.

\section{Methodology} \label{sec:2}


\subsection{Background and notation} \label{sec:2.1}

Suppose a longitudinal circadian biology experiment is conducted with $M$ individuals, where $n_i$ tissue samples are collected from the $i$-th individual over time. In this paper, it is assumed that each sample is processed to extract the expression levels of $G$ different genes. Specifically, let $\boldsymbol{Y_i^{(g)}}$ denote an $n_i\times 1$ vector $[Y^{(g)}_{i,1},\ldots, Y^{(g)}_{i,n_i}]^T$, in which $Y_{i,j}^{(g)}$ denotes the $j$-th recording of the $g$-th gene for the $i$-th individual. Further, let $\boldsymbol{X_i}$ denote an $n_i\times 1$ vector $[X_{i,1},\ldots, X_{i,n_i}]^T$, in which $X_{i,j}$ denotes the time at which $Y_{i,j}^{(g)}$ occurs for all $g = 1,\ldots, G$. It is emphasized that this paper adopts a notation convention where estimators, statistics, and other quantities related to the $g$-th gene are denoted with $(g)$ in their superscript.

When each sample is independent of every other sample, a cosinor model is often specified for modelling gene expression given time, which has the nonlinear amplitude-phase representation
\begin{align} \label{eq:cos}
    Y^{(g)}_{i,j} &= \mu_0^{(g)} + \theta^{(g)}_1\cos\left(\frac{\pi X_{i,j}}{12} + \theta_2^{(g)}\right)+\epsilon^{(g)}_{i,j}.
\end{align}
Here, $\mu_0^{(g)}$ denotes the mean expression levels of the $g$-th gene; $\theta_1^{(g)}$ denotes the amplitude of the $g$-th gene, or the deviation from mean expression levels to peak expression levels; and $\theta_2^{(g)}$ denotes the phase-shift of the $g$-th gene, which relates to the time at which gene expression levels peak \citep{Cornelissen2014}. In this paper, the gene-specific random noise  $\epsilon_{i,j}^{(g)}\sim \mathcal{N}(0, (\sigma^{(g)})^2)$. When the model in (\ref{eq:cos}) is specified, it can be transformed into the linear model
\begin{equation} \label{eq:3}
    Y^{(g)}_{i,j} = \mu_0^{(g)} + \beta^{(g)}_1\sin\left(\frac{\pi X_{i,j}}{12}\right)+\beta^{(g)}_2\cos\left(\frac{\pi X_{i,j}}{12}\right) + \epsilon^{(g)}_{i,j},
\end{equation}
where the identities
\begin{align}
    \theta_1^{(g)} = \sqrt{(\beta^{(g)}_{1})^2+(\beta^{(g)}_{2})^2}, \ \quad & \theta_2^{(g)} = \atan(-\beta^{(g)}_{1}, \beta^{(g)}_{2}), \nonumber \\
    \beta^{(g)}_{1} = -\theta_1^{(g)}\sin(\theta_2^{(g)}), \quad \quad \quad  & \beta^{(g)}_{2} = \theta_1^{(g)}\cos(\theta_2^{(g)}) \label{eq:alt_to_orig}
\end{align}
can be used to transform the linear model in (\ref{eq:3}) into the nonlinear model in (\ref{eq:cos}), and vice versa \citep{Tong1976}. It is noted that transforming this nonlinear model into a linear model enables unbiased estimation of its parameters without additional technical assumptions \citep[Theorem 6.7]{Boos2013}.

A common extension of the model in (\ref{eq:cos}) is to enable each individual to influence their repeated outcomes. Specifically, mixed effects models can enable this influence \citep{Davidian1995, Hedeker2006, McCulloch2000}, where the cosinor model would instead be specified as
\begin{align} \label{eq:cos_m}
    Y^{(g)}_{i,j} &= \mu_0^{(g)} + m_{i,0}^{(g)} + (\theta^{(g)}_1+c^{(g)}_{i,1})\cos\left(\frac{\pi X_{i,j}}{12} + \theta_2^{(g)}+c_{i,2}^{(g)}\right)+\epsilon^{(g)}_{i,j}.
\end{align}
Here, the parameters $\mu_0^{(g)}$, $\theta^{(g)}_1$, and $\theta^{(g)}_2$ from (\ref{eq:cos}) maintain their interpretation, and represent constant parameter estimands for the population, or fixed effects. Additionally, $m_{i,0}^{(g)}$ denotes the $i$-th individual's influence on their mean expression levels of the $g$-th gene; $c^{(g)}_{i,1}$ denotes the $i$-th individual's influence on their amplitude of the $g$-th gene; and $c^{(g)}_{i,2}$ denotes the $i$-th individual's influence on their phase-shift of the $g$-th gene. It is emphasized that $[m_{i,0}^{(g)}, c^{(g)}_{i,1}, c^{(g)}_{i,2}]^T \sim P^{(g)}$, or parameters related to an individual's influence on their repeated outcomes are considered random effects generated from a gene-specific probability distribution $P^{(g)}$. In this paper, it is assumed that the expectation of each random effect is zero. Further, it is not assumed that each gene-specific $c^{(g)}_{i,2}$ equals the offset of an individual's internal circadian time (ICT) relative to the 24 hour day-night cycle time (Zeitgeber time, or ZT).

Many circadian biology studies assume that (\ref{eq:cos_m}) can be expressed as the linear mixed effects model
\begin{align} \label{eq:lin_m}
    Y^{(g)}_{i,j} &= \left\{\mu^{(g)}_0+\beta^{(g)}_1\sin(X_{i,j})+\beta^{(g)}_2\cos(X_{i,j})\right\} \nonumber \\
    & \quad + \left\{m^{(g)}_{i,0}+b^{(g)}_{i,1}\sin(X_{i,j})+b^{(g)}_{i,2}\cos(X_{i,j}) \right\} + \epsilon^{(g)}_{i,j},
\end{align}
where $[m^{(g)}_{i,0}, b^{(g)}_{i,1}, b^{(g)}_{i,2}]^T \sim \mathcal{N}(0, \Phi^{(g)})$ and are independent of the random noise $\epsilon^{(g)}_{i,j}$ for all $g$ \citep{Archer2014, delolmo2022, Fontana2012, Hou2021, MllerLevet2013}. Under these assumptions,
\begin{align*}
    \boldsymbol{Y_i^{(g)}} \sim \mathcal{N}(\boldsymbol{W_i}^T\beta^{(g)}, \boldsymbol{V_i^{(g)}}),
\end{align*}
where $\beta^{(g)} =[{\mu}^{(g)}_0, {\beta}^{(g)}_1,  {\beta}^{(g)}_2]^T$ denotes the vector of fixed effect estimands for the $g$-th gene,
\begin{align*}
    \boldsymbol{W_i} &= \begin{bmatrix} 1 & \sin\left(\frac{\pi X_{i,1}}{12}\right) & \cos\left(\frac{\pi X_{i,1}}{12}\right) \\
    \vdots & \vdots & \vdots \\
    1 & \sin\left(\frac{\pi X_{i,n_i}}{12}\right) & \cos\left(\frac{\pi X_{i,n_i}}{12}\right)
    \end{bmatrix}
\end{align*}
denotes the design matrix for estimating fixed effects, and
\begin{align*}
\boldsymbol{V_i^{(g)}} = (\sigma^{(g)})^2I_{n_i} + \boldsymbol{W_i} \Psi^{(g)}\boldsymbol{W_i}^T
\end{align*}
denotes the covariance matrix of the $g$-th gene's expression levels for the $i$-th individual, with $I_{n_i}$ denoting an $n_i \times n_i$ identity matrix. The corresponding likelihood function that is maximized for estimating the parameters of this model is defined as
\begin{align*}
    L(\beta^{(g)}, \Psi^{(g)}, (\sigma^{(g)})^2 \mid  \boldsymbol{W_1},\ldots,\boldsymbol{W_M}, \boldsymbol{Y_1^{(g)}},\ldots,\boldsymbol{Y_M^{(g)}}) = \prod_{i=1}^M\frac{\exp\left\{-\frac{(\boldsymbol{Y_i^{(g)}}-\boldsymbol{W_i}^T\beta^{(g)})^T(\boldsymbol{V_i^{(g)}})^{-1}(\boldsymbol{Y_i^{(g)}}-\boldsymbol{W_i}^T\beta^{(g)})}{2} \right\}}{(2\pi)^{n_i/2} \sqrt{|\boldsymbol{V_i^{(g)}}|}}.
\end{align*}
When $(\sigma^{(g)})^2$ and $\Psi^{(g)}$ are known, the expression
\begin{align*}
    \hat{\beta}^{(g)} = \begin{bmatrix}
    \hat{\mu}^{(g)}_0 \\
    \hat{\beta}^{(g)}_1 \\
    \hat{\beta}^{(g)}_2
    \end{bmatrix} = \left(\sum_{i=1}^M\boldsymbol{W_i}^T(\boldsymbol{V_i^{(g)}})^{-1}\boldsymbol{W_i}\right)^{-1}\left(\sum_{i=1}^M\boldsymbol{W_i}^T(\boldsymbol{V_i^{(g)}})^{-1}\boldsymbol{Y_i^{(g)}}\right)
\end{align*}
yields maximum likelihood estimates of the fixed effects (\citealp[Page 78]{Davidian1995}; \citealp[Equation 6.19]{McCulloch2000}). When $(\sigma^{(g)})^2$ and $\Psi^{(g)}$ are unknown, it is not possible to obtain a closed-form expression of these parameter estimates, and an expectation-maximization algorithm is utilized to obtain them (\citealp[Chapter 4]{Hedeker2006}; \citealp{Laird1987}; \citealp[Chapter 14]{McCulloch2000}).

After parameter estimation, an investigator could perform hypothesis tests to assess the statistical significance of a gene's oscillatory behavior. This paper considers hypothesis testing with the fixed effects parameter vector $\hat{\beta}^{(g)}$, with which Wald-type test statistics can be computed as
\begin{align}
    \tau^{(g)} = \frac{1}{d}(\hat{\beta}^{(g)} - \hat{\beta}^{(g)}_{\text{Null}})^TL^T\left[L\left\{\sum_{i=1}^M\boldsymbol{W_i}^T(\boldsymbol{V_i^{(g)}})^{-1}\boldsymbol{W_i}\right\}^{-1}L^T\right]^{-1}L(\hat{\beta}^{(g)} - \hat{\beta}^{(g)}_{\text{Null}}). \label{eq:wald}
\end{align}
Here, $L$ denotes a $d\times 3$ matrix, where $d\leq 3$, and $\hat{\beta}^{(g)}_{\text{Null}}$ denotes the empirical parameter vector estimate under the null hypothesis, which is $H_0: L(\beta^{(g)} - \beta^{(g)}_{\text{Null}}) = 0$ \citep{Halekoh2014}. This paper defines 
\begin{align*}
    L = \begin{bmatrix}
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
\end{align*}
and $\hat{\beta}^{(g)}_{\text{Null}} = [0, 0, 0]^T$, which can be considered a determination of whether or not a gene displays oscillatory behavior \citep{Zong2023}. This test statistic has an asymptotic $\chi^2_2$ distribution, and the null hypothesis would be rejected at a pre-determined $\alpha$-level if $\tau^{(g)}$ surpasses the $1-\alpha$ percentile of this distribution \citep{Halekoh2014}. 

\subsection{Fixed effects estimation in the presence of random phase-shifts} \label{sec:2.2}
A long-standing challenge in computing parameter estimates and test statistics for a cosinor model is that these quantities are biased when each individual has a distinct, unknown phase-shift parameter \citep{Sollberger1962, Weaver1995}. Recent investigations into this bias were motivated by the underlying offset of an individual's ICT relative to ZT. Specifically, these investigations interpreted this offset as covariate measurement error, or mis-recorded time data, and theoretically  as well as empirically quantified how cosinor model parameter estimates and test statistics are biased when ZT is used for regression \citep{Gorczyca2023, Gorczyca2024}. Building upon these insights, this paper evaluates the accuracy of approximating the nonlinear mixed cosinor model in (\ref{eq:cos_m}) with the linear mixed cosinor model in (\ref{eq:lin_m}) when each individual's random phase-shift parameter is unknown. Specifically, we first introduce the following proposition, which quantifies this accuracy.

\begin{prop} \label{prop:1}
Suppose each $n_i = n$ and $X_{i,j} = 24(j-1)/n$. Assume the model in (\ref{eq:lin_m}) is specified to estimate the response in (\ref{eq:cos_m}), with both $(\sigma^{(g)})^2$ and $\Psi^{(g)}$ known such that $\Psi^{(g)} = \diag(\psi^{(g)}_1, \psi^{(g)}_2, \psi^{(g)}_3)$, or equals a $3\times 3$ diagonal matrix where $\psi^{(g)}_k$ denotes the $k$-th element along the diagonal. Further, assume $[m_{i,0}^{(g)}, c^{(g)}_{i,1}, c^{(g)}_{i,2}]^T \sim P^{(g)}$, where the corresponding probability density function of $P^{(g)}$ is symmetric around zero. Then the expectation of the fixed parameter estimates
\begin{align*}
    \mathbb{E}(\hat{\beta}^{(g)}) = \begin{bmatrix}
    \mathbb{E}(\hat{\mu}^{(g)}_0) \\
    \mathbb{E}(\hat{\beta}^{(g)}_1) \\
    \mathbb{E}(\hat{\beta}^{(g)}_2)
    \end{bmatrix} = \begin{bmatrix}
    \mu^{(g)}_0 \\
    \phi_{c^{(g)}_2}(1)\beta^{(g)}_1 \\
    \phi_{c^{(g)}_2}(1)\beta^{(g)}_2
    \end{bmatrix},
\end{align*}
where $\phi_{c^{(g)}_2}(t)$ denotes the characteristic function for $c^{(g)}_{i,2}$ evaluated at $t$. Further, the Wald test statistic in (\ref{eq:wald}) computed with the expectation of these parameter estimates can be expressed as
\begin{align*}
    \tau^{(g)} 
    &= \frac{Mn\phi^2_{c^{(g)}_2}(1)}{4}\left\{\left(\frac{1}{(\sigma^{(g)})^2} - \frac{n\psi^{(g)}_2}{(\sigma^{(g)})^2\{n\psi_2+2(\sigma^{(g)})^2\}}\right)(\beta_1^{(g)})^2 \right. \\
    & \quad \quad + \left. \left(\frac{1}{(\sigma^{(g)})^2} - \frac{n\psi^{(g)}_3}{(\sigma^{(g)})^2(n\psi^{(g)}_3+2(\sigma^{(g)})^2\}}\right)(\beta^{(g)}_2)^2 \right\}.
\end{align*}
\end{prop}

\noindent A derivation for this result is provided in Appendix \ref{app:B}. To clarify the setup for this result, when each $X_{i,j} = 24(j-1)/n$, tissue samples are collected from an equispaced design. This design is optimal for minimizing the estimation variance of a cosinor model under multiple statistical criteria \citep[Pages 241-243]{Pukelsheim2006}. Further, when each $n_i = n$, no samples are removed prior to estimation, which can occur when a collected tissue sample is poor quality \citep{Laurie2010}. It is noted that symmetry is assumed for the probability density function of $P^{(g)}$. As a consequence, the marginal density function of $c^{(g)}_{i,2}$ is also symmetric, which results in $\phi_{c_2^{(g)}}(t)$ being a real-valued function that is bounded by one in magnitude.

The significance of Proposition \ref{prop:1} is it presents a scenario where an investigator employs a common experimental design and has additional knowledge of $(\sigma^{(g)})^2$ and $\Phi^{(g)}$. In this scenario, the magnitude of the fixed parameter estimates $\hat{\beta}_1^{(g)}$ and $\hat{\beta}_2^{(g)}$ are biased towards zero, as the characteristic function $\phi_{c_2^{(g)}}(t)$ is bounded by one in magnitude. Further, application of (\ref{eq:alt_to_orig}) yields
\begin{align*}
    \sqrt{\mathbb{E}(\hat{\beta}^{(g)}_1)^2 + \mathbb{E}(\hat{\beta}^{(g)}_2)^2}  &= |\phi_{c^{(g)}_2}(1)|\theta^{(g)}_1, \\
    \atan\{-\mathbb{E}(\hat{\beta}^{(g)}_1), \mathbb{E}(\hat{\beta}^{(g)}_2)\}  &= \theta^{(g)}_2,
\end{align*}
which indicates that the fixed amplitude parameter estimate is biased, while the fixed phase-shift parameter estimate is unbiased. These results also indicate that the test statistic $\tau^{(g)}$ is attenuated by $\phi^2_{c^{(g)}_2}(1)$ relative to the scenario where there are no random phase-shift parameters.

To understand when the linear mixed effects model in (\ref{eq:lin_m}) produces unbiased parameter estimates if the random phase-shifts are unknown, an alternate representation that is equivalent to it is
\begin{align} \label{eq:res_2}
Y^{(g)}_{i,j} &= (\mu^{(g)}_0 + m^{(g)}_{i, 0}) + \theta^{(g)}_1\cos(X_{i,j}+\theta^{(g)}_{2}) + c^{(g)}_{i,1}\cos(X_{i,j}+c^{(g)}_{i,2}) + \epsilon^{(g)}_{i,j},
\end{align}
which separates fixed and random effects into different nonlinear components \citep{Mikulich2003}. The following proposition transforms the model in (\ref{eq:res_2}) into the model in (\ref{eq:cos_m}).
\begin{prop} \label{prop:2}
The model in (\ref{eq:res_2}) can be expressed as 
\begin{align*}
Y^{(g)}_{i,j} &= (\mu^{(g)}_0 + m^{(g)}_{i, 0}) + \sqrt{\left\{\theta^{(g)}_1\sin(\theta^{(g)}_2) + c^{(g)}_{i,1}\sin(c^{(g)}_{i,2})\right\}^2 + \left\{\theta^{(g)}_1\cos(\theta^{(g)}_2) + c^{(g)}_{i,1}\cos(c^{(g)}_{i,2})\right\}^2} \nonumber \\
    & \quad \quad \times \cos\left[\frac{\pi X_{i,j}}{12} + \atan\left\{\theta^{(g)}_1\sin(\theta^{(g)}_2) + c^{(g)}_{i,1}\sin(c^{(g)}_{i,2}), \theta^{(g)}_1\cos(\theta^{(g)}_2) + c^{(g)}_{i,1}\cos(c^{(g)}_{i,2})\right\}\right] + \epsilon^{(g)}_{i,j},
\end{align*}
where $\atan(a, b)$ denotes the two argument arctangent function.
\end{prop}
\noindent A derivation for this result is provided in Appendix \ref{app:C}. Proposition \ref{prop:2} implies that when each individual's random phase-shift is unknown, fixed parameter estimates obtained from specifying the linear mixed effects model in (\ref{eq:lin_m}) are biased unless the conditions
\begin{align*}
\theta_1^{(g)} &= \mathbb{E}\left[\sqrt{\left\{\theta^{(g)}_1\sin(\theta^{(g)}_2) + c^{(g)}_{i,1}\sin(c^{(g)}_{i,2})\right\}^2 + \left\{\theta^{(g)}_1\cos(\theta^{(g)}_2) + c^{(g)}_{i,1}\cos(c^{(g)}_{i,2})\right\}^2} \right], \\
\theta_2^{(g)} &= \mathbb{E}\left[\atan\left\{\theta^{(g)}_1\sin(\theta^{(g)}_2) + c^{(g)}_{i,1}\sin(c^{(g)}_{i,2}), \theta^{(g)}_1\cos(\theta^{(g)}_2) + c^{(g)}_{i,1}\cos(c^{(g)}_{i,2})\right\} \right]
\end{align*}
are satisfied.

\subsection{Overview of method} \label{sec:2.3}

Propositions \ref{prop:1} and \ref{prop:2} highlight that parameter estimates and inferences of a linear mixed effects cosinor model are biased when the random phase-shift parameters are unknown. A practical consequence of not addressing these biases are that study conclusions made from a linear mixed effects cosinor model would be inaccurate. For example, the attenuation bias in parameter estimates and test statistics presented in Proposition \ref{prop:1} would result in genes being misclassified as having statistically insignificant oscillatory behavior in a circadian biology study.   

To address this bias, an investigator could collect a sufficient number of samples from each individual to estimate individual-specific cosinor models for each gene. In this scenario, an investigator could utilize two-stage methods, which would leverage individual-specific nonlinear model parameter estimates as building blocks for estimation and inference \citep[Chapter 5]{Davidian1995}. It is noted that a two-stage method has been proposed for the cosinor model to address the presence of random phase-shifts \citep{Weaver1995}. 

A challenge in utilizing two-stage methods in circadian biology studies is each $c_{i,2}^{(g)}$ is distinct across genes, and each $c_{i,2}^{(g)}$ is not necessarily indicative of the underlying offset between an individual's internal circadian time (ICT) and Zeitgeber time (ZT). Specifically, each gene displays different oscillatory behavior, with approximately 50\
To address this challenge and recover parameter estimates and inferences that would be obtained when each individual's offset is known, we propose is a heuristic based on two-stage methods that involves the following steps. For each step, relevant comments are provided to clarify their significance and implications:
\begin{description}
    \item[Step 1.] Estimate a linear mixed effects cosinor model for each gene with data across every individual.
    \item[Step 2.] For each gene-specific (indexed by $g$) cosinor model estimated in Step 1, compute the amplitude parameter estimate ($\hat{\theta}_1^{(g)}$) and phase-shift parameter estimate ($\hat{\theta}_{2}^{(g)}$) with the identities in (\ref{eq:alt_to_orig}). 
    \item[Step 3.] Estimate individual-specific linear cosinor models for each gene.
    \item[Step 4.] For each individual-specific (indexed by $i$) and gene-specific (indexed by $g$) cosinor model estimated in Step 3, compute the amplitude parameter estimate ($\hat{\theta}_{i,1}^{(g)}$) and phase-shift parameter estimate ($\hat{\theta}_{i,2}^{(g)}$) with the identities in (\ref{eq:alt_to_orig}). 
    \item[Step 5.] Define $$\hat{d}^{(g)}_{i} = \left(\frac{\frac{1}{\mathrm{Var}(\hat{\theta}^{(g)}_{i,1})}}{\frac{1}{\mathrm{Var}(\hat{\theta}^{(g)}_1)}+\frac{1}{\mathrm{Var}(\hat{\theta}^{(g)}_{i,1})}}\right)\hat{\theta}^{(g)}_{i,2} + \left(\frac{\frac{1}{\mathrm{Var}(\hat{\theta}^{(g)}_{1})}}{\frac{1}{\mathrm{Var}(\hat{\theta}^{(g)}_1)}+\frac{1}{\mathrm{Var}(\hat{\theta}^{(g)}_{i,1})}}\right)\hat{\theta}^{(g)}_{2} - \hat{\theta}^{(g)}_2.$$Here, $\mathrm{Var}(\hat{\theta}^{(g)}_1)$ denotes the estimated variance for $\hat{\theta}^{(g)}_1$, and $\mathrm{Var}(\hat{\theta}^{(g)}_{i,1})$ denotes the estimated variance for $\hat{\theta}^{(g)}_{i,1}$.
    \item[Step 6.]  Define $\boldsymbol{\tilde{X}_i} = \boldsymbol{X_i}+\tilde{d}_{i}$, where $$\tilde{d}_{i} = \frac{12}{\pi}\left(\frac{1}{\sum_{g=1}^G\frac{1}{\mathrm{Var}(\hat{\theta}^{(g)}_{i,1})}}\right)\sum_{g=1}^G\frac{\hat{d}_{i,2}^{(g)}}{\mathrm{Var}(\hat{\theta}^{(g)}_{i,1})}.$$
    \item[Step 7.] Estimate a linear mixed effects cosinor model with the data $\{(\boldsymbol{\tilde{X}_i}, \boldsymbol{Y_i^{(g)}}), i = 1,\ldots, m\}$ for each gene.
\end{description}
\noindent Steps 1-4 obtain individual-specific and population parameter estimates. Step 5 first computes an inverse-variance weighted average of $\hat{\theta}_{i,2}^{(g)}$ and $\hat{\theta}_2^{(g)}$ \citep{Cochran1953}, where the variance quantities are derived from the respective amplitude estimates. It is emphasized that the difference between this average and $\hat{\theta}_2^{(g)}$ is an estimate of $w_i^{(g)}c_{i,2}^{(g)}$, where
\begin{align*}
w_{i}^{(g)} &= \frac{\frac{1}{\mathrm{Var}(\theta^{(g)}_{i,1})}}{\frac{1}{\mathrm{Var}(\theta^{(g)}_1)} + \frac{1}{\mathrm{Var}(\theta^{(g)}_{i,1})}}
\end{align*}
is bounded between zero and one. The estimated weight $\hat{w}_{i}^{(g)}$ can be interpreted as balancing the contribution of $\hat{\theta}^{(g)}_2$ relative to $\hat{\theta}^{(g)}_{i,2}$ based on their respective amplitude variances, where a derivation for the amplitude variance is provided in Appendix \ref{app:D}. In Step 6, an additional inverse-variance weighted average of each $\hat{d}_{i,2}^{(g)}$ across every gene is computed with the corresponding amplitude variance $\mathrm{Var}(\hat{\theta}^{(g)}_{i,1})$. The times of sample collection for the $i$-th individual are then translated by the quantity obtained from this weighted average, which is denoted as $\tilde{d}_{i}$. In Step 7, a linear mixed effects model is estimated with this translated data.

It is noted that the population phase-shift estimates $\theta_2^{(g)}$ obtained with this method can be different from those obtained with a linear mixed effects model where the times of sample collection are not translated. If an investigator is interested in analyzing estimates of $\theta_2^{(g)}$, this paper recommends that an additional adjustment is performed to ensure each $\hat{\theta}_2^{(g)}$ is the same regardless of whether $\boldsymbol{\tilde{X}_i}$ or $\boldsymbol{X_i}$ is used for estimation. This recommendation is supported by Proposition \ref{prop:1}, which implies that phase-shift estimates are unbiased when generated by a symmetric probability distribution. 

\section{Simulation Study} \label{sec:3}

A simulation study is conducted to evaluate the method proposed in Section \ref{sec:2.3}, where the design of this study is detailed in Appendix \ref{app:E}. Briefly, six settings are considered from the $\text{BioCycle}_{\text{Form}}$ simulation software \citep{Agostinelli2016, Ceglia2018}. Specifically, each setting represents a scenario in the software where a simulated gene displays periodic behavior over a 24 hour interval, where the cosinor model is mis-specified for many of these settings. For each simulation setting, 2,000 simulation trials are performed, and in each simulation trial, two data sets are generated: one in which each individual has a random phase-shift parameter, and one in which there are no random phase-shift parameters. It is noted the second data set represents a scenario where the random phase-shifts are known. 

This simulation study considers three estimation frameworks to obtain linear mixed effects cosinor models from these two data sets:
\begin{description}
    \item[Framework 1.] A linear mixed effects cosinor model is estimated using the method in Section \ref{sec:2.3} with the data set where each individual has an unknown random phase-shift parameter.
    \item[Framework 2.] A linear mixed effects cosinor model is estimated with the data set where each individual has an unknown random phase-shift parameter.
    \item[Framework 3.] A linear mixed effects cosinor model is estimated with the data set where there are no random phase-shift parameters. 
 \end{description}
It is emphasized that only one gene is generated in each simulation trial. As a consequence, Step 6 of the method in Framework 1 would set $\tilde{d}_{i} = \hat{d}^{(g)}_{i}$. 

After conducting 2,000 simulation trials, the mean and standard deviation of the following quantities are reported:
\begin{itemize}
    \item[1.] $\hat{\theta}_1^{(g)}$, or the estimated amplitude in a simulation trial.
    \item[2.] $\hat{\tau}^{(g)}$, or the computed Wald test statistic from (\ref{eq:wald}) in a simulation trial.
\end{itemize} 
It is noted that the phase-shift estimate is not considered, as Framework 1 can be modified to produce the same phase-shifts as Framework 2.

Table \ref{tab:sim1} presents the results for each simulation setting. Framework 1, or the method in Section \ref{sec:2.3}, consistently produces amplitude estimates and test statistics that align with those produced if each individual's $\theta_{i,2}^{(g)}$ is known. Framework 2, on the other hand, produces corresponding quantities that suffer from attenuation bias. 


\section{Illustrations} \label{sec:4}

Publicly available data from three longitudinal circadian biology studies are considered for illustration: \cite{Archer2014} produced data from two cohorts (a control cohort and an intervention cohort); \cite{Braun2018} produced data from a single cohort; and \cite{MllerLevet2013} produced data from two cohorts (a control cohort and an intervention cohort). Each data set has been described in detail in their respective studies and summarized by \cite{Gorczyca2024}. Additionally, each data set has been processed by \cite{Huang2024} and made publicly available. The significance of each data set is that their corresponding studies performed laboratory tests to determine the offset of each study participant's internal circadian time (ICT) relative to Zeitgeber time (ZT). However, gene expression measurements and offsets are unavailable for some study participants in these processed data sets. In this illustration, genes with missing expression measurements and samples from study participants with missing offsets within cohort-specific data are excluded before estimation and inference. It is noted that, for illustrative purposes, this illustration constructs two additional cohorts: one where data from both cohorts for \cite{Archer2014} are combined, and another where data from both cohorts for \cite{MllerLevet2013} are combined.

The illustrations follow the evaluation procedure from Section \ref{sec:3} using data from each of the seven cohorts considered. Specifically, Wald test statistics in (\ref{eq:wald}) and fixed amplitude parameters in (\ref{eq:cos_m}) are computed from Frameworks 1 and 2 given ZT. Framework 3 computes these same quantities given ICT, which are considered the true quantities in this illustration. It is emphasized that each cohort-specific data set consists of multiple genes. To account for this, a linear model-based evaluation is presented in this illustration \citep{Gorczyca2024}. Specifically, the covariate of this linear model is specified as a quantity obtained from a framework given ZT, while the corresponding response variable is the corresponding true quantity obtained from Framework 3. No intercept term is specified in this linear model, which results in a single regression parameter estimate, $\gamma$. This paper utilizes $\gamma$ to assess the relationship between the quantities obtained from each framework. If $\gamma=1$, a linear relationship exists between the quantity obtained with ZT and a quantity obtained with ICT. If $\gamma > 1$, then quantities obtained with ICT are consistently larger than quantities obtained with ZT. If $\gamma < 1$, then quantities obtained with ZT are consistently larger than quantities obtained with ICT. This assessment also reports the coefficient of determination ($R^2$) for each linear model, where higher $R^2$ values signify greater precision in linear model fit. It is noted that a stronger performing framework would obtain $\gamma$ values closer to one and a larger $R^2$ value.

Table \ref{tab:app} presents linear model $\gamma$ parameter estimates and coefficients of determination computed from each sample population's data. Notably, Framework 1, or the method proposed in Section \ref{sec:2.3}, reduced bias in amplitude estimation and Wald test statistic calculation, often yielding $\gamma$ estimates approximately equal to one. Framework 2 outperforms Framework 1 in amplitude estimation with intervention cohort data from \cite{Archer2014}, and Wald test statistic calculation with control cohort data from \cite{Archer2014}. Table \ref{tab:app} highlights that the estimated $\gamma$ values are consistently larger than one for Framework 2, which implies that estimation of a linear mixed effects model given ZT results in attenuated parameter estimates and test statistics.

\section{Discussion} \label{sec:5}

In this paper, a heuristic method is proposed to account for the offset of each individual's internal circadian time (ICT) relative to Zeitgeber time (ZT) when these offsets are unknown. If these offsets are left unaddressed by an investigator during statistical analysis, the parameter estimates for a cosinor model and test statistics computed with these estimates suffer from attenuation bias \citep{Sollberger1962, Weaver1995, Gorczyca2023, Gorczyca2024}, which would inflate type II error rates in identifying genes with oscillatory behavior. The method proposed in this paper requires that a sufficient number of samples are collected from each individual in a longitudinal design. The collection of samples in a longitudinal design enables estimation of individual-specific cosinor models for each gene, which the proposed method uses to translate the ZTs that samples are collected.

We recognize that there are limitations to this study. One limitation is that the cosinor model is biased towards the identification of genes that display sinusoidal oscillations. However, this model is common for representing gene expression over time \citep{Archer2014, delolmo2022, Fontana2012, Hou2021, MllerLevet2013}, and enables an investigator to obtain interpretable parameter estimates and perform hypothesis tests. Second, the translated times of sample collection obtained from this method do not necessarily correspond to translating the times of sample collection by each individual's true offset. To clarify, the method translates sample collection times based on the estimated times that genes peak in their expression levels. The resulting quantity identified for translating sample collection times could be offset from an individual's melatonin onset time under dim-light conditions, or DLMO time, which is a gold-standard biomarker for an individual's true offset. However, the proposed method produces amplitude estimates and Wald test statistics akin to those obtained when each individual's DLMO time is known.

The results presented in this paper open up avenues for future research. First, other frameworks have been proposed to identify genes with oscillatory behavior \citep{Mei2020}. There could be value in studying how statistical analyses with these frameworks are affected by the presence of individual-specific unknown offsets. Second, depending on the assumptions an investigator makes about the oscillatory behavior of a gene over time, there could be utility in extending recent non-parametric methods for data contaminated with covariate measurement error to longitudinal designs \citep{Delaigle2016, DiMarzio2021, Dimarzio2023, Nghiem2018, Nghiem2020}.

\section*{Acknowledgements}
The author would like to thank Forest Agostinelli at the University of South Carolina for conversation concerning the BioCycle simulation data sets, which supported development of the simulation study design in Section \ref{sec:3}; Thomas Brooks at the University of Pennsylvania for input on factors that could affect reproducibility in circadian biology studies, which helped refine the scope of this paper and method development; and David Kennaway at the University of Adelaide for sharing insights on the cost of DLMO time determination and providing examples of biological processes that exhibit minimal diurnal variations, which enhanced the presentation of this paper. 

\newpage
\clearpage


\begin{table*}[!h]
	\caption{Simulation study results for each framework. Framework 1 uses the method proposed in Section \ref{sec:2.3} when individuals have unknown offsets, Framework 2 performs linear mixed effects cosinor regression on data where individuals have unknown offsets, and Framework 3 performs linear mixed effects cosinor regression on data where individuals have known offsets. The mean amplitudes and test statistics are reported, with their corresponding standard deviations in parentheses. For Frameworks 1 and 2, mean quantities that are closer to the corresponding mean quantity obtained with Framework 3 are denoted in bold.} \label{tab:sim1}
 \centering
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
			\hline
   Simulation Setting & Framework  & $\theta_{1}^{(g)}$  & $\tau^{(g)}$ \\
   \hline
         \multirow{3}{*}{1} &  1 & \textbf{0.292 (0.067)} & \textbf{10.606 (6.166)} \\
                            &  2 & 0.273 (0.068) & 9.174 (5.958) \\
                            &  3 & 0.306 (0.071) & 10.816 (6.852) \\
         \hline 
         \multirow{3}{*}{2} &  1 & \textbf{0.322 (0.102)} & \textbf{4.528 (2.989)} \\
                            &  2 & 0.290 (0.103) & 3.663 (2.730) \\
                            & 3 & 0.322 (0.107) & 4.459 (3.192) \\
         \hline 
         \multirow{3}{*}{3}& 1 & \textbf{0.266 (0.079)} & \textbf{5.375 (3.391)} \\
                           &  2 & 0.237 (0.081) & 4.261 (3.098) \\
                           & 3 & 0.305 (0.084) & 6.878 (4.303) \\
         \hline 
         \multirow{3}{*}{4} & 1 & \textbf{0.150 (0.057)} & \textbf{2.731 (2.237)} \\
                            & 2 & 0.116 (0.057) & 1.678 (1.825) \\
                            & 3 & 0.250 (0.062) & 8.280 (4.783) \\
         \hline 
         \multirow{3}{*}{5} &  1 & \textbf{0.193 (0.067)} & \textbf{3.950 (2.888)} \\
                            &  2 & 0.165 (0.069) & 2.949 (2.578) \\
                            &  3 & 0.251 (0.072) & 6.689 (4.313) \\
         \hline 
         \multirow{3}{*}{6} & 1 & \textbf{0.306 (0.094)} & \textbf{5.833 (3.969)} \\
                            &  2 & 0.254 (0.096) & 3.979 (3.443) \\
                            &  3 &  0.314 (0.085) & 7.509 (4.800) \\
         \hline 
\end{tabular} \end{table*}

\clearpage
\newpage



\begin{table*}[!h]
	\caption{Comparison of both frameworks using data from each sample population. Framework 1 estimates a linear mixed effects cosinor model with the method proposed in Section \ref{sec:2.3} when an individual's offset is unknown, while Framework 2 assumes each individual has the same offset. The regression parameter estimate $\gamma$ is listed alongside the coefficient of determination ($R^2$), where the latter is in parentheses. Bold values indicate a value of $\gamma$ closer to one, which signifies that the quantities obtained from a framework are closer to the quantities obtained with data where each individual's offset is determined from laboratory tests.} \label{tab:app}
  \centering
		\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
			\hline
   \multirow{1}{*}{Sample Population} & \multirow{1}{*}{Framework}  &  \multicolumn{1}{c|}{$\theta^{(g)}_1$} & \multicolumn{1}{c|}{$\tau^{(g)}$} \\
   \hline
         \multirow{2}{*}{Archer (Control)} & 1 & \textbf{1.014 (0.978)} & 1.119 (0.944) \\ 
        & 2 & 1.019 (0.990) & \textbf{1.071 (0.978)} \\ 
         \hline 
         \multirow{2}{*}{Archer (Intervention)} & 1 & 0.976 (0.935) & \textbf{0.965 (0.904)} \\ 
    & 2 & \textbf{0.994 (0.936)} & 1.042 (0.911) \\
         \hline 
         \multirow{2}{*}{Archer (Combined)}& 1 & \textbf{1.030 (0.967)} & \textbf{1.052 (0.942)} \\
         & 2 & 1.072 (0.970) & 1.195 (0.953) \\
         \hline 
         \multirow{2}{*}{Braun} & 1 & \textbf{1.004 (0.996)} & \textbf{1.030 (0.980)} \\
         & 2 & 1.021 (0.995) & 1.068 (0.977) \\
         \hline 
         \multirow{2}{*}{M\"{o}ller-Levet (Control)} & 1 & \textbf{1.099 (0.990)} & \textbf{1.158 (0.967)} \\ 
         & 2 & 1.136 (0.987) & 1.242 (0.956) \\
         \hline 
         \multirow{2}{*}{M\"{o}ller-Levet (Intervention)} & 1 & \textbf{1.007 (0.989)} & \textbf{1.052 (0.967)} \\ 
         & 2 & 1.063 (0.991) & 1.135 (0.974) \\
         \hline 
         \multirow{2}{*}{M\"{o}ller-Levet (Combined)} & 1 & \textbf{1.080 (0.994)} & \textbf{1.145 (0.979)} \\ 
         & 2 & 1.110 (0.994) & 1.191 (0.978) \\
         \hline 
\end{tabular} \end{table*}


\newpage
\clearpage


\appendix

\section{Supporting Lemmas} \label{app:A}
\begin{lemma} \label{lem:1}
Suppose $[m_0, c_1, c_2]\sim P$ such that the probability density function for $P$, denoted as $\rho(m_0, c_1, c_2)$, is symmetric with a mean of zero. Then $\mathbb{E}\{c_{1}\sin(c_{2})\} = \mathbb{E}\{c_1\cos(c_2)\} = 0$.
\end{lemma}

\begin{proof}
Given that $\rho(m_0, c_1, c_2)$ is symmetric, the corresponding marginal probability density function $\rho(c_1, c_2)$ is also symmetric. For $\mathbb{E}\{c_1\cos(c_2)\}$, it follows that

\begin{align*}
    \mathbb{E}\{c_1\cos(c_2)\} &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \rho(c_1, c_2)c_1\cos(c_2) dc_1 dc_2 \\
    &= \int_{-\infty}^{\infty}\left\{\int_{0}^{\infty} \rho(c_1, c_2)c_1\cos(c_2) dc_1 + \int_{-\infty}^{0} \rho(c_1, c_2)c_1\cos(c_2) dc_1\right\} dc_2 \\
    &= \int_{-\infty}^{\infty}\left\{\int_{0}^{\infty} \rho(c_1, c_2)c_1\cos(c_2) dc_1 - \int_{0}^{\infty} \rho(-c_1, c_2)c_1\cos(c_2) dc_1\right\} dc_2 \\
    &= \int_{-\infty}^{\infty}\left\{\int_{0}^{\infty} \rho(c_1, c_2)c_1\cos(c_2) dc_1 - \int_{0}^{\infty} \rho(c_1, c_2)c_1\cos(c_2) dc_1\right\} dc_2 \\
    &= 0.
\end{align*}
The derivation is equivalent for $\mathbb{E}\{c_{1}\sin(c_{2})\}$.
\end{proof}

\begin{lemma} \label{lem:2}
Suppose each $n_i = n$, $\Psi^{(g)}_i= \diag(\psi^{(g)}_1, \psi^{(g)}_2, \psi^{(g)}_3)$ is a $3\times 3$ diagonal matrix, and $X_{i,j} = 24(j-1)/n$. Further, define 
\begin{align*}
    \boldsymbol{W_i} &= \begin{bmatrix}1 & \sin\left(\frac{\pi X_{i,1}}{12}\right) & \cos\left(\frac{\pi X_{i,1}}{12}\right) \\
    \vdots & \vdots & \vdots \\
    1 & \sin\left(\frac{\pi X_{i,n}}{12}\right) & \cos\left(\frac{\pi X_{i,n}}{12}\right)
    \end{bmatrix}.
\end{align*}
Then each element of the matrix $\boldsymbol{V^{(g)}_i}$ can be expressed as
\begin{align*}
    \left\{(\boldsymbol{V^{(g)}_i})^{-1}\right\}_{j,k} &= \left[\left\{(\sigma^{(g)})^2I_{n} + \boldsymbol{W_i}\Psi^{(g)} \boldsymbol{W_i}^T\right\}^{-1}\right]_{j,k} \\
     &= \frac{\mathbbm{1}\{j=k\}}{\sigma^2} \\
     &\quad \quad - \frac{\psi_1}{\sigma^2(N\psi_1+\sigma^2)} \\
     & \quad \quad - \frac{2\psi_2}{\sigma^2(N\psi_2+2\sigma^2)}\sin\left(\frac{\pi X_{i,j}}{12}\right)\sin\left(\frac{\pi X_{i,k}}{12}\right) \\
     & \quad \quad - \frac{2\psi_3}{\sigma^2(N\psi_3+2\sigma^2)}\cos\left(\frac{\pi X_{i,j}}{12}\right)\cos\left(\frac{\pi X_{i,k}}{12}\right),
\end{align*}
where 
\begin{equation*}
    \mathbbm{1}\{j=k\} =  \begin{cases} 
      1 & j=k, \\
      0 & j\neq k.
   \end{cases}
\end{equation*}
\end{lemma}

\begin{proof}
To simplify presentation, the superscript $(g)$ is omitted. Recall the identity presented in \citet[page 78]{Davidian1995},
\begin{align*}
    (R + Z\Psi Z^T)^{-1} &= R^{-1} - R^{-1}Z(Z^TR^{-1}Z + \Psi^{-1})^{-1}Z^TR^{-1},
\end{align*}
which yields
\begin{align*}
    \boldsymbol{V_i}^{-1} &= (\sigma^2I_{n} + \boldsymbol{W_i}\Psi \boldsymbol{W_i}^T)^{-1} \\
    &= \frac{1}{\sigma^2}I_{n} - \frac{1}{\sigma^4}\boldsymbol{W_i}\left(\frac{1}{\sigma^2}\boldsymbol{W_i}^T\boldsymbol{W_i}+\Psi^{-1}\right)^{-1}\boldsymbol{W_i}^T.
\end{align*}
To compute this matrix, first note
\begin{align}
    \frac{1}{\sigma^2}\boldsymbol{W_i}^T\boldsymbol{W_i} &= \frac{1}{\sigma^2}\begin{bmatrix}
    1 & \cdots & 1 \\ 
    \sin\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \sin\left(\frac{\pi X_{i,n}}{12}\right) \\ 
    \cos\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \cos\left(\frac{\pi X_{i,n}}{12}\right)   \end{bmatrix} 
    \begin{bmatrix}1 & \sin\left(\frac{\pi X_{i,1}}{12}\right) & \cos\left(\frac{\pi X_{i,1}}{12}\right) \\
    \vdots & \vdots & \vdots \\
    1 & \sin\left(\frac{\pi X_{i,n}}{12}\right) & \cos\left(\frac{\pi X_{i,n}}{12}\right)
    \end{bmatrix} \nonumber \\
    &= \frac{1}{\sigma^2}
    \begin{bmatrix} n & \sum_{j=1}^n\sin(X_{i,1}) & \sum_{j=1}^n\cos(X_{i,1}) \\
    \sum_{j=1}^n\sin(X_{i,1}) & \sum_{j=1}^n\sin^2(X_{i,1}) & \sum_{j=1}^n\sin(X_{i,1})\cos(X_{i,1}) \\
    \sum_{j=1}^n\cos(X_{i,1}) & \sum_{j=1}^n\sin(X_{i,1})\cos(X_{i,1}) & \sum_{j=1}^n\cos^2(X_{i,1})
    \end{bmatrix} \nonumber \\
    &= \frac{n}{\sigma^2}\begin{bmatrix}1 & 0 & 0 \\
    0 & \frac{1}{2} & 0 \\
    0 & 0 & \frac{1}{2}
    \end{bmatrix}, \label{eq:lem_2_1}
\end{align}
where (\ref{eq:lem_2_1}) is due to orthogonality of distinct terms in a trigonometric basis \citep[Lemma 1.7]{Tsybakov2009}. It follows that
\begin{align}
    \left(\frac{1}{\sigma^2}\boldsymbol{W_i}^T\boldsymbol{W_i}+\Psi^{-1}\right)^{-1} &= \left( \begin{bmatrix}\frac{n}{\sigma^2} & 0 & 0 \\
    0 & \frac{n}{2\sigma^2} & 0 \\
    0 & 0 & \frac{n}{2\sigma^2}
    \end{bmatrix} + \begin{bmatrix} \frac{1}{\psi_1} & 0 & 0 \\
    0 & \frac{1}{\psi_2} & 0 \\
    0 & 0 & \frac{1}{\psi_3}
    \end{bmatrix}\right)^{-1} \nonumber \\
    &= \begin{bmatrix}\frac{\psi_1\sigma^2}{n\psi_1+\sigma^2} & 0 & 0 \\
    0 & \frac{2\psi_2\sigma^2}{n\psi_2+2\sigma^2} & 0 \\
    0 & 0 &  \frac{2\psi_3\sigma^2}{n\psi_3+2\sigma^2} 
    \end{bmatrix}. \label{eq:comp_1}
\end{align}
The identity in (\ref{eq:comp_1}) can be utilized to obtain
\begin{align*}
\boldsymbol{W_i}\left(\frac{1}{\sigma^2}\boldsymbol{W_i}^T\boldsymbol{W_i}+\Psi^{-1}\right)^{-1}\boldsymbol{W_i}^T 
&= \begin{bmatrix}1 & \sin\left(\frac{\pi X_{i,1}}{12}\right) & \cos\left(\frac{\pi X_{i,1}}{12}\right) \\
    \vdots & \vdots & \vdots \\
    1 & \sin\left(\frac{\pi X_{i,n}}{12}\right) & \cos\left(\frac{\pi X_{i,n}}{12}\right) \end{bmatrix}\begin{bmatrix}\frac{\psi_1\sigma^2}{n\psi_1+\sigma^2} & 0 & 0 \\
    0 & \frac{2\psi_2\sigma^2}{n\psi_2+2\sigma^2} & 0 \\
    0 & 0 &  \frac{2\psi_3\sigma^2}{n\psi_3+2\sigma^2}
    \end{bmatrix} \\
    & \quad \quad \times \begin{bmatrix}
    1 & \cdots & 1 \\ 
    \sin\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \sin\left(\frac{\pi X_{i,n}}{12}\right) \\ 
    \cos\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \cos\left(\frac{\pi X_{i,n}}{12}\right)   \end{bmatrix} \\
    &= \begin{bmatrix}\frac{\psi_1\sigma^2}{n\psi_1+\sigma^2} & \frac{2\psi_2\sigma^2}{n\psi_2+2\sigma^2}\sin\left(\frac{\pi X_{i,1}}{12}\right) & \frac{2\psi_3\sigma^2}{n\psi_3+2\sigma^2}\cos\left(\frac{\pi X_{i,1}}{12}\right) \\
    \vdots & \vdots & \vdots \\
    \frac{\psi_1\sigma^2}{n\psi_1+\sigma^2} & \frac{2\psi_2\sigma^2}{n\psi_2+2\sigma^2}\sin\left(\frac{\pi X_{i,n}}{12}\right) & \frac{2\psi_3\sigma^2}{n\psi_3+2\sigma^2}\cos\left(\frac{\pi X_{i,n}}{12}\right) \end{bmatrix} \\
    & \quad \quad \times \begin{bmatrix}
    1 & \cdots & 1 \\ 
    \sin\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \sin\left(\frac{\pi X_{i,n}}{12}\right) \\ 
    \cos\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \cos\left(\frac{\pi X_{i,n}}{12}\right)   \end{bmatrix},
\end{align*}
which implies
\begin{align*}
\left\{\boldsymbol{W_i}\left(\frac{1}{\sigma^2}\boldsymbol{W_i}^T\boldsymbol{W_i}+\Psi^{-1}\right)^{-1}\boldsymbol{W_i}^T \right\}_{j,k} &= \frac{\psi_1\sigma^2}{n\psi_1+\sigma^2} \\
& \quad \quad + \frac{2\psi_2\sigma^2}{n\psi_2+2\sigma^2}\sin\left(\frac{\pi X_{i,j}}{12}\right)\sin\left(\frac{\pi X_{i,k}}{12}\right) \\
& \quad \quad + \frac{2\psi_3\sigma^2}{n\psi_3+2\sigma^2}\cos\left(\frac{\pi X_{i,j}}{12}\right)\cos\left(\frac{\pi X_{i,k}}{12}\right).
\end{align*}
To conclude,
\begin{align*}
 \left(\boldsymbol{V_i}^{-1}\right)_{j,k} &= \left\{(\sigma^2I_{n} + \boldsymbol{W_i}\Psi \boldsymbol{W_i}^T)^{-1}\right\}_{j,k} \\
    &= \left\{\frac{1}{\sigma^2}I_{n} - \frac{1}{\sigma^4}\boldsymbol{W_i}\left(\frac{1}{\sigma^2}\boldsymbol{W_i}^T\boldsymbol{W_i}+\Psi^{-1}\right)^{-1}\boldsymbol{W_i}^T\right\}_{j,k} \\
    &= \frac{\mathbbm{1}\{j=k\}}{\sigma^2} \\
    &\quad \quad - \frac{\psi_1}{\sigma^2(n\psi_1+\sigma^2)} \\
    & \quad \quad - \frac{2\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\sin\left(\frac{\pi X_{i,j}}{12}\right)\sin\left(\frac{\pi X_{i,k}}{12}\right) \\
    &\quad \quad - \frac{2\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\cos\left(\frac{\pi X_{i,j}}{12}\right)\cos\left(\frac{\pi X_{i,k}}{12}\right).
\end{align*}
\end{proof}
\begin{lemma} \label{lem:3}
Suppose $[m^{(g)}_0, c^{(g)}_1, c^{(g)}_2]\sim P^{(g)}$ such that probability density function of $P^{(g)}$, denoted as $\rho(m^{(g)}_0, c^{(g)}_1, c^{(g)}_2)$, is symmetric with a mean of zero. Then
\begin{align*}
    \mathbb{E}(Y^{(g)} \mid X_{i,j}) &= \mu^{(g)}_0 + \phi_{c^{(g)}_2}(1)\theta^{(g)}_1\left\{- \sin(\theta^{(g)}_2)\sin\left(\frac{\pi X_{i,j}}{12} \right) + \cos(\theta^{(g)}_2)\cos\left(\frac{\pi X_{i,j}}{12} \right)\right\}.
\end{align*}
\end{lemma}
\begin{proof}
The superscript $(g)$ is omitted. The result follows from the derivation of Theorem 1 in \cite{Gorczyca2023}, where we find
\begin{align}
\mathbb{E}(Y_{i,j} \mid X_{i,j}) &= \mathbb{E}\left\{\mu_0 + m_{i,0} +  (\theta_1+c_{i,1})\cos\left(\frac{\pi X_{i,j}}{12} + \theta_2 + c_{i,2}\right)+\epsilon\right\} \nonumber \\
&= \mathbb{E}\left\{\mu_0 +  (\theta_1+c_{i,1})\cos\left(\frac{\pi X_{i,j}}{12} + \theta_2 + c_2\right)\right\} \nonumber \\
&= \mu_0 + \theta_1\mathbb{E}\left\{\cos\left(\frac{\pi X_{i,j}}{12} + \theta_2 + c_2\right)\right\} + \mathbb{E}\left\{c_1\cos\left(\frac{\pi X_{i,j}}{12} + \theta_2 + c_2\right)\right\} \label{eq:lem_1_app} \\
&= \mu_0 + \theta_1\mathbb{E}\left\{\cos\left(\frac{\pi X_{i,j}}{12} + \theta_2 + c_2\right)\right\} \nonumber \\
&= \mu_0 + \theta_1\cos\left(\frac{\pi X_{i,j}}{12} + \theta_2\right)\phi_{c_2}(1) \label{eq:four} \\
&= \mu_0 + \phi_{c_2}(1)\theta_1\left\{- \sin(\theta_2)\sin\left(\frac{\pi X_{i,j}}{12} \right) + \cos(\theta_2)\cos\left(\frac{\pi X_{i,j}}{12} \right)\right\}. \label{eq:id_app}
\end{align}
Here, (\ref{eq:lem_1_app}) is due to Lemma \ref{lem:1}; (\ref{eq:four}) is attributed to Euler's formula, which yields $\phi_{c_{2}}(t) = \mathbb{E}\{\cos(tc_2)\}$ when the probability density function for $c_{2}$ is symmetric with mean zero; and (\ref{eq:id_app}) is by application of the identities in (\ref{eq:alt_to_orig}).
\end{proof}

\section{Derivation of Proposition \ref{prop:1}} \label{app:B}
\begin{proof}
The superscript $(g)$ is again omitted to simplify presentation. The expected parameter estimates are first obtained. Given that $\boldsymbol{V_i}$ is known, estimation of $\beta$ is analogous to solving the normal equation for generalized least squares estimation, or identifying the quantity $\hat{\beta}$ that satisfies the equality
\begin{align*}
    \left(\sum_{i=1}^M\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\boldsymbol{W_i}\right)\hat{\beta} - \left(\sum_{i=1}^M\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\boldsymbol{Y_i}\right) &= 0,
\end{align*}
which under expectation is expressed as
\begin{align*}
    \left(\sum_{i=1}^M\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\boldsymbol{W_i}\right)\mathbb{E}(\hat{\beta}) - \left\{\sum_{i=1}^M\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i} \mid \boldsymbol{X_i})\right\} &= 0
\end{align*}
given $\boldsymbol{V_i}$, $\boldsymbol{W_i}$, and $\boldsymbol{X_i}$ are non-random quantities. First, note that application of Lemma \ref{lem:2} yields
\begin{align*}
    \left(\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\right)_{1,k} &= \frac{1}{\sigma^2} - \sum_{k=1}^n\left\{\frac{\psi_1}{\sigma^2(n\psi_1+\sigma^2)} \right. \\
    & \left. \quad \quad - \frac{2\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\sin\left(\frac{\pi X_{i,j}}{12}\right)\sin\left(\frac{\pi X_{i,k}}{12}\right) \right. \\
    &\left. \quad \quad - \frac{2\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\cos\left(\frac{\pi X_{i,j}}{12}\right)\cos\left(\frac{\pi X_{i,k}}{12}\right)\right\} \\
    &= \frac{1}{\sigma^2} - \frac{n\psi_1}{\sigma^2(n\psi_1+\sigma^2)}
\end{align*}
for elements in the first row of $\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}$,
\begin{align*}
    \left(\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\right)_{2,k} &= \left[\frac{1}{\sigma^2} - \sum_{k=1}^n \left\{\frac{\psi_1}{\sigma^2(n\psi_1+\sigma^2)} \right.  \right. \\
    & \left. \left. \quad \quad + \frac{2\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\sin\left(\frac{\pi X_{i,j}}{12}\right)\sin\left(\frac{\pi X_{i,k}}{12}\right) \right. \right. \\
    & \left. \left. \quad \quad + \frac{2\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\cos\left(\frac{\pi X_{i,j}}{12}\right)\cos\left(\frac{\pi X_{i,k}}{12}\right)\right\}\right]\sin\left( \frac{\pi X_{i,k}}{12}\right) \\
    &= \left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\}\sin\left(\frac{\pi X_{i,j}}{12}\right)
\end{align*}
for elements in the second row of $\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}$, and
\begin{align*}
    \left(\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\right)_{3,k} &= \left[\frac{1}{\sigma^2} -\sum_{k=1}^n\left\{ \frac{\psi_1}{\sigma^2(n\psi_1+\sigma^2)} \right. \right. \\
    & \left. \left. \quad \quad + \frac{2\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\sin\left(\frac{\pi X_{i,j}}{12}\right)\sin\left(\frac{\pi X_{i,k}}{12}\right) \right. \right. \\
    & \left. \left. \quad \quad + \frac{2\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\cos\left(\frac{\pi X_{i,j}}{12}\right)\cos\left(\frac{\pi X_{i,k}}{12}\right)\right\}\right]\cos\left( \frac{\pi X_{i,k}}{12}\right) \\
    &= \left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}\cos\left(\frac{\pi X_{i,j}}{12}\right)
\end{align*}
for elements in the third row of $\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}$. From these three expressions, it follows that
\begin{align}
\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\boldsymbol{W_i} &= \begin{bmatrix} \frac{1}{\sigma^2} - \frac{n\psi_1}{\sigma^2(n\psi_1+\sigma^2)} & \cdots & \frac{1}{\sigma^2} - \frac{n\psi_1}{\sigma^2(n\psi_1+\sigma^2)} \\
 \left\{\frac{1}{\sigma^2} - \frac{\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\}\sin\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \left\{\frac{1}{\sigma^2} - \frac{\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\}\sin\left(\frac{\pi X_{i,n}}{12}\right) \\
 \left\{\frac{1}{\sigma^2} - \frac{\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}\cos\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \left\{\frac{1}{\sigma^2} - \frac{\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}\cos\left(\frac{\pi X_{i,n}}{12}\right)
\end{bmatrix} \nonumber \\
& \quad \quad \times 
\begin{bmatrix}1 & \sin\left(\frac{\pi X_{i,1}}{12}\right) & \cos\left(\frac{\pi X_{i,1}}{12}\right) \\
    \vdots & \vdots & \vdots \\
    1 & \sin\left(\frac{\pi X_{i,n}}{12}\right) & \cos\left(\frac{\pi X_{i,n}}{12}\right) 
    \end{bmatrix} \nonumber \\
    &= \begin{bmatrix}\frac{n}{\sigma^2} - \frac{n^2\psi_1}{\sigma^2(n\psi_1+\sigma^2)} & 0 & 0 \\
    0 & \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\} & 0 \\
    0 & 0 & \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}  \end{bmatrix}, \label{ortho_2}
\end{align}
where (\ref{ortho_2}) is due to orthogonality of distinct terms in a trigonometric basis noted in Lemma \ref{lem:2} \citep[Lemma 1.7]{Tsybakov2009}. Now, by application of Lemma \ref{lem:3}, we find
\begin{align}
\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i}\mid \boldsymbol{X_i}) 
&= \begin{bmatrix} \frac{1}{\sigma^2} - \frac{n\psi_1}{\sigma^2(n\psi_1+\sigma^2)} & \cdots & \frac{1}{\sigma^2} - \frac{n\psi_1}{\sigma^2(n\psi_1+\sigma^2)} \\
 \left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\}\sin\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\}\sin\left(\frac{\pi X_{i,n}}{12}\right)\\
 \left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}\cos\left(\frac{\pi X_{i,1}}{12}\right) & \cdots & \left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}\cos\left(\frac{\pi X_{i,n}}{12}\right)
\end{bmatrix} \nonumber \\
&\quad \quad \times 
\begin{bmatrix}
\mu_0 + \phi_{c_2}(1)\theta_1\left\{- \sin(\theta_2)\sin\left(\frac{\pi X_{i,1}}{12} \right) + \cos(\theta_2)\cos\left(\frac{\pi X_{i,1}}{12} \right)\right\} \\
\vdots \\
\mu_0 + \phi_{c_2}(1)\theta_1\left\{- \sin(\theta_2)\sin\left(\frac{\pi X_{i,n}}{12} \right) + \cos(\theta_2)\cos\left(\frac{\pi X_{i,n}}{12} \right)\right\}
\end{bmatrix}, \nonumber 
\end{align}
which yields
\begin{align*}
    \left\{\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i}\mid \boldsymbol{X_i})\right\}_{1,1} &= \left\{\frac{1}{\sigma^2} - \frac{n\psi_1}{\sigma^2(n\psi_1+\sigma^2)}\right\} \\
    & \quad \quad \times \sum_{j=1}^n\left[ \mu_0  + \phi_{c_2}(1)\theta_1\left\{- \sin(\theta_2)\sin\left(\frac{\pi X_{i,j}}{12} \right) \right. \right.  \\
    & \quad \quad \quad \quad \left. \left. + \cos(\theta_2)\cos\left(\frac{\pi X_{i,j}}{12} \right)\right\}\right] \\
    &= \mu_0\left\{\frac{n}{\sigma^2} - \frac{n^2\psi_1}{\sigma^2(n\psi_1+\sigma^2)}\right\}
\end{align*}
for the first element of $\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i}\mid \boldsymbol{X_i})$,
\begin{align*}
    \left\{\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i}\mid \boldsymbol{X_i})\right\}_{2,1} &= \left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\} \\
    & \quad \quad \times \sum_{j=1}^n\sin\left(\frac{\pi X_{i,j}}{12}\right)\left[ \mu_0 + \phi_{c_2}(1)\theta_1\left\{- \sin(\theta_2)\sin\left(\frac{\pi X_{i,j}}{12} \right) \right. \right. \\
    &\left. \left. \quad \quad \quad \quad + \cos(\theta_2)\cos\left(\frac{\pi X_{i,j}}{12} \right)\right\} \right] \\
    &= -\frac{n\phi_{c_2}(1)\theta_1\sin(\theta_2)}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\}
\end{align*}
for the second element of $\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i}\mid \boldsymbol{X_i})$, and
\begin{align*}
    \left\{\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i}\mid \boldsymbol{X_i})\right\}_{3,1} &= \left\{\frac{1}{\sigma^2} - \frac{\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\} \\
    & \quad \quad \times \sum_{j=1}^n\cos\left(\frac{\pi X_{i,j}}{12}\right)\left[ \mu_0 + \phi_{c_2}(1)\theta_1\left\{- \sin(\theta_2)\sin\left(\frac{\pi X_{i,j}}{12} \right) \right. \right. \\
    &\left. \left. \quad \quad \quad \quad + \cos(\theta_2)\cos\left(\frac{\pi X_{i,j}}{12} \right)\right\} \right] \\
    &= \frac{n\phi_{c_2}(1)\theta_1\cos(\theta_2)}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}
\end{align*}
for the third element of $\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i}\mid \boldsymbol{X_i})$. The expected fixed parameter estimates can subsequently be expressed as
\begin{align}
    \mathbb{E}(\hat{\beta}) &= \begin{bmatrix}
    \mathbb{E}(\hat{\mu}_0) \\
    \mathbb{E}(\hat{\beta}_1) \\
    \mathbb{E}(\hat{\beta}_2)
    \end{bmatrix} \nonumber \\
    &= \left(\sum_{i=1}^M\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\boldsymbol{W_i}\right)^{-1} \left(\sum_{i=1}^M\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\mathbb{E}(\boldsymbol{Y_i} \mid \boldsymbol{X_i})\right) \nonumber \\
    &= \left(M\begin{bmatrix}\frac{n}{\sigma^2} - \frac{n^2\psi_1}{\sigma^2(n\psi_1+\sigma^2)} & 0 & 0 \\
    0 & \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\} & 0 \\
    0 & 0 & \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}  \end{bmatrix}\right)^{-1} 
\nonumber \\
& \quad \quad \times M\begin{bmatrix} \mu_0\left\{\frac{n}{\sigma^2} - \frac{n^2\psi_1}{\sigma^2(n\psi_1+\sigma^2)}\right\}   \\
  -\frac{n\phi_{c_2}(1)\theta_1\sin(\theta_2)}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\} \\
 \frac{n\phi_{c_2}(1)\theta_1\cos(\theta_2)}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}
\end{bmatrix} \nonumber \\
    &= \begin{bmatrix}
    \mu_0 \\
    -\phi_{c_2}(1)\theta_1\sin(\theta_2) \\
    \phi_{c_2}(1)\theta_1\cos(\theta_2)
    \end{bmatrix} \nonumber \\
    &= \begin{bmatrix}
    \mu_0 \\
    \phi_{c_2}(1)\beta_1 \\
    \phi_{c_2}(1)\beta_2
    \end{bmatrix}, \label{eq:eq_3_res}
\end{align}
where (\ref{eq:eq_3_res}) is due to the identities in (\ref{eq:alt_to_orig}).

Consideration is now given towards computing the $\tau$ in (\ref{eq:wald}) given these parameter estimates. First, note that
\begin{align*}
L(\hat{\beta} - \hat{\beta}_{\text{Null}}) &= \begin{bmatrix}
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}\left(\begin{bmatrix}
    \mu_0 \\
    \phi_{c_2}(1)\beta_1 \\
    \phi_{c_2}(1)\beta_2
    \end{bmatrix} - \begin{bmatrix}
    0 \\
    0 \\
    0
    \end{bmatrix} \right) \\
    &= \begin{bmatrix}
    \phi_{c_2}(1)\beta_1 \\
    \phi_{c_2}(1)\beta_2
    \end{bmatrix}.
\end{align*}
Further,
\begin{align*}
    L\left\{\sum_{i=1}^M\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\boldsymbol{W_i}\right\}^{-1}L^T &= M^{-1}\begin{bmatrix}
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix} \\
    & \quad \quad \times \begin{bmatrix}\frac{n}{\sigma^2} - \frac{n^2\psi_1}{\sigma^2(n\psi_1+\sigma^2)} & 0 & 0 \\
    0 & \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\} & 0 \\
    0 & 0 & \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}  \end{bmatrix}^{-1} \\
    & \quad \quad \times \begin{bmatrix}
        0 & 0 \\
        1 & 0 \\
        0 & 1
    \end{bmatrix} \\
    &= M^{-1}\begin{bmatrix} \frac{2}{n}\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\}^{-1} & 0 \\
     0 & \frac{2}{n}\left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}^{-1}  \end{bmatrix},
\end{align*}
which implies that
\begin{align*}
\left[L\left\{\sum_{i=1}^M\boldsymbol{W_i}^T\boldsymbol{V_i}^{-1}\boldsymbol{W_i}\right\}^{-1}L^T\right]^{-1} &= M\begin{bmatrix} \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\} & 0 \\
     0 & \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}  \end{bmatrix}.
\end{align*}
To conclude, the Wald test statistic can be expressed as
\begin{align*}
    \tau &= \frac{M}{2}\begin{bmatrix}
    \phi_{c_2}(1)\beta_1 &
    \phi_{c_2}(1)\beta_2
    \end{bmatrix}\begin{bmatrix} \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\} & 0 \\
     0 & \frac{n}{2}\left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}  \end{bmatrix}\begin{bmatrix}
    \phi_{c_2}(1)\beta_1 \\
    \phi_{c_2}(1)\beta_2
    \end{bmatrix} \\
    &= \frac{Mn\phi^2_{c_2}(1)}{4}\left[\left\{\frac{1}{\sigma^2} - \frac{n\psi_2}{\sigma^2(n\psi_2+2\sigma^2)}\right\}\beta_1^2 + \left\{\frac{1}{\sigma^2} - \frac{n\psi_3}{\sigma^2(n\psi_3+2\sigma^2)}\right\}\beta_2^2 \right].
\end{align*}

\end{proof}

\section{Derivation of Proposition \ref{prop:2}} \label{app:C}
\begin{proof}
Omitting the superscript $(g)$, the response $Y_{i,j}$ can be expressed as
\begin{align}
Y_{i,j} &= (\mu_0 + m_{i, 0}) + \theta_1\cos(X_{i,j}+\theta_{2}) + c_{i,1}\cos(X_{i,j}+c_{i,2}) + \epsilon_{i,j} \nonumber \\
&= (\mu_0 + m_{i, 0}) -\theta_1\sin(\theta_2)\sin(X_{i,j}) + \theta_1\cos(\theta_2)\cos(X_{i,j}) \nonumber \\
& \quad \quad - c_{i,1}\sin(c_{i,2})\sin(X_{i,j}) + c_{i,1}\cos(c_{i,2})\cos(X_{i,j}) + \epsilon_{i,j} \nonumber \\
&= (\mu_0 + m_{i, 0}) -\left\{\theta_1\sin(\theta_2) + c_{i,1}\sin(c_{i,2})\right\}\sin(X_{i,j}) \nonumber \\
& \quad \quad + \left\{\theta_1\cos(\theta_2) + c_{i,1}\cos(c_{i,2})\right\}\cos(X_{i,j}) + \epsilon_{i,j} \nonumber \\ 
&= (\mu_0 + m_{i, 0}) + \sqrt{\left\{\theta_1\sin(\theta_2) + c_{i,1}\sin(c_{i,2})\right\}^2 + \left\{\theta_1\cos(\theta_2) + c_{i,1}\cos(c_{i,2})\right\}^2} \nonumber \\
    & \quad \quad \times \cos\left[\frac{\pi X_{i,j}}{12} + \atan\{-\theta_1\sin(\theta_2) - c_{i,1}\sin(c_{i,2}), \theta_1\cos(\theta_2) + c_{i,1}\cos(c_{i,2})\}\right] \nonumber \\
    & \quad \quad + \epsilon_{i,j} \label{eq:res_p2},
\end{align}
where (\ref{eq:res_p2}) is due to the identities presented in (\ref{eq:alt_to_orig}).

\end{proof}

\section{Derivation of Amplitude Variance} \label{app:D}
Define $h(\mu_0, \beta_1, \beta_2) = \sqrt{\beta_1^2+\beta_2^2}$, which implies
\begin{align*}
    \nabla h(\mu_0, \beta_1, \beta_2) &= \begin{bmatrix} 0 & \frac{\beta_1}{\sqrt{\beta_1^2+\beta_2^2}} & \frac{\beta_2}{\sqrt{\beta_1^2+\beta_2^2}}
    \end{bmatrix}.
\end{align*}
Further, let $\hat{\Sigma}$ denote the estimated covariance matrix for the parameter vector $[\hat{\mu}_0, \hat{\beta}_1, \hat{\beta}_2]^T$. By application of the Delta method \citep[Theorem 1.3]{Boos2013},
\begin{align*}
    \mathrm{Var}(\hat{\theta}_1) &= \begin{bmatrix} 0 & \frac{\hat{\beta}_1}{\sqrt{\hat{\beta}_1^2+\hat{\beta}_2^2}} & \frac{\hat{\beta}_2}{\sqrt{\hat{\beta}_1^2+\hat{\beta}_2^2}}
    \end{bmatrix} \begin{bmatrix}
    \hat{\Sigma}_{1,1} & \hat{\Sigma}_{1,2} & \hat{\Sigma}_{1,3} \\
    \hat{\Sigma}_{1,2} & \hat{\Sigma}_{2,2} & \hat{\Sigma}_{2,3} \\
    \hat{\Sigma}_{1,3} & \hat{\Sigma}_{2,3} & \hat{\Sigma}_{3,3}
    \end{bmatrix} \begin{bmatrix} 0 \\ \frac{\hat{\beta}_1}{\sqrt{\hat{\beta}_1^2+\hat{\beta}_2^2}} \\ \frac{\hat{\beta}_2}{\sqrt{\hat{\beta}_1^2+\hat{\beta}_2^2}}
    \end{bmatrix}  \\
    &= \frac{\hat{\Sigma}_{2,2}\hat{\beta}_1^2+\hat{\Sigma}_{3,3}\hat{\beta}_2^2 + 2\hat{\Sigma}_{2,3}\hat{\beta}_1\hat{\beta}_2}{\hat{\beta}_1^2+\hat{\beta}_2^2}.
\end{align*}

\section{Overview of Simulation Study Design from Section \ref{sec:3}} \label{app:E}
We present the following six simulation settings, where the names in parentheses are from the BioCycle$_{\text{Form}}$ software:
\begin{description}
    \item[Setting 1 (Cosine).] $\mu^{(g)}_0=6$, $\theta^{(g)}_1 = 0.3$, $\theta^{(g)}_2 = 0$, $m^{(g)}_{i,0} \sim \mathrm{N}(0, 1)$, $c^{(g)}_{i,1} \sim \mathrm{TN}(0, 0.5, -0.3, 0.3)$, $c^{(g)}_{i,2} \sim \mathrm{TN}(0, \pi^2/36, -\pi, \pi)$, $M = 10$, $n_i = 24$, $X_{i,j}=2j$, $\epsilon^{(g)}_{i,j} \sim \text{N}(0, 0.25)$, and $$Y^{(g)}_{i,j} = (\mu^{(g)}_0+m^{(g)}_{i,0}) + (\theta^{(g)}_1+c^{(g)}_{i,1})\cos\left(\frac{\pi X_{i,j}}{12} + \theta^{(g)}_2 + c^{(g)}_{i,2} \right)+\epsilon^{(g)}_{i,j}.$$  
    \item[Setting 2 (Cosine + Outlier).] $\mu^{(g)}_0=6$, $\theta^{(g)}_1 = 0.3$, $\theta^{(g)}_2 = \pi/6$, $m^{(g)}_{i,0} \sim \mathrm{N}(0, 1)$, $c^{(g)}_{i,1} \sim \mathrm{TN}(0, 0.5, -0.3, 0.3)$, $c^{(g)}_{i,2} \sim \mathrm{TN}(0, \pi^2/36, -\pi, \pi)$, $M = 10$, $n_i = 16$, $X_{i,j}=3j$, $\epsilon^{(g)}_{i,j} \sim \text{N}(0, 0.25)$, $P_{i,j} = \mathrm{Unif}(0, 1)$, and $$Y^{(g)}_{i,j} = g(P_{i,j})\left\{(\mu^{(g)}_0+m^{(g)}_{i,0}) + (\theta^{(g)}_1+c^{(g)}_{i,1})\cos\left(\frac{\pi X_{i,j}}{12} + \theta^{(g)}_2 + c^{(g)}_{i,2} \right)+\epsilon^{(g)}_{i,j}\right\},$$where $$g(P_{i,j}) =  \begin{cases} 
      1 & P_{i,j} \leq 0.95, \\
      1.5 & \text{otherwise.}
   \end{cases}
$$
    \item[Setting 3 (Cosine2).] $\mu^{(g)}_0=6$, $\theta^{(g)}_1 = 0.3$, $\theta^{(g)}_2 = \pi/3$, $m^{(g)}_{i,0} \sim \mathrm{N}(0, 1)$, $c^{(g)}_{i,1} \sim \mathrm{TN}(0, 0.5, -0.3, 0.3)$, $c^{(g)}_{i,2} \sim \mathrm{TN}(0, \pi^2/16, -\pi, \pi)$, $M = 10$, $n_i = 12$, $X_{i,j}=4j$, $\epsilon^{(g)}_{i,j} \sim \text{N}(0, 0.25)$, and $$Y^{(g)}_{i,j} = (\mu^{(g)}_0+m^{(g)}_{i,0}) + (\theta^{(g)}_1+c^{(g)}_{i,1})\left\{\cos\left(\frac{\pi X_{i,j}}{12} + \theta^{(g)}_2 + c^{(g)}_{i,2} \right) + \frac{\cos\left(\frac{\pi X_{i,j}}{4} + \frac{\pi}{2} + \theta^{(g)}_2 + c^{(g)}_{i,2} \right)}{2}\right\}+\epsilon^{(g)}_{i,j}.$$
    \item[Setting 4 (Peak).] $\mu^{(g)}_0=6$, $\theta^{(g)}_1 = 0.3$, $\theta^{(g)}_2 = \pi/2$, $m^{(g)}_{i,0} \sim \mathrm{N}(0, 1)$, $c^{(g)}_{i,1} \sim \mathrm{TN}(0, 0.5, -0.3, 0.3)$, $c^{(g)}_{i,2} \sim \mathrm{TN}(0, \pi^2/16, -\pi, \pi)$, $M = 10$, $n_i = 24$, $X_{i,j}=2j$, $\epsilon^{(g)}_{i,j} \sim \text{N}(0, 0.25)$, and $$Y_{i,j} = (\mu^{(g)}_0+m^{(g)}_{i,0}) + (\theta^{(g)}_1+c^{(g)}_{i,1})\left[-1+2\left\{\cos\left(\frac{\pi X_{i,j}}{12} + \theta^{(g)}_2 + c^{(g)}_{i,2} \right)\right\}^{10} \right]+\epsilon^{(g)}_{i,j}.$$
    \item[Setting 5 (Triangle).] $\mu^{(g)}_0=6$, $\theta^{(g)}_1 = 0.3$, $\theta^{(g)}_2 = 2\pi/3$, $m^{(g)}_{i,0} \sim \mathrm{N}(0, 1)$, $c^{(g)}_{i,1} \sim \mathrm{TN}(0, 0.5, -0.3, 0.3)$, $c^{(g)}_{i,2} \sim \mathrm{TN}(0, \pi^2/9, -\pi, \pi)$, $M = 10$, $n_i = 16$, $X_{i,j}=3j$, $\epsilon^{(g)}_{i,j} \sim \text{N}(0, 0.25)$, and 
    \begin{align*}
        Y^{(g)}_{i,j} &= (\mu^{(g)}_0+m^{(g)}_{i,0}) \\
        & \quad + \frac{8(\theta^{(g)}_1+c^{(g)}_{i,1})}{\pi^2}\left\{\cos\left(\frac{\pi X_{i,j}}{12} + \theta^{(g)}_2 + c^{(g)}_{i,2} \right) \right. \\
        & \quad \quad \quad \left. - \frac{\cos\left\{3\left(\frac{\pi X_{i,j}}{12} +  \theta^{(g)}_2 + c^{(g)}_{i,2} \right)\right\}}{9} \right. \\
        & \quad \quad \quad \left. + \frac{\cos\left\{5\left(\frac{\pi X_{i,j}}{12} +  \theta^{(g)}_2 + c^{(g)}_{i,2} \right)\right\}}{25}\right\} \\
        & \quad +\epsilon^{(g)}_{i,j}.
    \end{align*}
    \item[Setting 6 (Square).] $\mu^{(g)}_0=6$, $\theta^{(g)}_1 = 0.3$, $\theta^{(g)}_2 = 5\pi/6$, $m^{(g)}_{i,0} \sim \mathrm{N}(0, 1)$, $c^{(g)}_{i,1} \sim \mathrm{TN}(0, 0.5, -0.3, 0.3)$, $c^{(g)}_{i,2} \sim \mathrm{TN}(0, \pi^2/9, -\pi, \pi)$, $M = 10$, $n_i = 12$, $X_{i,j}=4j$, $\epsilon^{(g)}_{i,j} \sim \text{N}(0, 0.25)$, and \begin{align*}
        Y^{(g)}_{i,j} &= (\mu^{(g)}_0+m^{(g)}_{i,0}) \\
        & \quad + \frac{8(\theta^{(g)}_1+c^{(g)}_{i,1})}{\pi^2}\left\{\cos\left(\frac{\pi X_{i,j}}{12} + \theta^{(g)}_2 + c^{(g)}_{i,2} \right) \right. \\
        & \quad \quad \quad \left. - \frac{\cos\left\{3\left(\frac{\pi X_{i,j}}{12} +  \theta^{(g)}_2 + c^{(g)}_{i,2} \right)\right\}}{3} \right. \\
        & \quad \quad \quad \left. + \frac{\cos\left\{5\left(\frac{\pi X_{i,j}}{12} +  \theta^{(g)}_2 + c^{(g)}_{i,2} \right)\right\}}{5}\right\} \\
        & \quad +\epsilon^{(g)}_{i,j}.
    \end{align*}
 \end{description}
Here, $\mathrm{N}(\mu, \sigma^2)$ denotes a normal distribution with mean $\mu$ and variance $\sigma^2$; $\mathrm{TN}(\mu, \sigma^2, a, b)$ a truncated normal distribution with mean $\mu$, variance $\sigma^2$, lower bound $a$, and upper bound $b$; and $\mathrm{Unif}(a,b)$ denotes a uniform distribution with support from $a$ to $b$. For every simulation setting, the true population amplitude parameter $\theta_1^{(g)} = 0.3$, which has been reported as being frequently observed across genes \citep{MllerLevet2013}. Further, for Settings 1 and 4, a sample is collected from each individual once every two hours over a 48 hour period; Settings 2 and 5 once every three hours over a 48 hour period; and Settings 3 and 6 once every four hours over a 48 hour period. A 48 hour period is considered given guidelines for circadian biology experimental design \citep{Hughes2017}.


\bibliographystyle{apalike} 
\bibliography{bibliography}

\end{document}