\begin{document}
\affiliation{$$_affiliation_$$}
\title{The modified conditional sum-of-squares estimator for fractionally integrated models\thanks{We are grateful to J\"{o}
\maketitle



\begin{abstract}
  \noindent In this paper, we analyse the influence of estimating a constant term on the bias of the conditional sum-of-squares (CSS) estimator in a stationary or non-stationary type-II ARFIMA ($p_1, d, p_2$) model.  We derive expressions
  for the estimator's bias and show that the leading term can be easily removed by a simple modification of the CSS objective function. We call this new estimator the modified conditional sum-of-squares (MCSS) estimator. We show
  theoretically and by means of Monte Carlo simulations that its performance relative to that of the CSS estimator is markedly improved even for small sample sizes. Finally, we revisit three classical short datasets that have in
  the past been described by ARFIMA($p_1$,$d$,$p_2$) models with constant term, namely the post-second World War real GNP data, the extended Nelson-Plosser data, and the Nile data.

  \medskip \noindent \textbf{Keywords:} long memory, fractional integration, conditional sum-of-squares estimator, asymptotic expansion, small sample bias.

  \medskip \noindent \textbf{JEL Codes:} C22.

\end{abstract}


\clearpage


\section{Introduction}


Fractionally integrated autoregressive moving average (ARFIMA) models are applied in a wide range of fields for describing long-memory phenomena, witness inter alia the economic and political as well as the natural sciences; see
\textcite{hassler2019time} and \textcite{hualde2021frac} for general treatments. One particular variant of this model class that has recently gained popularity is the so-called type-II ARFIMA model, which sets the initial observations
equal to zero and allows both stationary and non-stationary processes to be described, see for example \textcite{nielsen2004efficient}, \textcite{ robinson2005distance} and \textcite{johansen2008representation}. A popular choice for estimating this model is the conditional sum-of-squares (CSS) estimator whose
main appealing features are that it is computationally straightforward and that the memory parameter can be estimated consistently as long as it lies in an arbitrary compact interval on the real line.
It was introduced by \textcite{li1986fractional} in the context of stationary fractionally integrated models. Subsequent papers allowed for non-stationary models, see for instance \textcite{beran1995maximum} and
\textcite{velasco2000whittle}. Local consistency proofs were provided by \textcite{tanaka1999nonstationary}, \textcite{nielsen2004efficient} and \textcite{robinson2006conditional}. Global consistency was proved by \textcite{hualde2011gaussian} and
\textcite{nielsen2015asymptotics} in a model without deterministic components.  Only recently, \textcite{hualde2020truncated,hualde2021truncated} derived global consistency and the asymptotic normality of the CSS estimator in a
model with deterministic components, such as a constant or a trending term. Empirical applications include \textcite{hualde2011gaussian} for aggregate income and consumption data and \textcite{johansen2016role} for opinion
poll data.

While the literature dealing with asymptotic inferences in the context of parametric ARFIMA models is well-developed, some issues still require attention. One such concern pertains to the small sample performance of the CSS
estimator. Despite the widespread use of the CSS estimator little is currently known about the impact deterministic terms have on the properties of the estimator of the memory parameter in small samples.  Early on,
\textcite{chung1993small} and \textcite{cheung1994maximum} conducted simulation studies and found that the inclusion of a constant term in the model can substantially increase the small-sample bias and mean squared error (MSE) of the
estimated memory parameter. \textcite{lieberman2005expansions} and \textcite{johansen2016role} are among the few theoretical contributions to shed light on the issue. \textcite{lieberman2005expansions}
derive the Edgeworth expansion of the memory parameter for the Gaussian maximum likelihood estimator in stationary fractional time series model. \textcite{johansen2016role} investigate the impact of observed and unobserved initial
values on the bias of the memory parameter estimator in a non-stationary fractional time series model. Neither paper, however, includes short-run dynamics in its model. In addition, we are not aware of any related work that
simultaneously tackles both stationary and non-stationary processes.

The purpose of the present paper is therefore to add to this literature and analyse the small-sample bias of the CSS estimator in a type-II fractional model with short-run dynamics and constant term from an analytical, empirical
and simulation point of view. In particular, our analysis reveals that incorporating the level parameter into the model introduces an additional bias in the CSS estimator. This bias is due to a biased score which is particularly
pronounced when the data is stationary. We will suggest what we call the modified conditional sum-of-squares (MCSS) estimator which is easy to compute, removes the leading bias term and allows much more accurate small-sample
inference.

To do so, we will interpret the constant term as nuisance parameter and draw on a large literature on bias correction. \textcite{laskar1998modified} provide an overview of this literature. We build on the
approach to dealing with nuisance parameters initiated by \textcite{conniffe1987expected} and \textcite{mccullagh1990simple} and recently applied by \textcite{bartolucci2016modified} and by \textcite{martellosio2020adjusted}, i.e.\ we adjust the
score function so that its expectation equals zero. The idea is as follows: We find a stochastic higher-order expansion of the estimator as a function of the derivatives of the profile likelihood, cf.\ \textcite{johansen2016role,lawley1956general}. The expansion is simplified by
approximating the derivatives by their leading terms. This allows the expectation of the estimator to be taken. We notice that premultiplying the objective function by a modification term results in the expected score evaluated at
the true parameter to be equal to zero, thereby mitigating the bias of the estimator.

The main contributions of this paper to the literature are threefold: First, we examine our MCSS estimator in type-II ARFIMA($p_1$,$d$,$p_2$) models with constant term and compare it to the standard CSS estimator. In particular,
we derive its exact bias and we show that it is consistent and asymptotically normally distributed. For expositional clarity, our treatment starts by covering the type-II ARFIMA(0,$d$,0) model before treating the general
ARFIMA($p_1$,$d$,$p_2$) case. The results generate new insights into bias correction of other models nested in our setup, such as stationary and invertible ARMA($p_1$,$p_2$) models. Secondly, we re-visit three classical datasets
that have in the past been described by ARFIMA($p_1$,$d$,$p_2$) models with constant term, namely the post-second World War real GNP data, the extended Nelson-Plosser dataset, and the Nile data, by applying our MCSS estimator to
estimate the long-memory parameter and the short-run dynamics. All three time series are short and therefore warrant the use of small-sample bias corrections. Our conclusion sheds new light on the interpretation of these
datasets. Thirdly, this paper paves the way to extending the analysis of small-sample bias from univariate type-II ARFIMA modes to panel settings, see also the contributions of \textcite{robinson2015efficient} and
\textcite{schumann2023role}.

The remainder of the paper is organised as follows. In Section \ref{S2} we present the MCSS estimator for ARFIMA$(0,d,0)$ models. The extension to ARFIMA$(p_1,d,p_2)$ models is covered in Section \ref{secgen}.  Section 4 presents
the empirical illustrations. Section \ref{S5} contains  concluding remarks. All proofs  are relegated to the appendix.



\section{The modified conditional sum-of-squares estimator}\label{S2}

In this section, we introduce the modified conditional sum-of-squares estimator, designed for estimating the ARFIMA(0,$d$,0) model with a constant term. In Section \ref{secgen}, we will expand our analysis to incorporate short-run dynamics, for instance covering the ARFIMA$(p_1,d,p_2)$ model as a particular case. Covering the ARFIMA(0,$d$,0) model first serves two purposes: it allows a more straightforward explanation of our methodology, without the need for cumbersome notation, and affords a direct comparison with \textcite{lieberman2005expansions} and \textcite{johansen2016role}, both of which do not consider short-term dynamics. The theorems presented in this section are special cases of the theorems in Section \ref{secgen}, proofs of which are presented in the appendix.

We start with introducing the simple type-II ARFIMA(0,$d$,0) model with a constant term $\mu$ in Section \ref{model}.  Building upon this, Section \ref{subsect22} introduces the conditional sum-of-squares (CSS) estimator and
discusses its asymptotic properties, distinguishing between two scenarios: one where the constant parameter $\mu$ is either known or unknown. Subsequently, Section \ref{sectionmod} shifts our focus towards examining score biases
in the CSS estimators. It unveils a methodological approach to mitigating these biases: It discusses a well-established approach by \textcite{mccullagh1990simple} and demonstrates that, in our setting, its application poses
challenges. An adjustment of their approach resolves the issues. Section \ref{Smcss} introduces the modified conditional sum-of-squares (MCSS) estimator and delineates its asymptotic properties. In Section \ref{relwother}, we
discuss how our MCSS estimator aligns with alternative bias-reduction methodologies. Section \ref{S3} assesses the asymptotic biases of the estimators, with specific attention to the performance of the MCSS
estimator relative to the CSS estimator. Analytical expressions for these biases are derived, offering an understanding of their behaviour across distinct regions of $d$. Finally, in Section \ref{S4}, we conduct a
simulation study to examine the small sample properties of the estimators.



\subsection{The model}
\label{model}

Consider a so-called type II fractional process $z_t$, $t = 0,\pm 1,\pm 2,\ldots$, generated by the model
\begin{align}
    z_t = \Delta_+^{-d} \epsilon_t, \label{qq11} 
\end{align} 
where $\Delta = 1 - L$ and $L$ are the difference and lag operators, respectively, and where $d$ can take any value in $\mathbb{R}$. For any series $u_t$, real number $\zeta$ and time index $t \geq 1$, the so-called truncation
operator $\Delta_+^{\zeta}$ is defined by
\begin{align}
    \Delta_+^{\zeta} u_t  = \Delta^{\zeta} \{ u_t I(t \geq 1)  \} = \sum_{i = 0}^{t-1} \pi_{i}(-\zeta) u_{t-i},  \label{e33}
\end{align}
with $I(\cdot)$ being the indicator function, and with $\pi_{i}(a) = 0$ for $i<0$, $\pi_{0}(a) = 1$ as well as
\begin{align}
    \pi_{i}(a) = \frac{\Gamma(a+i)}{\Gamma(a)\Gamma(1+i)}= \frac{a (a+1) \ldots (a+i-1)}{i!}, \text{ for } i \geq 1, \label{fracpar}
\end{align}
denoting the coefficients in the usual binomial expansion of $\Delta^{-a} = \sum_{i = 0}^{\infty} \pi_i(a) z^i$.  $\Gamma(\cdot)$ is the gamma function with the convention that $\Gamma(i) = 0$ for $i = 0 ,-1,-2,\ldots$ and that
$\Gamma(0)/\Gamma(0) = 1$. The parameter $d$ in \eqref{qq11} is known as the memory parameter or the fractional parameter. The process $z_{t}$ has been widely applied in the literature, see
\textcite{marinucci2000weak,marinucci2001semiparametric}, \textcite{robinson2003cointegration}, \textcite{nielsen2004efficient}, \textcite{shimotsu2005exact}, \textcite{robinson2005distance} and
\textcite{johansen2008representation}, among others.

Two comments on the memory parameter are of interest: First, its range is commonly divided into a ``stationary'' and a ``non-stationary'' region: $d < 1/2$ and $d \geq 1/2$, respectively. Yet the definition in \eqref{e33} implies
that $z_t = 0$ for $t \leq 0$, which means that when $d < 1/2$ and $d \neq 0$, $z_t$ is in fact not covariance stationary. However, it may be considered asymptotically stationary for any such $d$. To see this, consider the
so-called type-I fractional process
\begin{align}
    \tilde{z}_t = \Delta^{-d} \epsilon_t \label{type1zt}
\end{align}
which is known to be covariance stationary for any $d < 1/2$. \textcite{marinucci1999alternative} observe that for $|d| < 1/2$,
\begin{align} 
    E\left(  z_t - \tilde{z}_t \right)^2 = O(t^{2d -1}),\ \ \text{as } t \rightarrow \infty, \label{MR1}
\end{align}
and hence the difference to $z_t$ vanishes. Although \textcite{marinucci1999alternative} consider only $|d| < 1/2$, \eqref{MR1} actually holds for any $d < 1/2$. This follows from Stirling's approximation and \textcite[Lemma
A.1]{johansen2016role}. This asymptotic equivalence prompts us to retain the terminological dichotomy between stationarity an non-stationarity.  Secondly, it is worth noting that even for $d \geq 1/2$, i.e.\ in the non-stationary
region, the truncation operator in \eqref{e33} ensures that the process $z_{t}$ is well-defined in the mean-square sense, see \textcite[Section A.4]{johansen2008representation} and \textcite{hualde2011gaussian}.
    
While the model in \eqref{qq11} covers a wide range of dynamics, it seems unsuitable for many empirical applications because it implies that $E(z_t) = 0$. Nonetheless, a fair amount of theoretical work considers exclusively a
purely random process, see for instance \textcite{hualde2011gaussian} and \textcite{nielsen2015asymptotics}. In order to make our model more widely applicable, we complement the model in \eqref{qq11} by a constant term $\mu$, to yield
\begin{align}
    x_t = \mu I(t\geq 1) + z_t \label{q1}
\end{align}
and hence $E(x_t) = \mu$ for $t \geq 1$. The level parameter $\mu$ has the added advantage of reducing the bias in the estimate of $d$ arising from the pre-sample behaviour of $x_t$, as shown by \textcite{johansen2016role} for $d > 1/2$. 

The model in \eqref{q1} is the well-known ARFIMA(0,$d$,0) model plus a level parameter. It is considered as a special case in \textcite{hualde2020truncated} and \textcite{hualde2021truncated}, both of which include short-run dynamics and a trending component in the model. We do not include a trend
component in our analysis, but a discussion to that effect is presented in Section \ref{S5}. We do extend our results, however, by adding short-run dynamics to \eqref{q1} in Section \ref{secgen}.


\subsection{The conditional sum-of-squares estimator} \label{subsect22}

We now discuss the conditional sum-of-squares (CSS) estimator of the parameters in model \eqref{q1}. This is the estimator considered by e.g.\ \textcite{hualde2011gaussian} who, however, look at a model without the constant term. We
distinguish the case in which $\mu$ is unknown from that in which it is known. As will be seen in Section \ref{sectionmod} below, the CSS estimator may also be motivated as a maximum likelihood estimator under the
assumption of Gaussian innovation terms $\epsilon_t$, as in \textcite{johansen2016role} and \textcite{hualde2020truncated}.

Following \textcite{johansen2016role}, we make the following assumptions on the model's error term and the admissible parameter space. True parameter values are denoted by the subscript 0.
\begin{assumption}\label{a1}
The errors $\epsilon_t$ are \textit{IID}(0,$\sigma_0^2$) with finite fourth moment.
\end{assumption}
\begin{assumption}\label{a2}
The parameter space for $(d,\mu)$ is $ \mathbb{D} \times \mathbb{R}$, where
$ \mathbb{D} = [\nabla_1,\nabla_2]$, $ -\infty < \nabla_1  <  \nabla_2 < \infty$. The true value $d_0$ is in the interior of $\mathbb{D}$ and not equal to $1/2$. 
\end{assumption}

For any $(d, \mu) \in \mathbb{D} \times \mathbb{R}$, define the residuals $\epsilon_t (d, \mu) =  \Delta_{+}^{d} (x_t- \mu )$. The CSS objective function is then given by
\begin{align}
  L (d, \mu) &= \frac{1}{2} \sum_{t = 1}^T \epsilon^2_t (d, \mu),  \nonumber \\
             &= \frac{1}{2} \sum_{t = 1}^T \left(  \Delta_{+}^{d} (x_t- \mu ) \right)^2. \label{css}
\end{align}
Since \eqref{css} is quadratic in $\mu$ we can concentrate it by writing
\begin{align*}
    \Delta_{+}^{d} (x_t-\mu ) &= \Delta_{+}^{d} x_t - \sum_{n = 0}^{t-1} \pi_n(-d) \mu, \\
    &= \Delta_{+}^{d} x_t - \pi_{t-1}(1-d) \mu = \Delta_{+}^{d} x_t - \kappa_{0t}(d)\mu,
\end{align*}
where 
\begin{align}
    \kappa_{0t}(d) &= \Delta_{+}^{d} I(t \geq 1), \nonumber\\
                   &=  \sum_{n = 0}^{t-1} \pi_n(-d) = \pi_{t-1}(1-d), \label{definitionkappa}
\end{align}
the last line following from \textcite[Lemma A.4]{johansen2016role}.  Unsurprisingly, the CSS estimator of $\mu$ for fixed $d$ is given by
\begin{align}
    \hat{\mu}(d) = \frac{\sum_{t = 1}^T (\Delta_{+}^{d} x_t)\kappa_{0t}(d)}{\sum_{t = 1}^T \kappa^2_{0t}(d)}. \label{mu1}
\end{align}
Substituting $\hat \mu (d)$ into \eqref{css} yields the profile (or concentrated) CSS function
\begin{align}
     L^*(d) &= \frac{1}{2} \sum_{t = 1}^T \left(  \Delta_{+}^{d} (x_t-\hat{\mu}(d) ) \right)^2 . \label{likstar}
\end{align}
Note that we use asterisks to emphasise that we are dealing with a \textit{profile} objective function. The resulting CSS estimator of $d$ is given by
\begin{align}
    \hat{d} = \operatorname*{argmin}_{d \in \mathbb{D}}  L^*(d). \label{CSS}
\end{align}
As discussed in Section \ref{model}, the model effectively conditions on $z_t = 0$, for $t \leq 0$. For this reason, \textcite{hualde2011gaussian} and \textcite{hualde2020truncated} prefer to call the estimator in \eqref{CSS} the truncated sum-of-squares estimator.

\textcite[Theorem 1 and Theorem 2]{hualde2020truncated} show that if $x_t$ is generated by \eqref{q1} and if Assumption \ref{a1} and \ref{a2} hold, then, as $T \rightarrow \infty$, 
\begin{align}
    \hat{d} \xrightarrow{p} d_0
\end{align}
and
\begin{align}
    \sqrt{T}(\hat{d} - d_0) \xrightarrow{d} N(0,\zeta^{-1}_2), \label{hndis}
\end{align}
where $\zeta^{-1}_2 = 6/\pi^2$.

A few remarks about the estimator $\hat{\mu}(d)$ in \eqref{mu1} are instructive. For $d = d_0$ we have that 
\begin{align*}
    \hat{\mu}(d_0)-\mu_0 = \frac{\sum_{t = 1}^T \epsilon_t\kappa_{0t}(d_0)}{\sum_{t = 1}^T \kappa^2_{0t}(d_0)},
\end{align*}
which has mean zero and variance $\sigma_0^2 (\sum_{t = 1}^T \kappa^2_{0t}(d_0))^{-1}$. In the stationary region, i.e.\ when $d_0 < 1/2$, this variance goes to zero because then $\sum_{t = 1}^T \kappa^2_{0t}(d_0)$ diverges in $T$,
see Lemma \ref{genlemmaaaa2s}. As opposed to that, in the non-stationary region, i.e.\ when $d_0 > 1/2$, this variance does not go to zero because then $\sum_{t = 1}^T \kappa^2_{0t}(d_0)$ is bounded in $T$, see Lemma \ref{genlemmaaaa2n}. This
is the reason why
\begin{align}
\hat{\mu}(\hat{d}) \xrightarrow{p} \mu_0
\end{align}
only if $d_0 < 1/2$, see \textcite[Corollary 1]{hualde2020truncated} for the proof.

For comparison, we also analyse the situation where the true $\mu_0$ is known. As mentioned earlier, this may often not be particularly realistic in practice. The CSS estimator for this model can be derived by substituting $\mu_0$
into \eqref{css} to have 
\begin{align}
 L^*_{\mu_0}(d) &= \frac{1}{2} \sum_{t = 1}^T \left(  \Delta_{+}^{d} (x_t-\mu_0 ) \right)^2 \label{Lknownm}
\end{align}
such that
\begin{align}
     \hat{d}_{\mu_0} &= \operatorname*{argmin}_{d \in \mathbb{D}}  L^*_{\mu_0}(d) . \label{CSSknown} 
\end{align}
This estimator is considered by \textcite{hualde2011gaussian} and \textcite{nielsen2015asymptotics} who show that if $x_t$ is generated by \eqref{q1} and if Assumption \ref{a1} and \ref{a2} hold, then, as $T \rightarrow \infty$, 
\begin{align}
    \hat{d}_{\mu_0} \xrightarrow{p} d_0
\end{align}
and
\begin{align}
    \sqrt{T}(\hat{d}_{\mu_0} - d_0) \xrightarrow{d} N(0,\zeta^{-1}_2), \label{hndisknown}
\end{align}
where $\zeta^{-1}_2 = 6/\pi^2$. Remarkably, the asymptotic distribution of $\hat{d}_{\mu_0}$ is identical to that of $\hat{d}$ in \eqref{hndis}. In other words, the distribution does not depend on whether $\mu$ is known or needs
to be estimated. This contrasts to, for instance, unit root models in which the asymptotic distribution of the first-order serial correlation coefficient hinges on whether $\mu$ is known or not. 




\subsection{The modified profile likelihood} \label{sectionmod}

A central concern in this paper is to investigate the bias of $\hat{d}$ in \eqref{CSS} and of $\hat{d}_{\mu_0}$ in \eqref{CSSknown}. This will be done in Section \ref{S3} below. It will turn out that the expectation of the CSS estimators is a function of the expectation of the score functions, or first derivatives, of $L^*(d)$ and $L_{\mu_0}^*(d)$ evaluated at $d = d_0$, respectively. The present section will therefore examine the bias of the two scores
and builds on an approach by \textcite{mccullagh1990simple} to correct for it.

To that end, it will be instructive to interpret the CSS objective in \eqref{css} as a log-likelihood function, as do  \textcite{johansen2016role} and \textcite{hualde2020truncated}. Assuming that
$\epsilon_t \sim \textit{NID}(0,\sigma^2)$, the Gaussian log-likelihood of $x_t$ in \eqref{q1}, conditional on $x_t$ = $0$ for $t\leq 0$, is given by
\begin{align}
    \ell (d,\mu,\sigma^2) = -\frac{T}{2} \log\left(\sigma^2\right) - \frac{1}{2\sigma^2} \sum_{t = 1}^T \left(  \Delta_{+}^{d} (x_t-\mu ) \right)^2. \label{q2}
\end{align}
Throughout the paper, we omit additive constants in the likelihood functions for notational simplicity. 
Maximising \eqref{q2} with respect to $\sigma^2$ yields 
\begin{align*}
    \hat{\sigma}^2(d,\mu) = \frac{1}{T} \sum_{t = 1}^T \left(  \Delta_{+}^{d} (x_t-\mu ) \right)^2
\end{align*}
and the profile log-likelihood 
\begin{align}
     \ell(d,\mu) &= \ell (d,\mu,\hat{\sigma}^2(d,\mu)) \nonumber \\
     &= -\frac{T}{2} \log\left( \frac{1}{T} \sum_{t = 1}^T \left(  \Delta_{+}^{d} (x_t-\mu ) \right)^2\right). \label{lq2}
\end{align}  
Maximising \eqref{lq2} further with respect to $\mu$ results in $\hat \mu (d)$ in \eqref{mu1} and the  profile log-likelihood function
\begin{align}
    \ell^*(d) &= \ell (d,\hat{\mu}(d) ,\hat{\sigma}^2(d,\hat{\mu}(d)))  \nonumber \\
    &= -\frac{T}{2} \log\left( \frac{1}{T} \sum_{t = 1}^T \left(  \Delta_{+}^{d} (x_t-\hat{\mu}(d) ) \right)^2\right).  \label{lq3}
\end{align}
Clearly, the estimator of $d$ resulting from maximising \eqref{lq3} is identical to that obtained by minimising \eqref{likstar} since
\begin{align}
     L^*(d) &=  \frac{T}{2}  \exp \left( -\frac{2}{T} \ell^*(d) \right).  \label{Lstar}
\end{align}
So, the CSS objective $L^*(d)$ can be seen as a negative non-logged profile likelihood. As the maximum likelihood estimator of $d$ is asymptotically efficient, see \textcite{hualde2020truncated}, so is the CSS estimator $\hat d$ in
\eqref{CSS}. The same can of course be said of $\hat d_{\mu_0}$ in \eqref{CSSknown} since the profile CSS objective $ L^*_{\mu_0}(d)$ in \eqref{Lknownm} can be obtained from \eqref{lq2} by replacing $\mu$ by its known value
$\mu_0$ such that
\begin{align}
 L^*_{\mu_0}(d) &= \frac{T}{2}  \exp \left( -\frac{2}{T} \ell(d,\mu_0) \right) . \label{Lmulog}
\end{align}

We will in the present section therefore interpret $L^*(d)$ in \eqref{likstar} as a profile likelihood. As such, it is not a genuine likelihood, for it is not directly based on observable quantities, see \textcite{barndorff1983formula} and
\textcite{severini2000likelihood}. Instead, it is a function of the maximum likelihood estimators of $\mu$ and $\sigma^2$ which are treated as if they were the true parameter values. In large samples, the concentration procedure has
relatively minor effects, yet \textcite{chung1993small} showed in Monte Carlo simulations that in small samples it leads to a strong bias in $\hat{d}$. This is because profile likelihoods do not necessarily
possess the same properties as genuine likelihoods. It is well-known that, under classical regularity conditions and with a fixed number of regressors, the score of the profile likelihood is biased. In particular, its expectation
is $O(1)$, see \textcite{kalbfleisch1973marginal}, \textcite{mccullagh1990simple} and \textcite{liang1995inference}. The following theorem derives the bias of the score of $L^*(d)$. The proof will be given in Appendix
\ref{proofthm52}. Note that we adopt Euler's notation and denote the $m^\text{th}$ derivative of a function $f(d)$ with respect to $d$ by the operator $D^m$ such that $D^mf(d) = \frac{\partial^m}{\partial d^m} f(d)$.

\begin{theorem}\label{score_1} 
  Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{q1} and let Assumption \ref{a1} be satisfied. Then, the expected score of $L^* (d)$, evaluated at the true parameter $d_0$, is given by
\begin{align}
    E\left( \DL^*(d_0) \right) = O( \log(T)I(d_0 < 1/2) + I(d_0 > 1/2) ), \label{score1}
\end{align}
when $T \rightarrow \infty$.
\end{theorem}

Clearly, the score $\DL^*(d)$ is biased. In addition, the bias is not uniform in $d_0$: The classical result that $E( \DL^*(d_0) ) = O(1)$ only holds for $d_0 > 1/2$. For $d_0 < 1/2$, however, the expectation of the score
diverges at rate $\log(T)$. The competition between the stochastic and the deterministic component explains this difference in orders. In the non-stationary region, i.e.\ when $d_0 > 1/2$, we recall that $\hat{\mu}(\hat{d})$ is
not consistently estimated, see the discussion in Section \ref{subsect22}. The reason is that the stochastic component $z_t$ in \eqref{q1} dominates the deterministic component $\mu$. Hence, the bias in the score is less
influenced by $\hat{\mu}(\hat{d})$, resulting in the expected score being $O(1)$ for such $d_0$. On the other hand, if $d_0 < 1/2$, $\hat{\mu}(\hat{d})$ is consistently estimated and $\hat{\mu}(\hat{d})$ plays a more important
role in the bias of the score. This is reflected in the expected score being $O(\log(T))$ for such $d_0$.



The order of magnitude in \eqref{score1} also applies to the expectation of the score function of the profile log-likelihood function $\ell^*(d)$ in \eqref{lq3}. To see this, note that \eqref{Lstar} entails
\begin{align}
    D\ell^*(d) = - \frac{1}{2}\frac{\DL^*(d)}{T^{-1} L^*(d)}. \label{zzeta}
\end{align}
From Theorem \ref{score_1} we can then deduce the following corollary. The proof is omitted. 
\begin{corollary} \label{corr21}
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{q1} and let Assumption \ref{a1} be satisfied. Then, the expected score of $l^* (d)$, evaluated at the true parameter $d_0$, is given by
\begin{align*}
        E \left( D\ell^*(d_0) \right)  &= O( \log(T)I(d_0 < 1/2) + I(d_0 > 1/2) ),
\end{align*}
when $T \rightarrow \infty$.
\end{corollary}



The situation for $L_{\mu_0}^*(d)$ in \eqref{Lknownm} is somewhat different. Although, technically speaking, $L_{\mu_0}^*(d)$ is also a profile likelihood, it will be proved in Appendix \ref{proofthm52} that its score is unbiased despite the substitution of  $\hat{\sigma}^2$ for $\sigma^2$. This is summarised in the following theorem.
\begin{theorem}\label{score_2} 
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{q1} and let Assumption \ref{a1} be satisfied. Then, the expected score of $L^*_{\mu_0} (d)$, evaluated at the true parameter $d_0$, is given by
\begin{align}
     E\left( \DL_{\mu_0}^*(d_0) \right) = 0 \label{score2}
\end{align}
when $T \rightarrow \infty$.
\end{theorem}




This discussion highlights the need for a modification of the profile likelihood function such that it behaves more like a genuine likelihood in terms of score unbiasedness. This modification will eliminate the bias of
the CSS estimator $\hat d$ stemming from the presence of the unknown nuisance parameter, as will be seen in Section \ref{S3}. The idea of modifying the profile likelihood to obtain score unbiasedness is in fact not new
and was previously discussed by \textcite{mccullagh1990simple}. \textcite{martellosio2020adjusted}, for instance, implement this idea for a spatial model.

To obtain an unbiased score, \textcite{mccullagh1990simple} recenter the score of the profile log-likelihood function, yielding, say,
\begin{align}
    D  \ell^*_{a}(d) = D  \ell^*(d) - a(d),\label{tm21}
\end{align}
where $ \ell^*(d)$ denotes, as before, the profile log-likelihood function and where $a(d)$ is an adjustment function only depending on $d$. Then they require that 
\begin{align}
    E \left( D  \ell^*_{a}(d_0) \right) = 0, \label{tm1}
\end{align}
which implies that 
\begin{align}
    a(d_0) = E \left(  D  \ell^*(d_0)  \right), \label{tm3}
\end{align}
for all $d_0$.
Finally, they call 
\begin{align}
      \ell^*_{a}(d) = \int_{d}   D  \ell^*_{a}(t) dt, \label{tm2}
\end{align}
the adjusted profile log-likelihood for $d$, which is subsequently maximised w.r.t.\ $d$.

\begin{remark}
 \textcite{mccullagh1990simple} further adjust $D\ell^*_{a}(d)$ to make it information unbiased, i.e. making its variance equals to the negative expectation of the derivative of the score. While these adjustments may improve the efficiency of the estimator, they are not addressed in this paper because they do not affect the location of the zeros of $D\ell^*_{a}(d)$.
\end{remark}

The adjustment function $a(d)$ can in principle be computed from \eqref{tm3}. Yet this calculation is challenging as can be seen by rewriting \eqref{tm3} as 
\begin{align}
      a(d_0)  &= E \left( - \frac{1}{2}\frac{\DL^*(d_0)}{T^{-1} L^*(d_0)}  \right), \label{diffadj}
\end{align}
see \eqref{zzeta}. Evaluating the expectation of the fraction is not trivial. \textcite{martellosio2020adjusted} circumvent this problem by assuming $\epsilon_t$ to be Gaussian, thereby effectively using the profile
log-likelihood $\ell^*(d)$ as basis for the adjustment. We, however, avoid this strong assumption and consider the profile CSS objective function $L^*(d)$ instead, as explained in Section \ref{subsect22}. Indeed, \textcite{mccullagh1990simple} in their Remark 3 allude to the possibility of using an
objective function other than the profile likelihood for deriving an adjustment. To that end, we
need to frame the approach of \textcite{mccullagh1990simple} in terms of $L^*(d)$.

To do this, first note that 
\eqref{tm2} can be written as
\begin{align}
      \ell^*_{a}(d) &= \int_{d} \left(D  \ell^*(t) - a(t)\right) dt \nonumber \\ 
                &=  \ell^*(d) - A(d), \label{tbs51}
\end{align}
with $A(d) = \int_{d} a(t) dt$. Based on the relationship between $ \ell^*(d)$ and $L^*(d)$ in \eqref{Lstar}, we can write \eqref{tbs51} as
\begin{align}
      \ell^*_{a}(d) &= -\frac{T}{2} \log\left( \frac{2}{T}  L^*(d)  \right) - A(d). \label{tbs1}
\end{align}
Using a similar argument as in \eqref{Lstar}, it is clear that maximizing the adjusted profile log-likelihood $ \ell^*_a (d)$ in \eqref{tbs1} is equivalent to minimising the adjusted profile CSS objective, defined as
\begin{align}
     L^*_{a}(d) &=  \frac{T}{2}  \exp \left( -\frac{2}{T}  \ell^*_{a}(d) \right). \label{Lalog}
\end{align}
Finally, replacing $ \ell^*_a (d)$ in \eqref{Lalog} by \eqref{tbs1} yields
\begin{align}
     L^*_{a}(d) &= \exp{\left(\frac{2}{T} A(d)\right)}L^*(d). \label{tbs3}
\end{align}
It is important to note that while the adjustment in \eqref{tbs1} is additive, it is multiplicative in \eqref{tbs3}. Recall that $a(d)$ in \eqref{diffadj}, and thus $A(d)$ in \eqref{tbs3}, is difficult to compute. We therefore define, as an alternative, the modified profile CSS objective function
\begin{align}
    L^* _{m}(d) = m(d) L^*(d) \label{generformd}
\end{align}
where the multiplicative modification term $m(d) > 0$ depends only on $d$. The corresponding score function is the first derivative of \eqref{generformd}:
\begin{align}
   \DL^*_{m}(d) = m(d) \DL^*(d) + Dm(d)  L^*(d). \label{tibmc12}
\end{align}
As \textcite{mccullagh1990simple}, we now require that our objective function is score unbiased, i.e.\ that the score function in \eqref{tibmc12} satisfy
\begin{align}
    E \left( \DL^*_{m}(d_0) \right) = 0, \label{tibmc13}
\end{align}
cf.\ \eqref{tm1}. Using \eqref{tibmc12} and the fact that $D \log (m(d)) = Dm(d)/m(d)$ and $m(d) > 0$ it follows that \eqref{tibmc13} is equivalent to the condition that
\begin{align}
    D \log\left( m(d_0) \right) = -\frac{E\left( \DL^*(d_0) \right)}{E\left( L^*(d_0) \right)}. \label{solveform}
\end{align}
It will be seen below that the evaluation of the right-hand side of \eqref{solveform} is straightforward, as opposed to the evaluation of \eqref{diffadj}. In particular, it avoids imposing an additional normality
assumption.


    
\subsection{The modified conditional sum-of-squares estimator} \label{Smcss}

The condition in \eqref{solveform} is now used for finding the modification term $m(d)$ for the modified profile CSS objective  in \eqref{generformd}: First, it is shown in Lemma \ref{genlemmaexpectations} that the expectation of $\DL^*(d_0)$ equals 
\begin{align*}
  E\left( \DL^*(d_0) \right) = -\sigma^2_0 \frac{\sum_{t = 1}^T \kappa_{0t}(d_0) \kappa_{1t}(d_0)}{\sum_{t = 1}^T \kappa^2_{0t}(d_0)},
\end{align*}
where $\kappa_{0t}(d) = \pi_{t-1}(1-d)$ and $\kappa_{1t}(d) = D \kappa_{0t}(d) =  -D \pi_{t-1}(1-d)$ see also \eqref{definitionkappa}. It is also shown in Lemma \ref{genlemmaexpectations} that
\begin{align*}
    E\left( L^*(d_0) \right) =  \sigma^2_0 \frac{T-1}{2}.
\end{align*}
Consequently, from \eqref{solveform}, we have
\begin{align}
   D \log\left( m(d_0) \right) = \frac{2}{T-1} \frac{ \sum_{t = 1}^T \kappa_{0t}(d_0) \kappa_{1t}(d_0)}{  \sum_{t = 1}^T \kappa^2_{0t}(d_0) }. \label{anitd}
\end{align}
Upon integrating \eqref{anitd} we obtain
\begin{align*}
   \log\left( m(d) \right) =  \log \left( \sum_{t = 1}^T \kappa^2_{0t}(d) \right)^{\frac{1}{T-1}},
\end{align*}
before, finally, exponentiation yields
\begin{align}
    m(d) &= \left( \sum_{t = 1}^T \kappa^2_{0t}(d)  \right)^{\frac{1}{T-1}}. \label{modificationterm}
\end{align}

The modified profile CSS objective function in \eqref{generformd} is thus given by the product of $m(d)$ in \eqref{modificationterm} and $L^* (d)$ in \eqref{likstar}, i.e.\
\begin{align*}
  L^*_m (d) = \left( \sum_{t = 1}^T \kappa^2_{0t}(d)  \right)^{\frac{1}{T-1}}  \frac{1}{2} \sum_{t = 1}^T \left(  \Delta_{+}^{d} (x_t-\hat{\mu}(d) ) \right)^2 .
\end{align*}
We call the estimator that minimises $L^*_m (d)$ the modified conditional sum-of-squares (MCSS) estimator and denote the estimator of $d$ by $\hat{d}_{m}$, i.e.\ ,
\begin{align}
    \hat{d}_{m} = \operatorname*{argmin}_{d \in \mathbb{D}}  L^*_{m}(d).  \label{MCSS}
\end{align}

Two important properties of the modification term $m(d)$ are stated in the following lemma. See Appendix \ref{lemma31} for the proof. 

\begin{lemma} \label{lm:md}
  For all $d \in \mathbb{R}$,
  \begin{align}
    m(d) &\geq 1, \label{mqeq1}
    \end{align}
    where equality holds if and only if $d = 1$. Also, it holds that, for $T \rightarrow \infty$,
    \begin{align}
    m(d) &= 1 + O(T^{-1}\log(T) I(d < 1/2) + T^{-1} I(d > 1/2) ). \label{eq111} 
  \end{align}
  for all $d \in \mathbb{R} \backslash \{1/2\}$
\end{lemma}

The property in \eqref{mqeq1} implies that the modification term $m(d)$ acts as a penalisation in the minimisation of the modified profile likelihood $L_m^*(d)$ through inflating $L^*(d)$ by $m(d)$.  The property in
\eqref{eq111} ensures that $m(d) \rightarrow 1$ such that the asymptotic properties of the MCSS estimator $\hat d_m$ are the same as those of the CSS estimator $\hat d$ in \eqref{CSS}. This is desirable because the CSS estimator
is efficient under Gaussianity, as argued in Section \ref{sectionmod}. The asymptotic properties of $\hat d_m$ are summarised for completeness in the following theorem and are proved in Appendix \ref{proofthm51}.
\begin{theorem}\label{t2} 
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{q1} and let Assumption \ref{a1} and \ref{a2} be satisfied. Then, as $T \rightarrow \infty$, 
\begin{align}
    \hat{d}_{m} \overset{p}{\rightarrow} d_0,
\end{align}
and
\begin{align}
    \sqrt{T}(\hat{d}_{m}  - d_0) \xrightarrow{d} N(0,\zeta^{-1}_2), \label{mcssdis}
\end{align}
where $\zeta^{-1}_2 = 6/\pi^2$.
\end{theorem}

The intuitive explanation of Theorem \ref{t2} follows from noticing that
\begin{align}
\label{ObjecApprox}
\begin{split}
    L^*_{m}(d_0) &=  L^*(d_0) + O_P(1) \  \ \  \ \ \ \ \text{ for }d_0 > 1/2, \\
    L^*_{m}(d_0) &=  L^*(d_0) + O_P(\log(T))\text{ for }d_0 < 1/2, 
\end{split}
\end{align}
where use was made of the definition $L^*_{m}(d_0)$ in \eqref{generformd} and the asymptotic behaviour of $m(d)$ in \eqref{eq111} of Lemma \ref{lm:md}. Since $ L^*(d_0)$ in \eqref{ObjecApprox} is $O_P(T)$, the second summands have no influence on the asymptotic distribution of $\hat{d}_{m}$. For the bias, however, the latter terms require further analysis, which is carried out below in Section \ref{S3}.


\begin{figure}[H]
  \centering
  \hspace{-1.25ex}\subfloat[modification term]{
    \includegraphics[width=0.495\textwidth]{bias1_cropped.pdf}
  }
  \subfloat[likelihoods]{
    \includegraphics[width=0.48\textwidth]{Likihoodsgraph_cropped.pdf}
  }
  \caption{Panel (a) plots the modification term $m(d)$ in \eqref{modificationterm} for $d$ between $-1$ and 2, and $T$ = 32, 64, 128, 256. The value of $d = 1/2$ is added as a vertical line for clarity.  Panel (b) shows the Monte
    Carlo average over 10,000 replications of $L^*(d)$, $L_{\mu_0}^*(d)$ and $L_{m}^*(d)$. The DGP is given in \eqref{q1} with $\epsilon_t \sim \textit{NID}(0,1)$ with $d_0 = 0.2$, $\mu_0 = 0$ and $T$ = 64.}
  \label{fig1}\end{figure}





The modification term is plotted in panel (a) of Figure \ref{fig1} for some illustrative values of $d$ and $T$. Four important observations can be made: First, recall from \eqref{mqeq1} that the modification term $m(d)$ penalises
the CSS objective $L^*(d)$ through inflating it by the factor $m(d)$. It appears from the plot that in the stationary region, i.e.\ when $d < 1/2$, $m(d)$ inflates $L^*(d)$ more than in the non-stationary region, i.e.\ when
$d \geq 1/2$. This is a reflection of the fact that the bias in the score is larger in the stationary region, as was argued in Theorem \ref{score_1}.  Secondly, it is plain that when $d = 1$ the bias caused by estimating the
constant term $\mu$ is the smallest, as predicted in Lemma \ref{lm:md}. Thirdly, even for a moderately large sample of size $T = 256$, $m(d)$ still turns out to be substantial in the stationary region, implying that the
corresponding bias in the score is large. Fourthly, the negative slope of the modification term $m(d)$ for $d < 1$ implies that the minimum of $L^*_m(d)$ is shifted to the right of that of $L^*(d)$. This is illustrated in panel
(b) of Figure \ref{fig1} which displays a Monte Carlo simulation of the CSS and MCSS objective functions. The DGP is stationary and corresponds to the model in \eqref{q1} with $\epsilon_t \sim \textit{NID}(0,1)$, $d_0 = 0.2$ and
$\mu_0 = 0$. The sample size is $T = 64$ and the number of replications is 10,000. On display is the Monte Carlo average of the simulated $L^*(d)$, $L^*_{\mu_0}(d)$ and $L_{m}^*(d)$. The solid line represents the Monte Carlo
average of $L^*(d)$: it can be seen that the CSS estimator underestimates the true $d_0 = 0.2$ on average. The dash-dotted line represents the Monte Carlo average of $L_{\mu_0}^*(d)$, which takes the constant term as known. This
estimator is, on average, close to $d_0$. The dotted line represents the Monte Carlo average of $L_{m}^*(d)$, whose minimum is shifted to the right of that of $L^*(d)$. It therefore corrects for the distortion in $L^*(d)$ caused
by estimating $\mu$.



\subsection{Relationship with other modifications} \label{relwother}

There is a large literature on correcting the bias of maximum likelihood caused by the presence of unknown nuisance parameters.  Seminal contributions include \textcite{barndorff1983formula} who proposed the modified likelihood
function, and \textcite{cox1987parameter} who contributed the idea of the conditional profile likelihood by approximating the modified likelihood function. Both modifications result in modified profile likelihoods that are
approximately score unbiased, see \textcite{liang1987estimating} and \textcite{cox1994inference}. It is therefore illuminating to investigate how our MCSS objective, with an expected score exactly equal to zero, relates to alternative approaches to bias-reduction, or how our modification term $m(d)$ compares to alternative adjustments. This
section discusses two such ideas.

First, reconsider the adjusted profile log-likelihood $\ell^*_a (d)$ proposed by \textcite{mccullagh1990simple} and derived in Section \ref{sectionmod}. Denote the corresponding estimator by
\begin{align*}
    \hat{d}_{a} &= \operatorname*{argmax}_{d \in \mathbb{D}}  \ell^*_{a}(d).
\end{align*}
The proof of the following corollary follows easily from \eqref{diffadj} and \eqref{solveform} and is therefore omitted.
\begin{corollary} \label{corrrelationship}
  Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{q1} and let Assumption \ref{a1} and \ref{a2} be satisfied. Then if
  \begin{align} 
    E \left( \frac{ \DL^*(d_0) }{L^*(d_0) } \right) =  \frac{E \left( \DL^*(d_0) \right)}{E \left( L^*(d_0)  \right)} \label{relatlL}  
  \end{align}
  it holds that $\hat{d}_{m} = \hat{d}_{a}$.  
  \end{corollary}


As mentioned earlier, it is not easy to evaluate the left-hand side of \eqref{relatlL}. One exception is if we assume $\epsilon_t$ to be NID. Yet it turns out that even in this special case \eqref{relatlL} does not hold. The proof is omitted here but can be derived by applying \textcite[Lemma S.7.1]{martellosio2020adjusted}. 

Secondly, a modification term closely related to $m(d)$ in \eqref{modificationterm} is the one discussed in \textcite{an1993cox} who implement the idea of \textcite{cox1987parameter} to adjust the log-likelihood function.  The setup in
\textcite{an1993cox} is different from ours, however: They consider a stationary Gaussian type-I fractional process $\tilde{x}_t$ generated by the model
\begin{align}
    \tilde{x}_t = \mu + \tilde{z}_t, \label{type1model}
\end{align}
where $\tilde{z}_t$ is defined in \eqref{type1zt} with $\epsilon_t\sim \textit{NID}(0,\sigma^2)$ and $|d| < 1/2$. This contrasts to our type-II process whose $d$ we also allow to lie in the non-stationary region and whose error term
$\epsilon_t$ is not assumed to be Normally distributed.

It will prove helpful to phrase the approach by \textcite{an1993cox} in matrix notation: Define the $T \times 1$ vector $\tilde{x} = (\tilde{x}_1,\ldots,\tilde{x}_T)'$ such that $\tilde{x} \sim N(\mu \iota, \sigma^2 \Sigma(d))$ where
$\iota$ is a $T\times1$ vector of ones and $\Sigma(d) \sigma^2$ is the $T \times T$ variance-covariance matrix of $\tilde{x}$, see for instance \textcite[Theorem 1]{hoskingdiff1981} for the
elements of $\Sigma(d)$. The log-likelihood function is then given by
\begin{align}
    \tilde{\ell}(d,\mu,\sigma^2) = -\frac{1}{2}  \log( |\Sigma(d)| ) -\frac{T}{2}  \log( \sigma^2)  - \frac{1}{2 \sigma^2} \left(  \tilde{x}  - \iota \mu\right)' \Sigma(d)^{-1} \left(  \tilde{x}  - \iota \mu\right) \label{philipslieberman}
\end{align}
and the profile log-likelihood function for $d$ by
\begin{align*}
     \tilde{\ell}^*(d) &=  \tilde{\ell}(d,\hat{\mu}(d),\hat{\sigma}^2(d,\hat{\mu}(d))) \\
           &= -\frac{1}{2}  \log( |\Sigma(d)| ) - \frac{T}{2}  \log \left( \frac{1}{T} \left(  \tilde{x}  - \iota \hat \mu (d) \right)' \Sigma(d)^{-1} \left(  \tilde{x}  - \iota \hat \mu (d) \right) \right) ,
\end{align*}
where $\hat{\mu}(d)$ and $\hat{\sigma}^2(d,\mu)$ are the maximum likelihood estimator of $\mu$ and $\sigma$, respectively. The modified profile log-likelihood function that \textcite{an1993cox} find is
\begin{align}
    \tilde{\ell}^*_m(d) &=  \tilde{\ell}^*(d)  + \frac{3}{2} \log \left( \hat{\sigma}^2(d,\hat{\mu}(d)) \right) + \frac{1}{T} \log( |\Sigma(d)|) - \frac{1}{2} \log\left(   \iota' \Sigma(d)^{-1} \iota \right). \label{expcox}
\end{align}
It can be shown that the orders of magnitude of the last three summands on the right-hand side of \eqref{expcox} are $O_P(1)$, $O(1)$ and $ O(\log(T))$, respectively. For the proof of the second summand, we refer to
\textcite{dahlhaus1989efficient}. The order of magnitude of the third summand follows from Hadamard's inequality, see \textcite[Theorem 7.8.1]{horn2013matrix}, implying that
$T^{-1} \log |\Sigma(d)| \leq \log( \Var(\tilde{x}_t)/\sigma^2 )$ where the right-hand side is $O(1)$ because $|d| < 1/2$. The order of magnitude of the fourth summand follows from $\iota' \Sigma(d)^{-1} \iota = O(T^{1-2d+\varepsilon})$ for each $\varepsilon > 0$, cf.\ \textcite[Theorem 5.2]{adenstedt1974large}. The leading of the three summands
is therefore the last one, its  order of magnitude being $O(\log(T))$. 

In order to compare $\tilde{\ell}^*_m (d)$ to our MCSS function $L_m^*(d)$ in \eqref{generformd} we transform the latter in a fashion similar to that in \eqref{Lstar} or \eqref{Lalog} and define
\begin{align*}
     \ell^*_{m}(d) &= -\frac{T}{2} \log\left( \frac{2}{T}  L_{m}^*(d)  \right),
\end{align*}
such that
\begin{align*}
     \ell^*_{m}(d)  &= -\frac{T}{2} \log\left( \frac{2}{T}  m(d) L^*(d)  \right), \\
                 &=  \ell^*(d)-\frac{T}{2} \log\left(  m(d)  \right), 
\end{align*}
with $\ell^* (d)$ given in \eqref{lq3}. Using the definition of $m(d)$ in \eqref{modificationterm} yields
\begin{align}
     \ell^*_{m}(d)  &=  \ell^*(d)-\frac{T}{(T-1)} \frac{1}{2} \log\left(  \sum_{t = 1}^T \kappa^2_{0t}(d)   \right), \notag \\
                 &= \ell^*(d)-\frac{1}{2} \log\left(  \sum_{t = 1}^T \kappa^2_{0t}(d)   \right) + O_P(T^{-1}\log(T)), \label{modlogterm}
\end{align}
since $T/(T-1) = 1 + 1/(T-1)$. Clearly, the second summand is of order $O (\log T)$ as is shown in Lemma \ref{genlemmaaaa2s}.

Two observations are now instructive. First, the leading modification term in $\ell^*_m (d)$ is of the same order of magnitude as that in $\tilde \ell^*_m$ in \eqref{expcox}, namely $O(\log(T))$. Second, we note that the Cholesky factor of $\Sigma(d)^{-1}$ is
the GLS transformation matrix that filters out the correlation structure of the type-I error term $\tilde z_t$. Similarly, in our setting, $\Delta_{+}^{d}$ filters out the correlation structure of the type-II error
$z_t$. Indeed, if we could replace $\Sigma(d)^{-1/2} \iota$ in \eqref{expcox} by $\Delta_{+}^{d} \iota$, we would obtain
\begin{align*}
  \frac{1}{2} \log \left( (\Delta_{+}^{d} \iota )' (\Delta_{+}^{d} \iota) \right) = \frac{1}{2} \log \left( \sum_{t = 1}^T \kappa^2_{0t}(d)   \right) .
\end{align*}
using the definition of $\kappa_{0t}(d)$ in \eqref{definitionkappa}.
Let us emphasise again, however, that the approach by \textcite{an1993cox}, although asymptotically equivalent to ours, is based on a model that assumes stationary and Normally distributed data. In addition, it
necessitates the computation of the $T \times T$ variance-covariance matrix, or its Cholesky factor, which is often onerous computationally.


\subsection{Asymptotic biases} \label{S3}


This section investigates the asymptotic biases of the estimators considered so far, with particular attention paid to the bias of the MCSS estimator $\hat{d}_m$ in \eqref{MCSS}. Two questions are of central interest. First, by how much does the MCSS estimator $\hat{d}_m$ reduce the bias of the CSS estimator $\hat{d}$ in \eqref{CSS}? Second, is the bias of the MCSS estimator $\hat d_m$ comparable to that of the CSS estimator with known $\mu_0$? To address both questions, we find expressions for the asymptotic biases of $\hat{d}$, $\hat{d}_{\mu_0}$ and $\hat{d}_m$ based on a stochastic expansion of the estimators. For the non-stationary region, i.e.\ when $d_0 > 1/2$, the expansion of $\hat{d}$ and $\hat{d}_{\mu_0}$ as well as their asymptotic biases are derived in \textcite{johansen2016role}. In the analysis below, we also consider $\hat{d}$ and $\hat{d}_{\mu_0}$ in the stationary region, i.e.\ when $d_0 < 1/2$. The bias of the MCSS estimator $\hat{d}_m$ is derived for $d_0 > 1/2$ and $d_0 < 1/2$. 

\textcite[Section 3.2]{johansen2016role} consider a second-order Taylor series expansion of $DL^*(\hat{d}) = 0$ around $d_0$, yielding
\begin{align}
    0 = DL^*(\hat{d}) = DL^*(d_0) + (\hat{d} - d_0) D^2L^*(d_0) + \frac{1}{2} (\hat{d} - d_0)^2 D^3L^*(d^*), \label{mn1}
\end{align}
where $d^*$ is an intermediate value satisfying $|d^*-d_0| \leq |\hat{d}-d_0| \overset{p}{\rightarrow} 0$. For $d_0 > 1/2$, they demonstrate that the derivatives satisfy $DL^*(d_0) = O_P(T^{1/2})$, $D^2L^*(d_0) = O_P(T)$, and $D^3L^*(d) = O_P(T)$ with $d$ uniformly in a neighbourhood of $d_0$, see \textcite[Lemma B.4]{johansen2016role}, which allows them to obtain  
\begin{align}
    T^{1/2}\left(\hat{d}-d_0\right) =  - T^{1/2} \frac{DL^*(d_0)}{D^2L^*(d_0)} - \frac{1}{2} T^{-1/2}\frac{\left(DL^*(d_0)\right)^2 D^3L^*(d^*)}{\left(D^2L^*(d_0)\right)^3} + O_P(T^{-1}). \label{biasterm}
\end{align}
It is shown in Lemma \ref{asyappgenstat1} below that the derivatives $DL^*(d_0)$, $D^2L^*(d_0)$ and $D^3L^*(d_0)$ are of the same orders of magnitude for $d_0 < 1/2$, implying that \eqref{biasterm} holds for both the non-stationary and stationary region of $d_0$.

Based on \eqref{biasterm}, the asymptotic bias of $\hat d$ can be found. In a first step, the approximations of the derivatives in \textcite[Lemma B.4]{johansen2016role} for the region $d_0 > 1/2$ and in Lemma \ref{asyappgenstat1} for the region $d_0 < 1/2$ are used to find
\begin{align}
   E \left(\hat{d}-d_0\right)
                            =  -(T \zeta_{2})^{-1}\left[ \sigma_0^{-2} E \left(DL^*(d_0)\right)\right] -(T \zeta_{2})^{-1}\left[ 3 \zeta_{3} \zeta_{2}^{-1} \right] +  o(T^{-1}), \label{biastermscr} 
\end{align}
where $\zeta_{s}$ is Riemann's zeta function $\zeta_{s} = \sum_{j = 1}^{\infty} j^{-s}$, $s>1$. This shows the relationship between the bias of the estimator and that of the score function. The first term on the right-hand side of \eqref{biastermscr} dominates the remainder only if $d_0 < 1/2$, because then $E\left(DL^*(d_0)\right) = O(\log(T))$ from Theorem \ref{score_1}. As opposed to that, we have from the same theorem that $E\left(DL^*(d_0)\right) = O(1)$ if $d_0 > 1/2$ and hence the first term on the right-hand side of \eqref{biastermscr} is of the same order as the remainder.

It follows from Lemmata \ref{asyappgennon2}, \ref{asyappgennon3}, \ref{asyappgenstat2} and \ref{asyappgenstat3} that analogues of the derivation in the previous two paragraphs also hold for $\hat{d}_{\mu_0}$ and $\hat{d}_{m}$. Replacing in \eqref{biastermscr} $DL^*(d)$ by $DL_{\mu_0}^*(d)$  and using Theorem \ref{score_2}, it is clear that the expected score term in $E \left(\hat{d}_{\mu_0}-d_0\right)$ vanishes. Similarly, $E\left(DL_m^*(d_0)\right) =0$ by construction, justifying the modification of the CSS objective function in Section \ref{Smcss} in order to obtain score unbiasedness.

In a second step, explicit expressions of the expected scores are found and substituted into \eqref{biastermscr}, yielding one of the main results of this paper, summarised in the following theorem. The results in \eqref{t1eq1} and \eqref{t1eq2} are derived in \textcite[Theorem 4]{johansen2016role} and mentioned here for completeness. The proof of the other cases is presented in Appendix \ref{proofthm53}.
\begin{theorem}\label{t1} 
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{q1} and let Assumption \ref{a1} and \ref{a2} be satisfied. For the non-stationary region, i.e.\ when $d_0 > 1/2$, the biases of $\hat{d}$, $\hat{d}_{\mu_0}$ and $\hat{d}_{m}$ are
\begin{align}
    bias(\hat{d}) &= -(T \zeta_{2})^{-1}\left[ 3 \zeta_{3} \zeta_{2}^{-1} + (\Psi(d_0) - \Psi(2d_0-1) ) \right] + o(T^{-1}), \label{t1eq1}\\
    bias(\hat{d}_{\mu_0}) &= -(T \zeta_{2})^{-1}\left[ 3 \zeta_{3} \zeta_{2}^{-1}\right] + o(T^{-1}),\label{t1eq2}\\
    bias(\hat{d}_{m}) &= -(T \zeta_{2})^{-1}\left[ 3 \zeta_{3} \zeta_{2}^{-1}\right] + o(T^{-1}),\label{t1eq3}
\end{align}
where  $\zeta_{s}$ is Riemann's zeta function $\zeta_{s} = \sum_{j = 1}^{\infty} j^{-s}$, $s>1$, and $\Psi(d) = D\log \Gamma(d)$ denotes the Digamma function. For the stationary region, i.e.\ when $d_0 < 1/2$, the biases of $\hat{d}$, $\hat{d}_{\mu_0}$ and $\hat{d}_{m}$ are
\begin{align}
    bias(\hat{d}) &= -(T \zeta_{2})^{-1}\left[\log(T) + 3 \zeta_{3} \zeta_{2}^{-1} - ( \Psi(1-d_0)+(1-2d_0)^{-1})  \right] + o(T^{-1}), \label{t2eq1} \\
    bias(\hat{d}_{\mu_0}) &= -(T \zeta_{2})^{-1}\left[ 3 \zeta_{3} \zeta_{2}^{-1}\right] +  o(T^{-1}),\label{t2eq2}\\
    bias(\hat{d}_{m}) &= -(T \zeta_{2})^{-1}\left[ 3 \zeta_{3} \zeta_{2}^{-1}\right] + o(T^{-1}).\label{t2eq3}
\end{align} 
\end{theorem} 
The order of the leading term in \eqref{t1eq1} and \eqref{t2eq1} varies depending on $d_0$: if $d_0 < 1/2$ it is $O(T^{-1}\log(T))$ while if $d_0 > 1/2$ it is $O(T^{-1})$. Therefore, in the stationary region, the bias of the CSS estimator is stronger than in the non-stationary region. The difference in the orders can be ascribed to the expectation of the score function, see \eqref{biastermscr} and the subsequent discussion. As opposed to that, the leading bias term of $\hat{d}_m$ does not depend on $d_0$ and is identical in both regions. Furthermore, the leading bias terms in \eqref{t1eq2} and \eqref{t2eq2} are the same as the leading bias terms in \eqref{t1eq3} and \eqref{t2eq3}. That is, the estimated $\hat{d}_m$ behaves, on average, the same as if we had known the true value $\mu_0$, discounting the higher order bias term. The remaining bias term $(T \zeta_{2})^{-1}3 \zeta_{3} \zeta_{2}^{-1}$ is due to the correlations of the derivatives of the likelihood and is not eliminated by our modification. The same bias term appears also in \textcite{lieberman2004expansions} for the bias of the estimated memory parameter based on the maximum likelihood estimator in the type-I fractional model in \eqref{philipslieberman} with $0 < d_0 < 1/2$ and $\sigma^2$ as well as $\mu$ known, see \textcite[p.\ 1106]{johansen2016role} for a discussion. As \textcite[p.\ 1108]{johansen2016role} note, the key factor to assess the distortion in testing or calculating confidence intervals for $d_0$ is the relative bias, i.e.\ the ratio of asymptotic bias to asymptotic standard deviation. The asymptotic standard deviation of the three estimators is equal to $(T \zeta_{2})^{-1/2}$, see \eqref{hndis}, \eqref{hndisknown} and \eqref{mcssdis}. Then it follows from Theorem \ref{t1} that the relative bias for the three estimators is of order $O(T^{-1/2})$ in the non-stationary region. In the stationary region, it is of order $O(T^{-1/2} \log(T))$ for the CSS estimator with unknown $\mu_0$ while it of order $O(T^{-1/2})$ for the CSS estimator with known $\mu_0$ and the MCSS estimator.    


We would like to point out that the remaining bias term of $\hat{d}_m$ is pivotal and can be easily eliminated. We refer to this estimator as the bias-corrected MCSS ($bcm$) estimator and denote it by
\begin{align}
    \hat{d}_{bcm} = \hat{d}_{m} + T^{-1} 3\zeta_3 \zeta_2^{-2}.\label{bcmcc}
\end{align}
We obtain from \eqref{t1eq3} and \eqref{t2eq3} the following property of this estimator.
\begin{corollary} \label{bcmcorr}
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{q1} and let Assumption \ref{a1} and \ref{a2} be satisfied. Then, 
\begin{align*}
bias(\hat{d}_{bcm}) = o(T^{-1}). 
\end{align*}
\end{corollary}
The proof follows directly from Theorem \ref{t1} and is therefore omitted.


\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{EEEEEEEEEEEEEEEE}
\hline
&
\multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{l}{} &
\multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{l}{} &
\multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{l}{} &
   \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)}  \\ \cline{2-4} \cline{6-8} \cline{10-12}  \cline{14-16}
  \multicolumn{1}{c}{$d_0$ \textbackslash{} $T$} &
 \multicolumn{3}{c}{32} &
  \multicolumn{1}{l}{} &
  \multicolumn{3}{c}{64} &
  \multicolumn{1}{l}{} &
  \multicolumn{3}{c}{128} &
  \multicolumn{1}{l}{} &
  \multicolumn{3}{c}{256} \\ \hline
-0.2 & -9.94 & -4.16 & -4.16 &  & -5.63 & -2.08 & -2.08 &  & -3.14 & -1.04 & -1.04 &  & -1.74 & -0.52 & -0.52 \\
-0.1 & -9.97 & -4.16 & -4.16 & & -5.64 & -2.08 & -2.08 &  & -3.15 & -1.04 & -1.04 &  & -1.74 & -0.52 & -0.52 \\
0.0  & -9.95 & -4.16 & -4.16 & & -5.63 & -2.08 & -2.08 &  & -3.14 & -1.04 & -1.04 &  & -1.74 & -0.52 & -0.52 \\
0.1  & -9.81 & -4.16 & -4.16 & & -5.56 & -2.08 & -2.08 &  & -3.11 & -1.04 & -1.04 &  & -1.72 & -0.52 & -0.52 \\
0.2  & -9.42 & -4.16 & -4.16 & & -5.37 & -2.08 & -2.08 &  & -3.01 & -1.04 & -1.04 &  & -1.67 & -0.52 & -0.52 \\
0.3  & -8.32 & -4.16 & -4.16 & & -4.82 & -2.08 & -2.08 &  & -2.74 & -1.04 & -1.04 &  & -1.53 & -0.52 & -0.52 \\
0.4  & -4.18 & -4.16 & -4.16 & & -2.75 & -2.08 & -2.08 &  & -1.70 & -1.04 & -1.04 &  & -1.02 & -0.52 & -0.52 \\ \hline
0.5   & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-} &    & \multicolumn{1}{c}{-}      & \multicolumn{1}{c}{-}     & \multicolumn{1}{c}{-}  &    & \multicolumn{1}{c}{-}     & \multicolumn{1}{c}{-}     & \multicolumn{1}{c}{-}  &    & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-}      \\ \hline
0.6  & -11.29 & -4.16 & -4.16 &  & -5.64 & -2.08 & -2.08 &  & -2.82 & -1.04 & -1.04 &  & -1.41 & -0.52 & -0.52 \\
0.7  & -6.71 & -4.16 & -4.16 &  & -3.36 & -2.08 & -2.08 &  & -1.68 & -1.04 & -1.04 &  & -0.84 & -0.52 & -0.52 \\
0.8  & -5.26 & -4.16 & -4.16 &  & -2.63 & -2.08 & -2.08 &  & -1.31 & -1.04 & -1.04 &  & -0.66 & -0.52 & -0.52 \\
0.9  & -4.56 & -4.16 & -4.16 &  & -2.28 & -2.08 & -2.08 &  & -1.14 & -1.04 & -1.04 &  & -0.57 & -0.52 & -0.52 \\
1.0  & -4.16 & -4.16 & -4.16 &  & -2.08 & -2.08 & -2.08 &  & -1.04 & -1.04 & -1.04 &  & -0.52 & -0.52 & -0.52 \\
1.1  & -3.91 & -4.16 & -4.16 &  & -1.95 & -2.08 & -2.08 &  & -0.98 & -1.04 & -1.04 &  & -0.49 & -0.52 & -0.52 \\
1.2  & -3.73 & -4.16 & -4.16 &  & -1.87 & -2.08 & -2.08 &  & -0.93 & -1.04 & -1.04 &  & -0.47 & -0.52 & -0.52 \\
\hline
\end{tabular}}
\caption{(100 $\times$) Theoretical bias, up to $o(T^{-1})$ terms, of the CSS estimator of $d$ with unknown and known $\mu_0$ and of the MCSS estimator of $d$.}
\label{table2}
\end{table}

Analysing the theoretical bias terms through numerical comparisons may assist in building an intuition. Table \ref{table2} therefore presents the theoretical biases, up to $o(T^{-1})$ terms, of the CSS estimator with unknown and known $\mu_0$ and of the MCSS estimator for selected values of $d_0$ and $T$. It is evident that the bias of the CSS estimator decreases in both the stationary and non-stationary region as $d_0$ decreases, and decreases everywhere as $T$ increases. As \textcite[p.\ 1107]{johansen2016role} note, the bias of $\hat d$ is equal to that of $\hat d_{\mu_0}$ when $d_0 = 1$. Yet it is curious to see that bias($\hat{d}$) is actually smaller than  bias($\hat{d}_{\mu_0}$) for $d_0 = 1.1$ and $d_0 = 1.2$. In fact, this occurs for all $d_0 > 1$. The reason is that 
$\Psi(d_0) - \Psi(2d_0-1)$ becomes negative for $d_0 > 1$ and therefore reduces the term  $ 3 \zeta_{3} \zeta_{2}^{-1} \approx 2.1923$ in \eqref{t1eq1} of Theorem \ref{t1}. Note also that the term $\Psi(d_0) - \Psi(2d_0-1)$ is monotonically decreasing in $d_0 > 1/2$. Furthermore, from \textcite[eqn.\ 6.3.18]{abramowitz1964handbook}, for $d_0 \rightarrow \infty$, it holds that $\Psi(d_0) = \log(d_0) + O(d_0^{-1})$, implying that $\Psi(d_0) - \Psi(2d_0-1) \rightarrow -\log(2) \approx -0.6931$ for $d_0 \rightarrow \infty$. It then follows that,  for all $d_0 > 1$, 
\begin{align*}
-\log(2) < \Psi(d_0) - \Psi(2d_0-1) < 0,
\end{align*}
implying that $0 < 3 \zeta_{3} \zeta_{2}^{-1} + (\Psi(d_0) - \Psi(2d_0-1) ) < 3 \zeta_{3} \zeta_{2}^{-1} - \log(2) \approx 1.4992 $. Therefore, the leading bias term of $\hat{d}$ is smaller than the leading bias term of $\hat{d}_{\mu_0}$ for all $d_0 > 1$. 







\subsection{Simulation}\label{S4}

In this section, we report the results of a Monte Carlo simulation of the small sample properties of the various estimators considered so far. In particular, we look at the CSS estimator of the memory parameter with $\mu_0$ known
and unknown, see \eqref{CSS} and \eqref{CSSknown} respectively, the MCSS estimator in \eqref{MCSS}, together with the bias-corrected version thereof in \eqref{bcmcc}. We take as our DGP the model \eqref{q1} with
$\epsilon_t \sim \textit{NID}(0,1)$. Without loss of generality, we assume that $\mu_0 = 0$, since all estimators are invariant to the value of $\mu_0$. In all settings covered by our experiment, we generate $x_t$ for
$T = 32, 64, 128, 256$. We let the long memory parameter $d_0$ vary. In particular, we set $d_0 = -0.2,-0.1,\ldots, 1.1,1.2$. We compute the estimates using the optimising interval $d \in [d_0-5,d_0+5]$. All results are based on
10,000 replications\footnote{All computations in this paper are done using MATLAB 2019a, see \textcite{MATLAB}. The convergence criteria  used for numerical optimisation are the default ones. The code for replicating the main results in this paper is available on request. }. We use the fractional difference algorithm of \textcite{jensen2014fast} to generate the
fractionally integrated series, as well as to filter the fractionally integrated series in order to evaluate the objective function of the estimators.


Table \ref{table1} shows the Monte Carlo bias (multiplied by 100) of $\hat{d}$,  $\hat{d}_{\mu_0}$, $\hat{d}_{m}$ and $\hat{d}_{bcm}$. We also report the percentage increase of the bias from |bias($\hat{d}_{m}$)| to |bias($\hat{d}$)| by $\Delta \
In Table \ref{table4}, the Monte Carlo MSEs (multiplied by 100) are reported. The last column for each $T$ reports the percentage increase of the MSE from $\text{MSE}(\hat{d}_{m})$ to $\text{MSE}(\hat{d})$  by $\Delta \

\begin{landscape}
\vfill
\begin{table}[H]
\centering
\resizebox{1.45\textwidth}{!}{\begin{tabular}{EEEEEEEEEEEEEEEEEEEEEEEE}
\hline
        &
  \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{bcm}$)} &
  \multicolumn{1}{c}{$\Delta \ &
  \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{bcm}$)} &
  \multicolumn{1}{c}{$\Delta \   &
  \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{bcm}$)} &
  \multicolumn{1}{c}{$\Delta \   &
  \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{bcm}$)} &
  \multicolumn{1}{c}{$\Delta \\multicolumn{1}{c}{$d_0$ \textbackslash{} $T$} & \multicolumn{5}{c}{32}                 &  & \multicolumn{5}{c}{64}                &  & \multicolumn{5}{c}{128}               &  & \multicolumn{5}{c}{256}               \\ \hline
-0.2                 & -14.45 & -3.46 & -3.52 & 0.65 & 310.49 &  & -7.06 & -1.81 & -1.75 & 0.33 & 303.28 &  & -3.57 & -0.91 & -0.86 & 0.18 & 313.79 &  & -1.79 & -0.41 & -0.37 & 0.15 & 383.30 \\
-0.1                 & -14.58 & -3.46 & -3.55 & 0.61 & 310.65 &  & -7.10 & -1.81 & -1.75 & 0.33 & 304.45 &  & -3.58 & -0.91 & -0.86 & 0.18 & 315.06 &  & -1.80 & -0.41 & -0.37 & 0.15 & 384.17 \\
0.0                  & -14.63 & -3.46 & -3.59 & 0.57 & 307.26 &  & -7.10 & -1.81 & -1.76 & 0.32 & 302.66 &  & -3.58 & -0.91 & -0.87 & 0.17 & 313.29 &  & -1.80 & -0.41 & -0.37 & 0.15 & 382.02 \\
0.1                  & -14.53 & -3.46 & -3.64 & 0.53 & 299.17 &  & -7.03 & -1.81 & -1.77 & 0.31 & 296.34 &  & -3.55 & -0.91 & -0.87 & 0.17 & 306.60 &  & -1.78 & -0.41 & -0.38 & 0.15 & 374.83 \\
0.2                  & -14.21 & -3.46 & -3.70 & 0.47 & 284.41 &  & -6.86 & -1.81 & -1.79 & 0.29 & 282.77 &  & -3.46 & -0.91 & -0.88 & 0.16 & 292.39 &  & -1.74 & -0.41 & -0.38 & 0.14 & 359.08 \\
0.3                  & -13.59 & -3.46 & -3.78 & 0.38 & 259.51 &  & -6.52 & -1.81 & -1.82 & 0.27 & 258.74 &  & -3.29 & -0.91 & -0.89 & 0.15 & 267.35 &  & -1.64 & -0.41 & -0.38 & 0.14 & 328.95 \\
0.4                  & -12.66 & -3.46 & -3.88 & 0.29 & 226.60 &  & -5.97 & -1.81 & -1.85 & 0.23 & 222.04 &  & -2.98 & -0.91 & -0.91 & 0.13 & 228.25 &  & -1.47 & -0.41 & -0.39 & 0.13 & 277.33 \\ \hline
0.5                  & -11.35 & -3.46 & -3.98 & 0.19 & 185.54 &  & -5.21 & -1.81 & -1.89 & 0.19 & 175.14 &  & -2.53 & -0.91 & -0.92 & 0.12 & 175.91 &  & -1.22 & -0.41 & -0.40 & 0.12 & 205.10 \\ \hline
0.6                  & -9.78  & -3.46 & -4.02 & 0.15 & 143.34 &  & -4.31 & -1.81 & -1.91 & 0.17 & 125.58 &  & -2.03 & -0.91 & -0.92 & 0.12 & 119.75 &  & -0.95 & -0.41 & -0.41 & 0.11 & 130.70 \\
0.7                  & -8.07  & -3.46 & -3.97 & 0.19 & 103.18 &  & -3.45 & -1.81 & -1.91 & 0.17 & 80.79  &  & -1.58 & -0.91 & -0.92 & 0.12 & 71.61  &  & -0.72 & -0.41 & -0.42 & 0.10 & 73.36  \\
0.8                  & -6.48  & -3.46 & -3.85 & 0.31 & 68.06  &  & -2.76 & -1.81 & -1.89 & 0.19 & 45.95  &  & -1.26 & -0.91 & -0.92 & 0.12 & 37.42  &  & -0.57 & -0.41 & -0.42 & 0.10 & 36.58  \\
0.9                  & -5.19  & -3.46 & -3.74 & 0.43 & 38.79  &  & -2.27 & -1.81 & -1.87 & 0.22 & 21.62  &  & -1.06 & -0.91 & -0.92 & 0.12 & 15.66  &  & -0.47 & -0.41 & -0.41 & 0.11 & 14.51  \\
1.0                  & -4.27  & -3.46 & -3.66 & 0.51 & 16.87  &  & -1.95 & -1.81 & -1.85 & 0.23 & 5.54   &  & -0.94 & -0.91 & -0.92 & 0.12 & 2.21   &  & -0.42 & -0.41 & -0.41 & 0.11 & 1.05   \\
1.1                  & -3.65  & -3.46 & -3.60 & 0.56 & 1.39   &  & -1.75 & -1.81 & -1.84 & 0.24 & -4.92  &  & -0.86 & -0.91 & -0.92 & 0.12 & -6.37  &  & -0.38 & -0.41 & -0.41 & 0.11 & -7.62  \\
1.2                  & -3.23  & -3.46 & -3.56 & 0.60 & -9.28  &  & -1.61 & -1.81 & -1.83 & 0.25 & -11.92 &  & -0.81 & -0.91 & -0.92 & 0.12 & -12.16 &  & -0.35 & -0.41 & -0.41 & 0.11 & -13.57 \\ \hline
\end{tabular}
}
\caption{ (100 $\times$) Monte Carlo bias of the CSS estimator of $d$ with unknown and known $\mu_0$ and the MCSS estimator of $d$, together with the bias-corrected MCSS estimator of $d$. $\Delta \\label{table1}
\end{table}
\vfill






\vfill
\begin{table}[H]
\centering
\resizebox{1.45\textwidth}{!}{\begin{tabular}{EEEEEEEEEEEEEEEEEEEEEEEE}
\hline
        &
  \multicolumn{1}{c}{MSE($\hat{d}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{bcm}$)} &
  \multicolumn{1}{c}{$\Delta \ &
 \multicolumn{1}{c}{MSE($\hat{d}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{bcm}$)} &
  \multicolumn{1}{c}{$\Delta \   &
  \multicolumn{1}{c}{MSE($\hat{d}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{bcm}$)} &
  \multicolumn{1}{c}{$\Delta \   &
 \multicolumn{1}{c}{MSE($\hat{d}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{bcm}$)} &
  \multicolumn{1}{c}{$\Delta \\multicolumn{1}{c}{$d_0$ \textbackslash{} $T$} & \multicolumn{5}{c}{32}                 &  & \multicolumn{5}{c}{64}                &  & \multicolumn{5}{c}{128}               &  & \multicolumn{5}{c}{256}               \\ \hline
-0.2 & 6.33 & 2.79 & 4.39 & 4.27 & 44.18 &  & 2.04 & 1.18 & 1.58 & 1.55 & 29.11 &  & 0.77 & 0.54 & 0.65 & 0.64 & 18.43 &  & 0.32 & 0.26 & 0.29 & 0.29 & 10.53 \\
-0.1 & 6.44 & 2.79 & 4.40 & 4.28 & 46.43 &  & 2.06 & 1.18 & 1.58 & 1.55 & 30.04 &  & 0.77 & 0.54 & 0.65 & 0.64 & 18.85 &  & 0.32 & 0.26 & 0.29 & 0.29 & 10.72 \\
0.0 & 6.54 & 2.79 & 4.39 & 4.26 & 49.09 &  & 2.07 & 1.18 & 1.58 & 1.55 & 31.12 &  & 0.78 & 0.54 & 0.65 & 0.64 & 19.32 &  & 0.32 & 0.26 & 0.29 & 0.29 & 10.92 \\
0.1 & 6.62 & 2.79 & 4.35 & 4.22 & 52.18 &  & 2.08 & 1.18 & 1.57 & 1.54 & 32.39 &  & 0.78 & 0.54 & 0.65 & 0.64 & 19.86 &  & 0.32 & 0.26 & 0.29 & 0.29 & 11.14 \\
0.2 & 6.64 & 2.79 & 4.27 & 4.13 & 55.53 &  & 2.08 & 1.18 & 1.55 & 1.52 & 33.90 &  & 0.78 & 0.54 & 0.64 & 0.64 & 20.47 &  & 0.32 & 0.26 & 0.29 & 0.29 & 11.39 \\
0.3 & 6.60 & 2.79 & 4.14 & 4.00 & 59.28 &  & 2.06 & 1.18 & 1.52 & 1.49 & 35.46 &  & 0.77 & 0.54 & 0.64 & 0.63 & 21.01 &  & 0.32 & 0.26 & 0.29 & 0.29 & 11.62 \\
0.4 & 6.47 & 2.79 & 3.99 & 3.84 & 62.24 &  & 2.01 & 1.18 & 1.48 & 1.44 & 36.37 &  & 0.76 & 0.54 & 0.63 & 0.62 & 21.05 &  & 0.32 & 0.26 & 0.29 & 0.28 & 11.60 \\ \hline
0.5 & 6.22 & 2.79 & 3.82 & 3.66 & 62.68 &  & 1.92 & 1.18 & 1.42 & 1.39 & 35.25 &  & 0.73 & 0.54 & 0.61 & 0.60 & 19.63 &  & 0.31 & 0.26 & 0.28 & 0.28 & 10.65 \\ \hline
0.6 & 5.77 & 2.79 & 3.62 & 3.46 & 59.24 &  & 1.78 & 1.18 & 1.36 & 1.33 & 30.79 &  & 0.68 & 0.54 & 0.59 & 0.58 & 16.01 &  & 0.30 & 0.26 & 0.27 & 0.27 & 8.28 \\
0.7 & 5.18 & 2.79 & 3.42 & 3.26 & 51.47 &  & 1.62 & 1.18 & 1.31 & 1.27 & 23.70 &  & 0.63 & 0.54 & 0.57 & 0.56 & 11.15 &  & 0.28 & 0.26 & 0.27 & 0.27 & 5.35 \\
0.8 & 4.54 & 2.79 & 3.22 & 3.08 & 40.77 &  & 1.47 & 1.18 & 1.27 & 1.23 & 16.20 &  & 0.59 & 0.54 & 0.56 & 0.55 & 6.84 &  & 0.27 & 0.26 & 0.27 & 0.26 & 3.06 \\
0.9 & 3.99 & 2.79 & 3.09 & 2.95 & 29.21 &  & 1.35 & 1.18 & 1.23 & 1.20 & 9.99 &  & 0.57 & 0.54 & 0.55 & 0.54 & 3.91 &  & 0.27 & 0.26 & 0.26 & 0.26 & 1.68 \\
1.0 & 3.58 & 2.79 & 3.00 & 2.87 & 19.55 &  & 1.29 & 1.18 & 1.21 & 1.18 & 5.80 &  & 0.56 & 0.54 & 0.55 & 0.54 & 2.17 &  & 0.26 & 0.26 & 0.26 & 0.26 & 0.93 \\
1.1 & 3.31 & 2.79 & 2.95 & 2.82 & 12.37 &  & 1.24 & 1.18 & 1.20 & 1.17 & 3.23 &  & 0.55 & 0.54 & 0.54 & 0.54 & 1.17 &  & 0.26 & 0.26 & 0.26 & 0.26 & 0.51 \\
1.2 & 3.13 & 2.79 & 2.91 & 2.79 & 7.51 &  & 1.22 & 1.18 & 1.20 & 1.17 & 1.71 &  & 0.55 & 0.54 & 0.54 & 0.53 & 0.58 &  & 0.26 & 0.26 & 0.26 & 0.26 & 0.26 \\ \hline
\end{tabular}
}
\caption{(100 $\times$)  Monte Carlo MSE of the CSS estimator of $d$ with unknown and known $\mu_0$ and the MCSS estimator of $d$, together with the bias-corrected MCSS estimator of $d$. $\Delta \\label{table4}
\end{table}
\vfill


\end{landscape}



\section{Generalisation} \label{secgen}


We now consider an extension of the analysis to the case where the short-run dynamics are allowed to have a more general structure than the simple IID shocks assumed so far in model \eqref{q1}. Theoretical aspects are presented in Section \ref{theorygen}, including the derivation of the MCSS estimator and of the analytical bias expression for the general model. Following that, Section \ref{specialcases} focuses on obtaining analytic expressions for the asymptotic biases of specific models. This encompasses bias expressions for the ARFIMA(1,$d$,0) model in Section \ref{arfima1d}, and bias expressions for short-memory models in Section \ref{bshortm}. The analysis concludes with a simulation study presented in Section \ref{Ssimgen}.





\subsection{Asymptotic biases} \label{theorygen}

The extended model is given by 
\begin{align}
    x_t &= \mu I(t \geq 1) + \Delta_+^{-d} u_t, \label{genq1}\\
    u_t &= \omega(L;\varphi) \epsilon_t. \label{genq2}
\end{align} 
where $t = 0,\pm 1,\pm 2,\ldots $. The lag polynomial $\omega$ captures the short-run dependence structure parametrically and is given by 
\begin{align}
     \omega(L;\varphi) = \sum_{j = 0}^{\infty} \omega_j(\varphi) L^j, \label{repmainf}
\end{align}
where $\varphi$ is an unknown $p\times1$ vector and $\omega_0(\varphi) = 1$, $|\omega(s;\varphi)| \neq 0$ for $|s| \leq 1$, and $\sum_{j = 0}^{\infty} |\omega_j(\varphi)| < \infty$. More precise conditions on $\omega$  will be specified below.
The representation of $u_t$ in \eqref{genq2} as a MA$(\infty)$ model in \eqref{repmainf} is common in the literature and considered by, among others, \textcite{hualde2011gaussian} and \textcite{hualde2020truncated,hualde2021truncated}. One well-known special case of $u_t$ in \eqref{genq2} is an ARMA$(p_1,p_2)$ model which is given by 
\begin{align}
    \omega(L;\varphi) = \frac{\alpha(L;\varphi)}{\beta(L;\varphi)}, \label{arma}
\end{align}
where $\beta(L;\varphi)$ is the AR polynomial of order $p_1$ and  $\alpha(L;\varphi)$ is the MA polynomial of order $p_2$. It is assumed that the polynomials do not have common roots and that their roots lie outside the unit circle. Then \eqref{genq1}, \eqref{genq2} and \eqref{arma} is an ARFIMA$(p_1,d,p_2)$ model. Another special case of \eqref{repmainf} is \citeauthor{bloomfield1973exponential}'s (1973) exponential spectrum model, see \textcite{robinson1994efficient} and \textcite{hassler2019time}. 

Following \textcite{hualde2020truncated}, we make the following assumptions. We use the notation $\vartheta = (d,\varphi')'$, with true value denoted again by subscript 0, i.e.\ $\vartheta_0 = (d_0,\varphi_0')'$. 



\begin{assumption}\label{A2}
The errors $\epsilon_t$ are \textit{IID}(0,$\sigma_0^2$) with finite fourth moments.
\end{assumption}




\begin{assumption}\label{A3}
The parameter space for $\vartheta = (d,\varphi')'$ is given by $\Theta  = [\nabla_1,\nabla_2] \times \Phi$, with $-\infty < \nabla_1 < \nabla_2 < \infty$ and $\Phi$ being a compact and convex subset of $\mathbb{R}^p$. The true value $\vartheta_0 = (d_0,\varphi_0')' \in \Theta $ with $d_0$ not equal to 1/2. The parameter space for $\mu$ is $\mathbb{R}$.
\end{assumption}



\begin{assumption}\label{A1}
\begin{enumerate}[label=(\roman*)]
    \item For all $\varphi \in \Phi \backslash \{\varphi_0 \}$, $|\omega(s;\varphi)|\neq|\omega(s;\varphi_0)|$  on a set $S \subset \{ s : |s| = 1 \}$ of positive Lebesgue measure. \label{i}
    \item For all $\varphi \in \Phi$, $\omega(e^{i\lambda};\varphi)$ is differentiable in $\lambda$ with derivative in Lip$(\varsigma )$ for $1/2 < \varsigma  \leq 1$. 
    \item For all $\lambda$, $\omega(e^{i\lambda};\varphi)$ is continuous in $\varphi$.
    \item For all $\varphi \in \Phi $, $|\omega(s;\varphi)|\neq 0$, $|s| \leq 1$. 
    \item The true value $\vartheta_0$ is in the interior of $\Theta$.
    \item For all $\lambda$,  $\omega(e^{i\lambda};\varphi)$ is thrice continuously differentiable in $\varphi$ in a closed neighbourhood $\mathcal{N}_{\varepsilon}(\varphi_0)$ of radius $\varepsilon \in (0,1/2)$ about $\varphi_0$. For all $\varphi \in \mathcal{N}_{\varepsilon}(\varphi_0)$ these partial derivatives with respect to $\varphi$ are themselves differentiable in $\lambda$ with derivative in Lip$(\varsigma )$ for $1/2 < \varsigma  \leq 1$. 
    \item The matrix 
    \begin{align}
    A = \begin{pmatrix}
\pi^2/6 & - \sum_{j = 1}^{\infty} b_{\varphi' j}(\varphi_0)/j \\
- \sum_{j = 1}^{\infty} b_{\varphi j}(\varphi_0)/j  & \sum_{j = 1}^{\infty} b_{\varphi j}(\varphi_0) b_{\varphi' j}(\varphi_0)
\end{pmatrix}  \label{genA}
\end{align}
is nonsingular, where 
\begin{align}
  b_{\varphi j}(\varphi_0) = \sum_{k = 0}^{j-1} \omega_k(\varphi_0) \partial \phi_{j-k}(\varphi_0)/\partial \varphi  \label{bphii}
\end{align}
and where $\phi_{j}$ is defined by 
\begin{align}
 \phi(s;\varphi) = \omega^{-1}(s;\varphi) = \sum_{j = 0}^{\infty} \phi_j(\varphi) s^j.   \label{inverselag}
\end{align}
    \end{enumerate}
\end{assumption}


Assumption \ref{A2} is the same as Assumption \ref{a1}. In fact, the IID assumption can be weakened to martingale difference series as in \textcite{hualde2020truncated,hualde2021truncated} but for the sake of convenience we keep this
condition simple. Assumption \ref{A3} extends Assumption \ref{a2} by including the parameter space of $\varphi$. Assumption \ref{A1}$(i)$-$(iv)$, which ensures the identification of short-term dynamics, is standard in the
literature on parametric short-memory models since its introduction by \textcite{hannan1973asymptotic}. Assumption \ref{A1}$(v)$-$(vii)$ serve as additional regulatory conditions necessary to establish the asymptotic distribution
theory. We refer to the papers of \textcite{hualde2011gaussian}, \textcite{nielsen2015asymptotics}, \textcite{hualde2020truncated,hualde2021truncated} for a discussion. Importantly, Assumption \ref{A1} is satisfied for the stationary and
invertible ARMA model and also the exponential spectrum model of \textcite{bloomfield1973exponential}.



For any $(d,\varphi,\mu)$ in the admissible parameter space, define the residuals $\epsilon_t(d,\varphi,\mu) =  \phi(L;\varphi) \Delta_+^{d}(x_t-\mu)$. The CSS objective function is then given by 
\begin{align}
    L(d,\varphi,\mu) &= \frac{1}{2} \sum_{t = 1}^T \epsilon^2_t(d,\varphi,\mu), \nonumber \\
                     &=   \frac{1}{2} \sum_{t = 1}^T \left(  \phi(L;\varphi)\Delta_{+}^{d} x_t- \mu c_t(d,\varphi)  \right)^2, \label{genlikmu1}
\end{align}
where we define the convoluted coefficient
\begin{align}
    c_t(d,\varphi) = \phi(L;\varphi)\Delta_{+}^{d}  I(t \geq 1) = \sum_{j = 0}^{t-1} \phi_j(\varphi) \kappa_{0(t-j)}(d), \label{convcoef}
\end{align}
with $\kappa_{0t}(d)$ given in \eqref{definitionkappa}. Since \eqref{genlikmu1} is quadratic in $\mu$ we can concentrate it. Differentiating with respect to $\mu$ and solving yields 
\begin{align}
    \hat{\mu}(\vartheta) = \frac{\sum_{t = 1}^T ( \phi(L;\varphi)\Delta_{+}^{d} x_t)c_t(d,\varphi)}{\sum_{t = 1}^T c^2_t(d,\varphi)}. \label{genmu1}
\end{align}
with the profile CSS function 
\begin{align} 
    L^*(\vartheta) = \frac{1}{2} \sum_{t = 1}^T \left(  \phi(L;\varphi)\Delta_{+}^{d} x_t- \hat{\mu}(d,\varphi) c_t(d,\varphi)  \right)^2. \label{genL1}
\end{align} 
Note that, as in Section 2, we use asterisks to emphasise that we are dealing with a \textit{profile} objective
function. The resulting CSS estimator of $\vartheta = (d,\varphi')'$ is given by
\begin{align}
    \hat{\vartheta} = \operatorname*{argmin}_{\vartheta \in \Theta}  L^*(\vartheta). \label{genCSS}
\end{align}


\textcite{hualde2020truncated} show that if $x_t$ is generated by \eqref{genq1}-\eqref{repmainf} and if Assumptions \ref{A2} to \ref{A1} hold, then, as $T \rightarrow \infty$,  
\begin{align}
    \hat{\vartheta} \overset{p}{\rightarrow} \vartheta_0, 
\end{align}
and
\begin{align}
    \sqrt{T} (\hat{\vartheta} - \vartheta_0 ) \xrightarrow{d} N(0_{p+1},A^{-1}). \label{limitdistgen}
\end{align}
where $A$ is given in \eqref{genA}.

For comparison, we also analyse the situation where the true $\mu_0$ is known, as in Section \ref{subsect22}. The CSS estimator for this model can be derived by substituting $\mu_0$ into \eqref{genlikmu1} to have 
\begin{align}
    L^*_{\mu_0}(\vartheta)   &=   \frac{1}{2} \sum_{t = 1}^T \left(  \phi(L;\varphi)\Delta_{+}^{d} x_t- \mu_0 c_t(d,\varphi)  \right)^2, \label{genlikmu1known}
\end{align}
such that 
\begin{align}
    \hat{\vartheta}_{\mu_0} = \operatorname*{argmin}_{\vartheta \in \Theta} L^*_{\mu_0}(\vartheta). \label{genCSSmuknown}
\end{align}
\textcite{hualde2011gaussian} show that if $x_t$ is generated by \eqref{genq1}-\eqref{repmainf} and if Assumptions \ref{A2} to \ref{A1} hold, then this estimator is consistent, too, and attains the same limiting distribution as $\hat{\vartheta}$ in \eqref{genCSS}, see \eqref{limitdistgen}.  

The CSS objectives in \eqref{genL1} and \eqref{genlikmu1known} may again be motivated in terms of a Gaussian likelihood, see \textcite{hualde2020truncated}. Therefore, as in Section \ref{sectionmod}, it will be instructive to interpret the CSS objectives in \eqref{genL1} and \eqref{genlikmu1known} as profile likelihood functions. As such, they are not genuine likelihoods, for they are not based on observable quantities. Instead, they are functions of the maximum likelihood estimators of $\mu$ and $\sigma^2$ which are treated as if they were the true parameter values. This shows that profile likelihoods do not necessarily possess the same properties as genuine likelihoods. Accordingly, we show below that the score function of $L^*(d,\varphi)$ in \eqref{genL1} evaluated at the true parameters is biased. The score function of $L^*_{\mu_0}(d,\varphi)$ in \eqref{genlikmu1known} is unbiased.

For simplicity, we will assume in the sequel that the initial observations of the short-run dynamics are equal to $0$ for $t\leq 0$. Alternatively, an argument of asymptotic negligibility of pre-sample observations as in \textcite[Lemma 2]{hualde2011gaussian} could be made.
\begin{assumption}\label{A5}
 For all $t \leq 0$, we assume that $\epsilon_t = 0$  in \eqref{genq2}.
\end{assumption} 



The following theorem generalises Theorem \ref{score_1} and Theorem \ref{score_2}. The proof will be given in Appendix \ref{generalizationsappendix}. Note that we use the notation $D_i f(d, \varphi)$ and  $D_{ij} f(d, \varphi)$ to denote, respectively, the first derivative and second derivative of a function $f(d, \varphi)$ with respect to parameters $i,j \in \{d, \varphi\}$.  
\begin{theorem}\label{t52} 
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{genq1}-\eqref{repmainf} and let Assumptions \ref{A2} to \ref{A5} be satisfied. Then, the expected scores of $L^* (d,\varphi)$, evaluated at the true parameter $d_0$ and $\varphi_0$, are given by
\begin{align}
    E( \DLd^*(d_0,\varphi_0)) &= O(\log(T)I(d_0 < 1/2) + I(d_0 > 1/2) ), \label{genscored}\\
    E( \DLvp^*(d_0,\varphi_0)) &=  O(1) \label{genscorevarphi},
\end{align}
when $T \rightarrow \infty$. The expected scores of $L_{\mu_0}^* (d,\varphi)$, evaluated at the true parameter $d_0$ and $\varphi_0$, are given by
\begin{align}
    E( \DLd_{\mu_0}^*(d_0,\varphi_0)) &= 0, \label{genscoredknown}\\
    E( \DLvp_{\mu_0}^*(d_0,\varphi_0)) &= 0_{p}. \label{genscorevarphiknown}
\end{align}

\end{theorem} 











The expectation of the score in \eqref{genscored}, i.e.\ the score with respect to $d$, is not uniform in $d_0$. For $d_0 < 1/2$ it diverges at the rate of $\log(T)$, while it is $O(1)$ for $d_0 > 1/2$. This is not true for the expectation of the score in \eqref{genscorevarphi}, i.e.\ the score with respect to $\varphi$, which is $O(1)$ uniformly in $d_0$. The rationale behind this is that the score bias measures the relative strength of the level parameter and the stochastic component. The score bias with respect to $d$ gauges the strength of the level parameter relative to the fractional dynamics, whereas the score bias with respect to $\varphi$ evaluates the strength of the level parameter in relation to short-run dynamics. Recall that in the pure fractional model the bias of the CSS estimator is a function of the score bias scaled by $T$, see \eqref{biastermscr} in Section \ref{S3}. If an analogous relationship were to hold in the present setting one might be tempted to think, from \eqref{genscored} and \eqref{genscorevarphi}, that in the stationary region the bias of $\hat{d}$ will be of a larger order of magnitude than the bias of $\hat{\varphi}$.
Yet this turns out not to be true. As will be shown below, the biases of $\hat{d}$ and $\hat{\varphi}$ are functions of not only their own score biases but, instead, of a weighted sum of both score biases. This will lead to the bias of the short-run dynamics to behave the same as the order of the bias of the memory parameter. The expectation of the score of $L_{\mu_0}^*(d,\varphi)$ is equal to zero, see \eqref{genscoredknown} and \eqref{genscorevarphiknown}. 

The discussion above again highlights the need for a modification of the CSS profile likelihood $L^*(d_0,\varphi_0)$ such that it behaves more like a genuine likelihood or, equivalently, more like  $L_{\mu_0}^*(d_0,\varphi_0)$. Following the same arguments as in Section \ref{S2}, we consider a multiplicative modification term for the profile CSS objective function in \eqref{genL1}. We therefore define again the modified profile CSS objective function as  
\begin{align}
    L^*_{m}(\vartheta) &= m(\vartheta) L^*(\vartheta),  \label{genmlik}
\end{align}
where the multiplicative modification term $m(\vartheta) > 0$ depends only on $\vartheta$. The corresponding $(p+1)\times1$ vector of scores is the first derivative of \eqref{genmlik}:
\begin{align}
    \DLvt^*_{m}(\vartheta) = m(\vartheta) \DLvt^*(\vartheta) +  D_{\vartheta} m(\vartheta)   L^*(\vartheta). \label{gentibmc12}
\end{align}
We again require that our objective function is score unbiased, i.e.\ that the score functions in \eqref{gentibmc12} satisfy
\begin{align}
    E \left( \DLvt^*_{m}(\vartheta_0)  \right) = 0_{p+1}, \label{gentibmc13}
\end{align}
cf.\ \eqref{tibmc13}. It is important to stress that \eqref{gentibmc13} requires all the scores to be unbiased. As was alluded to above and will be shown below, this is due to the fact that the bias of the CSS estimator depends on the biases of all scores and not of their own scores only. Using \eqref{gentibmc12} and the fact that $ D_{\vartheta}\log (m(\vartheta)) = D_{\vartheta} m(\vartheta) /m(\vartheta)$ and $m(\vartheta) > 0$ it follows that \eqref{gentibmc13} is equivalent to the condition that
\begin{align}
     D_{\vartheta} \log\left( m(\vartheta_0) \right) = -\frac{E\left( \DLvt^*(\vartheta_0) \right)}{E\left(L^*(\vartheta_0) \right)}. \label{solveformgen}
\end{align}
Evaluating the right-hand side of \eqref{solveformgen} yields
\begin{align}
  D_{\vartheta} \log\left( m(\vartheta_0) \right) = \frac{2}{T-1} \frac{ \sum_{t = 1}^T c_t(\vartheta_0)  D_{\vartheta}c_t(\vartheta_0)   }{  \sum_{t = 1}^T  c^2_t(\vartheta_0) }, \label{genanitd}
\end{align}
see Lemma \ref{genlemmaexpectations}.
As can be easily seen, the primitive function of \eqref{genanitd} is
\begin{align*}
   \log\left(  m(\vartheta) \right) =  \log \left( \sum_{t = 1}^T c^2_{t}(\vartheta) \right)^{\frac{1}{T-1}}.
\end{align*}
Finally, exponentiation yields
\begin{align}
     m(\vartheta) &= \left( \sum_{t = 1}^T c^2_t(\vartheta)  \right)^{\frac{1}{T-1}}, \label{genmodificationterm}
\end{align}
cf.\ \eqref{modificationterm}. The modified profile CSS objective function in \eqref{genmlik} is thus given by the product of $ m(\vartheta)$ in \eqref{genmodificationterm} and $ L^*(\vartheta)$ in \eqref{genL1}, i.e.\
\begin{align*}
   L_m^*(\vartheta) = \left( \sum_{t = 1}^T c^2_t(d,\varphi)  \right)^{\frac{1}{T-1}}  \frac{1}{2} \sum_{t = 1}^T \left(  \phi(L;\varphi)\Delta_{+}^{d} x_t- \hat{\mu}(d,\varphi) c_t(d,\varphi)  \right)^2 .
\end{align*}
We again call the estimator that minimises $L_m^*(\vartheta)$ the modified conditional sum-of-squares (MCSS) estimator, as in Section \ref{Smcss}, and denote the estimator of $\vartheta$ by $\hat{\vartheta}_{m}$, i.e.\,
\begin{align}
    \hat{\vartheta}_{m} = \operatorname*{argmin}_{\vartheta \in \Theta} L^*_{m}(\vartheta).  \label{MCSSgen1}
\end{align}

The following lemma shows that the two properties of the modification term $m(d)$ in Lemma \ref{lm:md} carry over, to $m(\vartheta)$ in \eqref{genmodificationterm}. As such, the modification term $m(\vartheta)$ acts again as penalisation through inflating the CSS objective function $L^*(\vartheta)$ by the factor $m(\vartheta)$. See Appendix \ref{lemma31} for the proof.     

\begin{lemma} \label{gen:lm:md}
  For all $d \in \mathbb{R} $ and $\varphi \in \Phi$,
  \begin{align}
    m(\vartheta) &\geq 1. \label{genmqeq1}
    \end{align}
Here, equality holds if $d$ = 1 and $\varphi = 0$.
Also, it holds that, for $T \rightarrow \infty$,
    \begin{align}
    m(\vartheta) &= 1 + O(T^{-1}\log(T) I(d < 1/2) + T^{-1} I(d > 1/2) ) \label{geneq111} 
  \end{align}
   for all $d \in \mathbb{R} \backslash \{1/2\}$. 
\end{lemma}



The asymptotic properties of the MCSS estimator are the same as of the CSS estimator and are summarised for completeness by the following theorem. The proof is given in Appendix \ref{generalizationsappendix}. The result follows from the fact that the modification term  $m(\vartheta) \rightarrow 1$, see \eqref{geneq111} in Lemma \ref{gen:lm:md}.
\begin{theorem}\label{t51} 
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{genq1}-\eqref{repmainf} and let Assumptions \ref{A2} to \ref{A1} be satisfied. Then, as $T \rightarrow \infty$, 
\begin{align}
    \hat{\vartheta}_{m} \overset{p}{\rightarrow} \vartheta_0,
\end{align}
and
\begin{align}
    \sqrt{T} (\hat{\vartheta}_{m} - \vartheta_0 ) \xrightarrow{d} N(0_{p+1},A^{-1}), \label{genmcssdis}
\end{align}
where $A$ is given in \eqref{genA}.
\end{theorem} 
However, the asymptotic bias of the estimator $\hat{\vartheta}_{m}$ behaves differently from that of $\hat{\vartheta}$. In order to find the asymptotic biases, we proceed in a similar fashion as in Section \ref{S3}, involving two steps: finding the asymptotic expansion of $\hat{\vartheta}$ and approximating this expansion. The bottom line of the derivation is that 
\begin{align}
    E\left( \hat{\vartheta} - \vartheta_0 \right) = S_T(d_0,\varphi_0) + B_T(\varphi_0) + o(T^{-1}), \label{trian}
\end{align}
where we call $S_T(d_0,\varphi_0) = - A^{-1} T^{-1}\left[ \sigma^{-2}_0 E \left( \DLvt^*(\vartheta_0) \right)  \right]$
the score bias and $B_T(\varphi_0)$ the intrinsic bias. Detailed expressions are given in Appendix \ref{generalizationsappendix}. 

Importantly, we refer to $S_T(d_0,\varphi_0)$ and $B_T(\varphi_0)$ as ``exact'' biases, as we evaluate the expectations terms without taking the limits. The asymptotic counterparts, for $T \rightarrow \infty$ with appropriate scaling,
are denoted by $S(d_0,\varphi_0)$ and $B(\varphi_0)$.  We refer to these asymptotic biases as ``approximate''. \textcite{lieberman2005expansions} also make a comparable dichotomy concerning the Edgeworth expansions of the memory
parameter. The rationale behind emphasising the distinction lies in the exact and approximate biases being potentially very different from each other. This is because the number of quantities in $B_T(\varphi_0)$ that are
approximated by $B(\varphi_0)$ amounts to a maximum of $3\times (p+1)^3 + (p+1)^2$. If $T$ is relatively small, the difference between $B_T(\varphi_0)$ and $B(\varphi_0)$ can therefore be substantial.



It follows from Lemmata \ref{asyappgennon2}, \ref{asyappgennon3}, \ref{asyappgenstat2} and \ref{asyappgenstat3} that analogues of \eqref{trian} also hold for $\hat{\vartheta}_{\mu_0}$ and $\hat{\vartheta}_m$. Replacing
$\DLvt^*(\vartheta_0)$ by $\DLvt_{\mu_0}^*(\vartheta_0)$, it is clear that the expected score term gets eliminated. Similarly, $E (\DLvt_{m}^*(\vartheta_0)) = 0$ by construction, justifying the MCSS objective function in order to
obtain score unbiasedness in all parameters. Indeed, the leading bias of $\hat{\vartheta}_m$ is the same as that of $\hat{\vartheta}_{\mu_0}$. The following theorem is the main result of this paper and presents the approximate
bias of $\hat{\vartheta}$, $\hat{\vartheta}_{\mu_0}$ and $\hat{\vartheta}_{m}$. The proof is given in Appendix \ref{generalizationsappendix}.
\begin{theorem}\label{t53} 
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{genq1}-\eqref{repmainf} and let Assumptions \ref{A2} to \ref{A5} be satisfied. The approximate biases of $\hat{\vartheta}$, $\hat{\vartheta}_{\mu_0}$ and $\hat{\vartheta}_{m}$ are 
\begin{align}
    bias(\hat{\vartheta}) &= S(d_0,\varphi_0) + B(\varphi_0) + o(T^{-1}), \\
    bias(\hat{\vartheta}_{\mu_0}) &= B(\varphi_0) + o(T^{-1}), \\
    bias(\hat{\vartheta}_{m}) &=  B(\varphi_0) + o(T^{-1}),
\end{align}
where the intrinsic bias $B(\varphi_0)$ is given in \eqref{BtINT} of Appendix \ref{generalizationsappendix}. The score bias for $d_0 > 1/2$ is given by  
\begin{align*}
    TS(d_0,\varphi_0) =  A^{-1} \frac{\sum_{t = 1}^{\infty} c_{t}(\vartheta) c_{\vartheta t}(\vartheta)}{ \sum_{t = 1}^{\infty} c^2_{t}(\vartheta) } 
\end{align*}
while the score bias for $d_0 < 1/2$ is given by 
\begin{align*}
    TS(d_0,\varphi_0) = A^{-1}  \begin{bmatrix}
  -\log(T)+\left(\Psi(1-d_0) + (1-2d_0)^{-1}\right) \\
\frac{D_{\varphi_1 } \phi(1;\varphi)}{ \phi(1;\varphi) } \\
\vdots    \\
\frac{D_{\varphi_p } \phi(1;\varphi)}{ \phi(1;\varphi) }
\end{bmatrix}
\end{align*}
where $A$ is given in \eqref{genA}, $c_{t}(\vartheta)$ in \eqref{convcoef} and  $\phi(1;\varphi)$ in \eqref{inverselag}.  Furthermore, the intrinsic bias $B(\varphi_0) = O(T^{-1})$, whereas the score bias $S(d_0,\varphi_0) = O(T^{-1} \log(T))$ in the stationary region and $S(d_0,\varphi_0) = O(T^{-1})$ in the non-stationary region. 
\end{theorem} 


The approximate biases reveal two key points. Firstly, the bias in the estimators is again composed of two terms: the score bias and the intrinsic bias. As outlined earlier in the discussion below Theorem \ref{t52}, the score bias measures the relative strength of the deterministic component when contrasted with the stochastic component. The intrinsic bias is what remains even if the true value of the deterministic component $\mu_0$ were known. The score bias term will dominate as the leading bias term in the stationary region. Secondly, the value of the memory parameter solely affects the bias through the score bias, not through the intrinsic bias, the latter merely depending on the short-run dynamics $\varphi_0$. 


As discussed in Section \ref{S3}, to quantify the accuracy of test statistics or confidence intervals for $\hat{\vartheta}_0$, the relevant quantity is the relative bias, i.e.\ the ratio of asymptotic bias to the asymptotic standard deviation. Theorem \ref{t53} implies that the relative bias for the three estimators is of order $O(T^{-1/2})$ in the non-stationary region. In the stationary region, the relative bias is of order $O(T^{-1/2} \log(T))$ for the CSS estimator with unknown $\mu_0$, while it is of order $O(T^{-1/2})$ for the CSS estimator with known $\mu_0$ and the MCSS estimator. Thus, especially in the stationary region, the $t$-test for the memory parameter as well as the short-run dynamics are more accurate when using the MCSS estimator compared to the CSS estimator. Later in the empirical study, we will exploit this feature to our advantage.   





\subsection{Special cases} \label{specialcases}

In this section, we find analytic expressions of the asymptotic biases for specific models. Section \ref{arfima1d} presents bias expressions for the ARFIMA(1,$d$,0) model, while Section \ref{bshortm} covers bias expressions for short-memory models, concluding with the biases of the AR(1) model.

\subsubsection{ARFIMA(1,\texorpdfstring{\(d\)}{d},0) model} \label{arfima1d}

Theorem \ref{t53} enables us to express the bias of particular models explicitly. One such model is the ARFIMA(1,$d$,0), commonly used for modelling fractional integrated time series. In a Monte Carlo simulation performed by \textcite{nielsen2005finite}, this specific model underwent thorough analysis. This section will provide an explanation of their findings. In the following theorem, we will present the approximate bias expression for this particular model.
\begin{theorem}\label{t54} 
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{genq1} and let $u_t = \varphi u_{t-1} + \epsilon_t$. Let Assumptions \ref{A2} to \ref{A5} be satisfied. The approximate biases of $\hat{\vartheta}$, $\hat{\vartheta}_{\mu_0}$ and $\hat{\vartheta}_{m}$ are 
\begin{align}
    bias(\hat{\vartheta}) &= S(d_0,\varphi_0)  + B(\varphi_0) + o(T^{-1}), \\
    bias(\hat{\vartheta}_{\mu_0}) &= B(\varphi_0) + o(T^{-1}), \\
    bias(\hat{\vartheta}_{m}) &=  B(\varphi_0) + o(T^{-1}),
\end{align}
where the intrinsic bias $B(\varphi_0)$ is given by 
\begin{align*}
    T B(\varphi) &=   A^{-1}   \begin{bmatrix}
\iota' \left( A^{-1} \odot \left(G_1 + F_1 \right) \right)\iota \\
\iota' \left( A^{-1} \odot \left(G_2 + F_2 \right) \right)\iota 
\end{bmatrix} - \frac{1}{2} A^{-1} \begin{bmatrix}
\iota' \left(\left(A^{-1} C_{01} A^{-1} \right) \odot A \right) \iota \\
\iota' \left(\left(A^{-1} C_{02} A^{-1} \right) \odot A \right) \iota
\end{bmatrix} \\
 A &= \begin{bmatrix}
\pi^2/6 &  - \varphi^{-1} \log(1-\varphi) \\
 - \varphi^{-1} \log(1-\varphi)  &  (1-\varphi^2)^{-1}
\end{bmatrix}  \\
C_{01} &= \begin{bmatrix}
  -6 \zeta_3 &  2 \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi}) - \varphi^{-1} \log^2(1-\varphi) \\
 2 \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi}) - \varphi^{-1} \log^2(1-\varphi)  & 2 \frac{\log(1 - \varphi)}{1-\varphi^2}
\end{bmatrix} \\
C_{02} &= \begin{bmatrix}
 2 \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi}) - \varphi^{-1} \log^2(1-\varphi)  & 2 \frac{\log(1 - \varphi)}{1-\varphi^2} \\
 2 \frac{\log(1 - \varphi)}{1-\varphi^2} & 0
\end{bmatrix} \\
  F_{1} &= \begin{bmatrix}
-2 \zeta_3 & -\varphi^{-1}\log^2(1-\varphi)   \\
  \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi}) &  \frac{\log(1-\varphi)}{1-\varphi^2}
\end{bmatrix} \\
F_{2} &= \begin{bmatrix}
  \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi})   & \frac{\log(1-\varphi)}{1-\varphi^2}\\
 0  & 0
\end{bmatrix} \\
   G_{1} &= \begin{bmatrix}
-4 \zeta_3 &  2 \varphi^{-1} Li_2(-\frac{\varphi}{1-\varphi}) \\
 - \varphi^{-1} \log^2(1- \varphi) + \varphi^{-1} Li_2(-\frac{\varphi}{1-\varphi}) & \frac{\log(1-\varphi)}{1-\varphi^2} - \varphi^{-2} \left(\frac{\varphi }{1-\varphi } + \log(1-\varphi )\right) 
\end{bmatrix}  \\
 G_{2} &= \begin{bmatrix}
 - \varphi^{-1} \log^2(1- \varphi) + \varphi^{-1} Li_2(-\frac{\varphi}{1-\varphi})  & \frac{\log(1-\varphi)}{1-\varphi^2} - \varphi^{-2} \left(\frac{\varphi }{1-\varphi } + \log(1-\varphi )\right) \\
2\log(1-\varphi) \frac{1}{1-\varphi^2}& -2 \frac{\varphi}{(1-\varphi^2)^2}
\end{bmatrix}.
\end{align*}
The score bias $S(d_0,\varphi_0) $ for $d_0 > 1/2$ is given by
\begin{align*}
     T S(d,\varphi)  &= A^{-1} \left[ (1-\varphi)^2 \binom{2d-2}{d-1} +  \varphi  \binom{2d}{d} \right]^{-1} \times
     \\ 
     &\begin{bmatrix} (1-\varphi)^2  \binom{2d-2}{d-1} 
     \left( \Psi(2d-1)-\Psi(d) \right)  + \varphi  \binom{2d}{d} \left( \Psi(2d+1)-\Psi(d+1) \right)
  \\
(\varphi- 1)   \binom{2d-2}{d-1}  +  0.5\binom{2d}{d}
\end{bmatrix}
\end{align*}
while the score bias $ S(d_0,\varphi_0)$ for $d_0 < 1/2$ is given by
\begin{align}
     T S(d,\varphi)  = A^{-1} 
     &\begin{bmatrix} -\log(T)+\Psi(1-d_0) + (1-2d_0)^{-1}
  \\
-\frac{1}{1-\varphi}
\end{bmatrix} \label{SstatARFI}
\end{align}
where $\zeta_{s}$ is the Riemann's zeta function $\zeta_{s} = \sum_{j = 1}^{\infty} j^{-s}$, $s>1$, and $\Psi(d) = D\log \Gamma(d)$ denotes the Digamma function and $Li_{2}(\varphi) = \sum_{i = 1}^{\infty} i^{-2}\varphi^{i}$ is the dilogarithm function (Spence's integral). The binomial coefficients are represented using the notation $\binom{\cdot}{\cdot}$.
\end{theorem} 





\begin{figure}[H]
  \centering
  \subfloat[$B(\varphi_0)$ ]{
    \includegraphics[width=0.48\textwidth]{approximateiB_cropped.pdf}
  }
  \subfloat[$B_T(\varphi_0)$]{
    \includegraphics[width=0.48\textwidth]{exactiB_cropped.pdf}
  }
  \caption{ The approximate and exact intrinsic bias for the ARFIMA(1,$d$,0) model with $T = 128$ in panel (a) and (b), respectively.}  \label{figib}\end{figure}

Figure \ref{figib}(a) shows the approximate intrinsic bias of both the memory parameter and the autoregressive coefficient. Interestingly, the biases for these two parameters exhibit near-symmetry. Specifically, the bias for the
memory parameter tends to be negative, while the bias for the autoregressive parameter is typically positive. Furthermore, both biases display a degree of moderation and linearity when the true value of $\varphi_0$ is below
0. However, beyond this value, a pronounced acceleration in bias growth becomes evident until reaching a peak at around 0.5; beyond this point, the biases start to diminish rapidly. This same trend was noticed in a Monte Carlo
simulation by \textcite{nielsen2005finite}. They noted that the memory parameter's downward bias is particularly pronounced when the AR coefficient is either 0 or 0.4 and that the estimation methods seem robust against stronger
positive AR coefficient, such as 0.8, which aligns with the bias expression in Theorem \ref{t54}.  Figure \ref{figib}(b) displays the exact intrinsic bias of both the memory parameter and the autoregressive coefficient. The
patterns of the biases closely resemble those of the approximated intrinsic bias in Figure \ref{figib}(a). However, the specific values differ significantly, particularly in the range between 0 and 0.6. This discrepancy suggests
that the asymptotic approximation can lead to notable distortion, especially within this range. Consequently, we suggest to use the exact intrinsic bias when correcting for it.  As we will observe later, these biases also align
with the findings of the simulation study. It is suboptimal to use the approximate intrinsic bias when the sample size is small.


\begin{figure}[H]
    \centering
   
\subfloat[$S(d_0,\varphi_0)$ for $d_0 = -0.2$ ]{
  \includegraphics[width=0.48\textwidth]{ApproxSdn02_cropped.pdf}
}
\subfloat[$S_T(d_0,\varphi_0)$ for $d_0 = -0.2$]{
  \includegraphics[width=0.48\textwidth]{ExactSdn02_cropped.pdf}
}
\hspace{0mm}
   
\subfloat[$S(d_0,\varphi_0)$ for $d_0 = 0.4$ ]{
  \includegraphics[width=0.48\textwidth]{ApproxSd04_cropped.pdf}
}
\subfloat[$S_T(d_0,\varphi_0)$ for $d_0 = 0.4$]{
  \includegraphics[width=0.48\textwidth]{ExactSd04_cropped.pdf}
}

\hspace{0mm}
   
\subfloat[$S(d_0,\varphi_0)$ for $d_0 = 1$]{
  \includegraphics[width=0.48\textwidth]{ApproxSd1_cropped.pdf}
}
\subfloat[$S_T(d_0,\varphi_0)$ for $d_0 = 1$]{
  \includegraphics[width=0.48\textwidth]{ExactSd1_cropped.pdf}
}
    \caption{ The approximate and exact score bias for ARFIMA(1,$d$,0) model with $T = 128$ in the graphs on the left-hand side and right-hand side, respectively.}    \label{figic}\end{figure}

Figure \ref{figic} illustrates the approximate score bias as well as the exact score bias of the ARFIMA(1,$d_0$,0) model with the memory parameter $d_0$ taking on values of $-0.2$, 0.4, and 1. Several observations can be drawn from these score biases. Firstly, the score bias tends to be more pronounced in the stationary region compared to the non-stationary region, which aligns with what we anticipated from Theorem \ref{t52}. Secondly, there exists a noticeable symmetry between the memory parameter and the autoregressive coefficient. Thirdly, a close match is observed between the exact and approximate score biases for $d_0 = -0.2$ and $d$ = 1, except when the autoregressive coefficient approaches 1 in the case of $d_0 = -0.2$. However, this correspondence breaks down when $d_0$ = 0.4; the approximate biases become distorted. This distortion arises due to the presence of the term $(1-2d_0)^{-1}$ in \eqref{SstatARFI}, which diverges as $d_0$ approaches 0.5. \textcite{robinson2015efficient}, who, in a panel setting, correct for the score bias of the CSS estimator, also observe that the precision of the approximate score bias expression diminishes unless the region is non-stationary. It is, therefore, recommendable to employ the exact score biases in practical empirical applications. Importantly, this bias is inherently eliminated by the MCSS estimator, obviating the need for additional correction.



\subsubsection{Short-memory models} \label{bshortm}

Theorem \ref{t53} also covers bias expressions in cases where long memory is absent in the model. This derivation is straightforward and hence the proof is not included; it involves truncating the matrix $B(\varphi_0)$ by removing the components related to long memory and by setting $d_0 = 0$ in $S(d_0,\varphi_0)$. The following theorem presents an approximate bias expression applicable to models characterised by short memory. It is important to emphasise that our model is not restricted to ARMA models alone; rather, it encompasses a broader category of short-memory models, with ARMA models being just one particular instance. Indeed, any representation that conforms to \eqref{repmainf} is allowed, incorporating models like the Bloomfield exponential model.

\begin{theorem}\label{t55} 
Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{genq1} with $d_0 = 0$ and let Assumptions \ref{A2} to \ref{A5} be satisfied. Furthermore, when $d$ is set to zero in the respective objective functions, the approximate biases of $\hat{\varphi}$, $\hat{\varphi}_{\mu_0}$ and $\hat{\varphi}_{m}$ are given by 
\begin{align}
    bias(\hat{\varphi}) &= \tilde{S}(\varphi_0) + \tilde{B}(\varphi_0) + o(T^{-1}), \\
    bias(\hat{\varphi}_{\mu_0}) &= \tilde{B}(\varphi_0)  + o(T^{-1}), \\
    bias(\hat{\varphi}_{m}) &=  \tilde{B}(\varphi_0)  + o(T^{-1}),
\end{align}
where intrinsic bias is given by
\begin{align*}
    T \tilde{B}(\varphi) &=   \tilde{A}^{-1}   \begin{bmatrix}
\iota' \left( \tilde{A}^{-1} \odot \left(\tilde{G}_1 + \tilde{F}_1 \right) \right)\iota \\
\vdots    \\
\iota' \left( \tilde{A}^{-1} \odot \left(\tilde{G}_p + \tilde{F}_p \right) \right)\iota 
\end{bmatrix} - \frac{1}{2} \tilde{A}^{-1} \begin{bmatrix}
\iota' \left(\left(\tilde{A}^{-1} \tilde{C}_{01} \tilde{A}^{-1} \right) \odot \tilde{A} \right) \iota \\
\vdots    \\
\iota' \left(\left(\tilde{A}^{-1} \tilde{C}_{0p} \tilde{A}^{-1} \right) \odot \tilde{A} \right) \iota
\end{bmatrix} \\
\end{align*}
and the score bias is given by  
\begin{align*}
    T\tilde{S}(\varphi_0) = \tilde{A}^{-1} \frac{ D_{\varphi_{k}}\phi(1;\varphi)  }{ \phi(1;\varphi) } 
\end{align*}
with 
\begin{align*}
    \tilde{A} &= \sum_{j = 1}^{\infty} b_{\varphi j}(\varphi_0) b_{\varphi' j}(\varphi_0) \\
    \tilde{F}_m &= \sum_{i = 1}^{\infty}  b_{\varphi \varphi_m i}(\varphi_0) b_{\varphi' i}(\varphi_0) \\
    \tilde{G}_m &= \sum_{k = 1}^{\infty}  \sum_{s = 1}^{\infty} \left(  b_{\varphi_m s}(\varphi_0) b_{\varphi (s+k)}(\varphi_0) + b_{\varphi_m (s+k)}(\varphi_0) b_{\varphi s}(\varphi_0) \right)  b_{\varphi' k}(\varphi_0)\\
    \tilde{C}_{0m} &= \left[ \sum_{i = 1}^{\infty}  b_{\varphi i}(\varphi_0) b_{\varphi' \varphi_m i}(\varphi_0) \right]' + \sum_{i = 1}^{\infty}  b_{\varphi  i}(\varphi_0) b_{\varphi' \varphi_m i}(\varphi_0)  + \sum_{i = 1}^{\infty}  b_{\varphi_m i}(\varphi_0) b_{\varphi \varphi' i}(\varphi_0)
\end{align*}
for $m = 1,\ldots,p$. Here, $b_{\cdot i}(\varphi_0)$ is defined in \eqref{bphii}. 
\end{theorem}


It is instructive to compare these expressions to analogous results for stationary and invertible ARFIMA models in Theorem \ref{t53}. In that context, the bias of the short-run dynamics in the stationary region for the CSS
estimator is of order $O(T^{-1} \log(T))$. This, however, is not true for the bias of the short-run dynamics of the CSS estimator for stationary and invertible ARMA models models, which is of order $O(T^{-1})$, as shown above in
Theorem \ref{t55}.  An extension of a stationary ARMA model to an ARFIMA model increases the bias of the short-run dynamics to be of the same order of magnitude as that of the memory parameter. Nevertheless, the MCSS estimator
effectively eliminates this bias through its construction, leading to a reduction in the bias order of $\hat{\varphi}$ to $O(T^{-1})$ for general ARFIMA or ARMA models.


Based on this theorem, we can deduce the analytic bias of an AR(1) model as an illustration. The following corollary presents the expressions describing the analytic biases of the three estimators. The proof of this corollary is
omitted because it follows straightforwardly.


    
\begin{corollary}\label{t56} 
  Let $x_t$, $t$ = 1,$\ldots$,$T$, be given by \eqref{genq1} with $d_0 = 0$ and let $u_t = \varphi u_{t-1} + \epsilon_t$. Let Assumptions \ref{A2} to \ref{A5} be satisfied. Furthermore, when $d$ is set to zero in the respective
  objective functions, the approximate biases of $\hat{\varphi}$, $\hat{\varphi}_{\mu_0}$ and $\hat{\varphi}_{m}$ are given by
\begin{align}
    bias(\hat{\varphi}) &= \tilde{S}(\varphi_0) + \tilde{B}(\varphi_0) + o(T^{-1}), \\
    bias(\hat{\varphi}_{\mu_0}) &= \tilde{B}(\varphi_0)  + o(T^{-1}), \\
    bias(\hat{\varphi}_{m}) &=  \tilde{B}(\varphi_0)  + o(T^{-1}),
\end{align}
where intrinsic bias is given by $T \tilde{B}(\varphi) = -2\varphi$ and the score bias is given by  
$T\tilde{S}(\varphi_0) = -\varphi-1$.
\end{corollary}

The bias expression is the same as the one discovered by \textcite{tanaka1984asymptotic}. This observation is intriguing as Tanaka's focus was on maximum likelihood estimation, while we consider the CSS estimator. This suggests
that the bias of the estimators remains unaffected in this basic model when the initial observation is set to zero.







\subsection{Simulation} \label{Ssimgen}

In this section we conduct a simulation study of the finite sample properties of the CSS estimators with known and unknown $\mu_0$ and of the MCSS estimator. In Section \ref{S2} the focus was on the pure fractional case, here we
incorporate an autoregressive component. In particular, we take as our DGP the model in \eqref{genq1} with $u_t$ an AR(1) model, i.e.\
\begin{align*}
    u_t = \varphi_0 u_{t-1} + \epsilon_t, 
\end{align*}
with $\epsilon_t \sim \textit{NID}(0,1)$. We choose the same values for $d_0$ as in Section \ref{S4} and for the autoregressive parameter we let $\varphi_0 \in \{-0.5,0,0.5 \}$. We computed the estimates using the optimising interval
$d \in [d_0-5,d_0+5]$, $\varphi \in [-0.9999,0.9999]$. All results are based on 10,000 replications.  In accordance with Assumption \ref{A5}, we put the initial observation of the short-run dynamics equal to 0, i.e\ $u_0 = 0$. It
should be noted, however, that this condition is not necessary to achieve consistency and asymptotic normality of the MCSS estimator. 



Table \ref{tablear1} and \ref{tablear2} present the Monte Carlo biases (multiplied by 100) of the memory parameter and the autoregressive parameter, respectively. We also report the percentage increase of the bias of the CSS
estimator relative to the bias of the MCSS estimator by $\Delta \parameter and the autoregressive parameter, respectively. We now summarise the main findings. To explain the simulation results, it is useful to consider each case of $\varphi_0 \in \{ -0.5,0,0.5 \}$ separately.

We will first consider the case where $\varphi_0 = -0.5$. The addition of an autoregressive term to the pure fractional model considerably increases the bias of the CSS estimator of $d$, especially in the stationary region, cf.\
Table \ref{table1}. The CSS estimator $\hat{d}$ clearly underestimates the true $d_0$, while the $\hat{\varphi}$ overestimates the true $\varphi_0$. The MCSS estimator, however, reduces a large part of the bias. The largest
reduction occurs in the stationary region, which is also expected from Theorem \ref{t53}. Importantly, the bias of the MCSS estimator and the bias of the CSS estimator with known $\mu_0$ are close to each other, confirming our
theoretical findings that the leading bias of $\hat{\vartheta}_m$ is the same as that of $\hat{\vartheta}_{\mu_0}$. It can also be seen that the MCSS estimator almost everywhere outperforms the CSS estimator in terms of MSE. The
largest improvement occurs again in the stationary region, which is also where the largest bias reduction occurs according to Theorem \ref{t53}. The CSS estimator with known $\mu_0$ performs the best and outperforms the MCSS
estimator, while the biases are somewhat similar, the variance of this estimator is significantly lower because $\mu_0$ is known.


We next discuss the situation where $\varphi_0 = 0$. This situation is not covered in our theoretical analysis, since Assumption \ref{A1}\ref{i} does not allow for overspecification of the AR polynomial. Nevertheless, this is an
interesting case and a realistic one. Usually the AR lags of the regression model are not known and often a lag selection procedure, such as the one by \textcite{box1990time} or an information criterion, is used to estimate the
true number of lags, see for example \textcite{huang2022consistent}. It is possible and not unlikely that the lag selection procedure or the information criterion overestimates the number of lags. Then, according to our
simulation, the estimated parameters are strongly biased when the level parameter term is not known, so wrong conclusions may be drawn from these approaches. The MCSS estimator, as opposed to the CSS estimator, significantly
reduces the bias and therefore seems a better alternative to base the lag selection procedure on, e.g.\ see \textcite{lee2015model}.  We also note that the biases of the MCSS estimator and the bias of the CSS estimator with known
$\mu_0$ are close to each other. This result indicates that the leading bias terms are indeed the same for both estimators, which was also true for the purely fractional case, see Theorem \ref{t1}. It can also be seen that the
MCSS estimator outperforms everywhere the CSS estimator in terms of MSE. The largest improvement occurs again in the stationary region. The CSS estimator with known $\mu_0$ performs again the best and outperforms the MCSS
estimator.

Finally, we now discuss the situation where $\varphi_0 = 0.5$. The comments made above are also true for the non-stationary region. In particular, the bias of the MCSS estimator is close to that of the CSS estimator with known
$\mu_0$. In the stationary region, however, the two estimators behave differently. Nevertheless, the differences become small for $T = 256$. Furthermore, the CSS estimator performs the worst in terms of the bias, while the other
two estimators significantly improve on this estimator. In terms of the MSE, we see that the MSE of the MCSS estimator and that of the CSS estimator with known $\mu_0$ is significantly lower than that of the CSS estimator. As
opposed to that, the MSE of the CSS estimator for $\varphi$ is lower than that of the MCSS estimator and also the CSS estimator with known $\mu_0$ when $T = 32$. In order to better understand the differences in the bias and MSE of
the estimators, we have plotted in Figure \ref{fig711} densities of the three estimators for the constellation $d_0 = -0.2$, $\varphi_0 = 0.5$ and $T = 32$ (upper panel) and $T = 256$ (lower panel). It can be seen that the CSS
estimators strongly underestimate the true $d_0 = -0.2$ and strongly overestimate the true $\varphi_0 = 0.5$ for $T = 32$. This strong bias in the CSS estimator contrasts with less variation. The CSS estimator's poor performance
extends somewhat to the case $T = 256$. The MCSS and CSS estimators are well-centred, but this centring comes at the cost of an increase in the variance. This explains the differences in the MSE of the CSS estimator relative to
that of the MCSS estimator and the CSS estimator with known $\mu_0$. Also, it appears that for small $T$ the MCSS estimator recentres the memory parameter relatively more than the autoregressive component, explaining the
differences with the bias of the CSS estimator with known $\mu_0$. Nevertheless, these plots show that MCSS density estimates are more similar to CSS density estimates with known $\mu_0$ than to CSS density estimates with unknown
$\mu$. The good finite sample performance of the MCSS estimator is again evident.





\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{EEEEEEEEEEEEEEEEEEEEEE}
\hline
\multicolumn{1}{c}{$\varphi_0$} &
 &
  \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \ &
  \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \   &
  \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \   &
  \multicolumn{1}{c}{bias($\hat{d}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \ & \multicolumn{1}{c}{$d_0$ \textbackslash{} $T$} & \multicolumn{4}{c}{32} &  & \multicolumn{4}{c}{64} &  & \multicolumn{4}{c}{128} &  & \multicolumn{4}{c}{256} \\ \hline
 & -0.2 & -37.62 & -9.02  & -12.15 & 209.55  &  & -13.70 & -3.91  & -4.50  & 204.65  &  & -5.91  & -1.89 & -2.00 & 194.61 &  & -2.88  & -0.90 & -0.90 & 219.88 \\
 & -0.1 & -38.59 & -9.02  & -12.20 & 216.22  &  & -13.86 & -3.91  & -4.54  & 205.57  &  & -5.95  & -1.89 & -2.01 & 196.14 &  & -2.89  & -0.90 & -0.90 & 220.69 \\
 & 0.0  & -39.26 & -9.02  & -12.35 & 217.98  &  & -13.86 & -3.91  & -4.58  & 202.82  &  & -5.96  & -1.89 & -2.01 & 195.97 &  & -2.89  & -0.90 & -0.90 & 220.00 \\
 & 0.1  & -39.60 & -9.02  & -12.35 & 220.59  &  & -13.92 & -3.91  & -4.65  & 199.07  &  & -5.94  & -1.89 & -2.02 & 193.49 &  & -2.87  & -0.90 & -0.91 & 216.75 \\
 & 0.2  & -39.80 & -9.02  & -12.49 & 218.60  &  & -13.87 & -3.91  & -4.65  & 198.41  &  & -5.83  & -1.89 & -2.03 & 186.95 &  & -2.82  & -0.90 & -0.91 & 209.18 \\
 & 0.3  & -39.18 & -9.02  & -12.42 & 215.46  &  & -13.54 & -3.91  & -4.67  & 189.92  &  & -5.60  & -1.89 & -2.04 & 173.99 &  & -2.70  & -0.90 & -0.92 & 194.59 \\
 & 0.4  & -38.01 & -9.02  & -12.35 & 207.92  &  & -12.88 & -3.91  & -4.64  & 177.86  &  & -5.24  & -1.89 & -2.05 & 155.29 &  & -2.49  & -0.90 & -0.92 & 169.89 \\ \cline{2-21} 
 \multicolumn{1}{c}{$-0.5$} & 0.5  & -36.47 & -9.02  & -12.17 & 199.53  &  & -11.83 & -3.91  & -4.58  & 158.39  &  & -4.68  & -1.89 & -2.05 & 128.31 &  & -2.18  & -0.90 & -0.93 & 134.69 \\ \cline{2-21} 
 & 0.6  & -33.69 & -9.02  & -12.10 & 178.53  &  & -10.32 & -3.91  & -4.52  & 128.34  &  & -3.98  & -1.89 & -2.03 & 96.41  &  & -1.82  & -0.90 & -0.93 & 94.92  \\
 & 0.7  & -29.72 & -9.02  & -11.68 & 154.40  &  & -8.73  & -3.91  & -4.43  & 97.08   &  & -3.28  & -1.89 & -1.99 & 64.77  &  & -1.49  & -0.90 & -0.93 & 59.79  \\
 & 0.8  & -25.26 & -9.01  & -10.96 & 130.49  &  & -7.25  & -3.91  & -4.29  & 69.05   &  & -2.72  & -1.89 & -1.95 & 39.25  &  & -1.24  & -0.90 & -0.92 & 34.33  \\
 & 0.9  & -20.84 & -9.02  & -10.39 & 100.68  &  & -5.99  & -3.91  & -4.14  & 44.64   &  & -2.35  & -1.89 & -1.93 & 21.77  &  & -1.08  & -0.90 & -0.91 & 17.89  \\
 & 1.0  & -16.97 & -9.02  & -9.82  & 72.78   &  & -5.13  & -3.91  & -4.06  & 26.45   &  & -2.10  & -1.89 & -1.91 & 9.60   &  & -0.98  & -0.90 & -0.91 & 7.59   \\
 & 1.1  & -13.97 & -9.02  & -9.67  & 44.41   &  & -4.46  & -3.91  & -4.00  & 11.50   &  & -1.94  & -1.89 & -1.90 & 2.12   &  & -0.91  & -0.90 & -0.90 & 1.00   \\
 & 1.2  & -11.89 & -9.02  & -9.51  & 25.08   &  & -4.05  & -3.91  & -3.98  & 1.84    &  & -1.85  & -1.89 & -1.90 & -2.73  &  & -0.87  & -0.90 & -0.90 & -3.40  \\ \hline
 &      &        &        &        &         &  &        &        &        &         &  &        &       &       &        &  &        &       &       &        \\ \hline
 & -0.2 & -57.19 & -16.48 & -16.99 & 236.65  &  & -36.69 & -10.32 & -12.17 & 201.58  &  & -17.91 & -4.87 & -6.18 & 189.68 &  & -6.90  & -2.09 & -2.36 & 191.75 \\
 & -0.1 & -58.58 & -16.49 & -17.18 & 241.02  &  & -37.44 & -10.31 & -12.22 & 206.35  &  & -18.19 & -4.84 & -6.19 & 194.03 &  & -6.97  & -2.09 & -2.39 & 191.14 \\
 & 0.0  & -59.70 & -16.48 & -17.31 & 244.86  &  & -37.95 & -10.32 & -12.28 & 208.95  &  & -18.48 & -4.86 & -6.24 & 196.00 &  & -7.01  & -2.08 & -2.40 & 192.48 \\
 & 0.1  & -60.38 & -16.50 & -17.42 & 246.56  &  & -37.97 & -10.32 & -12.18 & 211.67  &  & -18.62 & -4.86 & -6.28 & 196.74 &  & -7.01  & -2.09 & -2.39 & 193.12 \\
 & 0.2  & -60.14 & -16.48 & -17.40 & 245.57  &  & -37.64 & -10.33 & -12.17 & 209.18  &  & -18.44 & -4.85 & -6.30 & 192.98 &  & -6.84  & -2.09 & -2.39 & 186.62 \\
 & 0.3  & -58.88 & -16.49 & -17.67 & 233.21  &  & -36.57 & -10.31 & -12.15 & 201.11  &  & -17.77 & -4.85 & -6.21 & 186.04 &  & -6.51  & -2.09 & -2.37 & 174.95 \\
 & 0.4  & -56.06 & -16.49 & -17.81 & 214.74  &  & -34.64 & -10.33 & -12.06 & 187.11  &  & -16.57 & -4.87 & -6.29 & 163.68 &  & -6.00  & -2.09 & -2.36 & 154.47 \\ \cline{2-21} 
 \multicolumn{1}{c}{0} & 0.5  & -51.49 & -16.49 & -18.14 & 183.83  &  & -31.31 & -10.30 & -11.86 & 163.92  &  & -14.55 & -4.83 & -6.03 & 141.40 &  & -5.22  & -2.08 & -2.30 & 126.77 \\ \cline{2-21} 
 & 0.6  & -45.50 & -16.50 & -18.20 & 149.98  &  & -26.72 & -10.32 & -11.57 & 130.98  &  & -12.12 & -4.85 & -5.78 & 109.84 &  & -4.19  & -2.08 & -2.24 & 87.40  \\
 & 0.7  & -38.84 & -16.50 & -18.01 & 115.63  &  & -21.77 & -10.33 & -11.23 & 93.86   &  & -9.37  & -4.85 & -5.41 & 73.28  &  & -3.26  & -2.09 & -2.19 & 48.88  \\
 & 0.8  & -31.76 & -16.49 & -17.34 & 83.18   &  & -17.36 & -10.32 & -10.91 & 59.14   &  & -7.27  & -4.87 & -5.20 & 39.92  &  & -2.65  & -2.09 & -2.14 & 23.78  \\
 & 0.9  & -25.92 & -16.49 & -17.06 & 51.93   &  & -14.10 & -10.32 & -10.76 & 31.10   &  & -5.95  & -4.84 & -5.01 & 18.74  &  & -2.34  & -2.08 & -2.12 & 10.22  \\
 & 1.0  & -21.81 & -16.49 & -16.80 & 29.85   &  & -12.12 & -10.33 & -10.55 & 14.89   &  & -5.32  & -4.87 & -4.95 & 7.49   &  & -2.16  & -2.09 & -2.10 & 3.03   \\
 & 1.1  & -19.14 & -16.51 & -16.75 & 14.28   &  & -11.04 & -10.32 & -10.42 & 5.95    &  & -5.01  & -4.87 & -4.92 & 1.88   &  & -2.07  & -2.08 & -2.06 & 0.44   \\
 & 1.2  & -17.51 & -16.50 & -16.60 & 5.44    &  & -10.52 & -10.32 & -10.35 & 1.60    &  & -4.91  & -4.88 & -4.93 & -0.43  &  & -2.06  & -2.09 & -2.08 & -1.27  \\ \hline
 &      &        &        &        &         &  &        &        &        &         &  &        &       &       &        &  &        &       &       &        \\ \hline
 & -0.2 & -33.35 & -5.93  & 2.77   & 1105.96 &  & -24.73 & -5.95  & -0.81  & 2957.56 &  & -19.03 & -5.82 & -3.08 & 517.33 &  & -14.34 & -4.82 & -4.17 & 243.92 \\
 & -0.1 & -33.57 & -5.93  & 2.56   & 1209.35 &  & -24.85 & -5.96  & -0.63  & 3848.19 &  & -19.15 & -5.82 & -3.02 & 534.82 &  & -14.40 & -4.82 & -4.13 & 248.21 \\
 & 0.0  & -32.95 & -5.92  & 2.15   & 1430.67 &  & -24.58 & -5.96  & -0.70  & 3422.44 &  & -19.02 & -5.82 & -3.01 & 531.20 &  & -14.31 & -4.82 & -4.18 & 242.10 \\
 & 0.1  & -31.69 & -5.93  & 1.45   & 2091.57 &  & -23.63 & -5.96  & -1.00  & 2264.08 &  & -18.51 & -5.82 & -3.11 & 495.28 &  & -14.13 & -4.82 & -4.20 & 235.94 \\
 & 0.2  & -28.86 & -5.93  & 0.37   & 7615.84 &  & -21.52 & -5.96  & -1.75  & 1129.85 &  & -17.44 & -5.82 & -3.38 & 415.34 &  & -13.48 & -4.82 & -4.19 & 221.80 \\
 & 0.3  & -24.89 & -5.94  & -1.34  & 1755.66 &  & -18.68 & -5.95  & -2.69  & 594.38  &  & -15.41 & -5.82 & -3.98 & 287.05 &  & -12.08 & -4.82 & -4.34 & 178.18 \\
 & 0.4  & -20.17 & -5.93  & -3.11  & 547.84  &  & -15.00 & -5.96  & -3.83  & 291.96  &  & -12.62 & -5.83 & -4.64 & 171.91 &  & -10.05 & -4.82 & -4.66 & 115.78 \\ \cline{2-21} 
 \multicolumn{1}{c}{0.5} & 0.5  & -15.68 & -5.94  & -4.28  & 266.08  &  & -11.55 & -5.95  & -4.88  & 136.53  &  & -9.64  & -5.82 & -5.11 & 88.55  &  & -7.84  & -4.82 & -4.83 & 62.35  \\ \cline{2-21} 
 & 0.6  & -11.66 & -5.94  & -4.93  & 136.48  &  & -8.97  & -5.95  & -5.53  & 62.16   &  & -7.65  & -5.82 & -5.51 & 39.03  &  & -6.18  & -4.82 & -4.83 & 27.91  \\
 & 0.7  & -8.79  & -5.95  & -5.30  & 65.83   &  & -7.36  & -5.96  & -5.82  & 26.52   &  & -6.52  & -5.82 & -5.72 & 14.03  &  & -5.37  & -4.82 & -4.89 & 9.83   \\
 & 0.8  & -6.97  & -5.93  & -5.48  & 27.06   &  & -6.50  & -5.95  & -5.98  & 8.70    &  & -6.04  & -5.82 & -5.82 & 3.83   &  & -5.03  & -4.82 & -4.88 & 3.10   \\
 & 0.9  & -5.93  & -5.94  & -5.69  & 4.35    &  & -6.02  & -5.95  & -6.03  & -0.14   &  & -5.86  & -5.82 & -5.85 & 0.07   &  & -4.86  & -4.82 & -4.85 & 0.25   \\
 & 1.0  & -5.52  & -5.94  & -5.88  & -6.15   &  & -5.91  & -5.96  & -6.01  & -1.67   &  & -5.77  & -5.83 & -5.85 & -1.35  &  & -4.82  & -4.82 & -4.84 & -0.34  \\
 & 1.1  & -5.32  & -5.95  & -5.96  & -10.73  &  & -5.84  & -5.95  & -6.05  & -3.45   &  & -5.74  & -5.82 & -5.84 & -1.67  &  & -4.81  & -4.82 & -4.81 & -0.09  \\
 & 1.2  & -5.22  & -5.94  & -6.00  & -13.00  &  & -5.83  & -5.96  & -6.06  & -3.76   &  & -5.77  & -5.82 & -5.83 & -0.99  &  & -4.79  & -4.82 & -4.82 & -0.69 \\ \hline
\end{tabular}
}
\caption{(100 $\times$) Monte Carlo bias of the estimated memory parameter for ARFIMA(1,$d_0$,0) of CSS estimator with unknown and known $\mu_0$ and the MCSS estimator.}
\label{tablear1}
\end{table}





\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{EEEEEEEEEEEEEEEEEEEEEE}
\hline
\multicolumn{1}{c}{$\varphi_0$} &
 &
  \multicolumn{1}{c}{bias($\hat{\varphi}$)} &
  \multicolumn{1}{c}{bias($\hat{\varphi}_{\mu_0}$)} &
  \multicolumn{1}{c}{bias($\hat{\varphi}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \ &
 \multicolumn{1}{c}{bias($\hat{\varphi}$)} &
  \multicolumn{1}{c}{bias($\hat{\varphi}_{\mu_0}$) } &
  \multicolumn{1}{c}{bias($\hat{\varphi}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \   &
 \multicolumn{1}{c}{bias($\hat{\varphi}$)} &
  \multicolumn{1}{c}{bias($\hat{\varphi}_{\mu_0}$) } &
  \multicolumn{1}{c}{bias($\hat{\varphi}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \   &
  \multicolumn{1}{c}{bias($\hat{\varphi}$)} &
  \multicolumn{1}{c}{bias($\hat{\varphi}_{\mu_0}$) } &
  \multicolumn{1}{c}{bias($\hat{\varphi}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \ & \multicolumn{1}{c}{$d_0$ \textbackslash{} $T$}  & \multicolumn{4}{c}{32} & & \multicolumn{4}{c}{64} &  & \multicolumn{4}{c}{128} &  & \multicolumn{4}{c}{256} \\ \hline
 & -0.2 & 26.93 & 8.25  & 9.89  & 172.41 &  & 9.43  & 3.67 & 3.96  & 137.96  &  & 3.95  & 1.86 & 1.90  & 107.99   &  & 1.97  & 0.95 & 0.95 & 107.64 \\
 & -0.1 & 27.70 & 8.24  & 9.88  & 180.42 &  & 9.55  & 3.67 & 3.99  & 139.20  &  & 3.97  & 1.86 & 1.90  & 109.35   &  & 1.98  & 0.95 & 0.95 & 108.25 \\
 & 0.0  & 28.22 & 8.25  & 9.94  & 183.75 &  & 9.53  & 3.68 & 4.01  & 137.38  &  & 3.99  & 1.86 & 1.90  & 109.73   &  & 1.98  & 0.95 & 0.95 & 108.10 \\
 & 0.1  & 28.48 & 8.25  & 9.87  & 188.51 &  & 9.59  & 3.67 & 4.07  & 135.59  &  & 3.97  & 1.86 & 1.90  & 108.76   &  & 1.96  & 0.95 & 0.95 & 106.58 \\
 & 0.2  & 28.76 & 8.25  & 9.95  & 189.11 &  & 9.63  & 3.67 & 4.06  & 137.42  &  & 3.91  & 1.86 & 1.91  & 105.22   &  & 1.93  & 0.95 & 0.95 & 102.69 \\
 & 0.3  & 28.40 & 8.24  & 9.82  & 189.15 &  & 9.46  & 3.67 & 4.06  & 132.98  &  & 3.77  & 1.86 & 1.91  & 97.22    &  & 1.86  & 0.95 & 0.95 & 94.89  \\
 & 0.4  & 27.68 & 8.24  & 9.72  & 184.83 &  & 9.07  & 3.67 & 4.01  & 126.00  &  & 3.56  & 1.86 & 1.91  & 85.99    &  & 1.73  & 0.95 & 0.96 & 81.33  \\ \cline{2-21} 
 \multicolumn{1}{c}{$-0.5$}  & 0.5  & 26.80 & 8.24  & 9.57  & 180.01 &  & 8.43  & 3.67 & 3.97  & 112.61  &  & 3.23  & 1.86 & 1.91  & 68.64    &  & 1.55  & 0.95 & 0.96 & 61.61  \\ \cline{2-21} 
 & 0.6  & 24.95 & 8.25  & 9.59  & 160.21 &  & 7.43  & 3.67 & 3.94  & 88.78   &  & 2.81  & 1.86 & 1.91  & 47.58    &  & 1.34  & 0.95 & 0.96 & 39.01  \\
 & 0.7  & 22.16 & 8.24  & 9.39  & 135.92 &  & 6.40  & 3.67 & 3.90  & 63.94   &  & 2.40  & 1.86 & 1.89  & 26.62    &  & 1.15  & 0.95 & 0.96 & 19.10  \\
 & 0.8  & 19.05 & 8.24  & 9.01  & 111.49 &  & 5.44  & 3.67 & 3.84  & 41.84   &  & 2.07  & 1.86 & 1.88  & 10.31    &  & 1.01  & 0.95 & 0.96 & 5.13   \\
 & 0.9  & 15.91 & 8.24  & 8.72  & 82.36  &  & 4.62  & 3.67 & 3.76  & 22.86   &  & 1.87  & 1.86 & 1.87  & 0.26     &  & 0.93  & 0.95 & 0.96 & -3.26  \\
 & 1.0  & 13.15 & 8.24  & 8.43  & 56.02  &  & 4.09  & 3.67 & 3.73  & 9.73    &  & 1.74  & 1.86 & 1.86  & -6.41    &  & 0.88  & 0.95 & 0.95 & -7.88  \\
 & 1.1  & 11.07 & 8.24  & 8.47  & 30.58  &  & 3.66  & 3.67 & 3.71  & -1.14   &  & 1.68  & 1.86 & 1.86  & -9.74    &  & 0.85  & 0.95 & 0.95 & -10.29 \\
 & 1.2  & 9.64  & 8.25  & 8.44  & 14.24  &  & 3.43  & 3.67 & 3.70  & -7.41   &  & 1.65  & 1.86 & 1.86  & -11.34   &  & 0.84  & 0.95 & 0.95 & -11.42 \\ \hline
 &      &       &       &       &        &  &       &      &       &         &  &       &      &       &          &  &       &      &      &        \\ \hline
 & -0.2 & 41.35 & 12.08 & 10.83 & 281.83 &  & 30.07 & 8.43 & 9.35  & 221.49  &  & 15.59 & 4.18 & 5.18  & 200.96   &  & 6.19  & 1.89 & 2.10 & 195.07 \\
 & -0.1 & 42.40 & 12.09 & 10.93 & 288.02 &  & 30.70 & 8.41 & 9.39  & 227.04  &  & 15.85 & 4.16 & 5.19  & 205.38   &  & 6.26  & 1.89 & 2.13 & 194.01 \\
 & 0.0  & 43.24 & 12.09 & 10.96 & 294.39 &  & 31.12 & 8.43 & 9.43  & 230.15  &  & 16.10 & 4.17 & 5.23  & 208.00   &  & 6.30  & 1.89 & 2.13 & 195.69 \\
 & 0.1  & 43.72 & 12.10 & 10.99 & 297.66 &  & 31.10 & 8.44 & 9.31  & 234.19  &  & 16.22 & 4.17 & 5.24  & 209.35   &  & 6.30  & 1.89 & 2.12 & 196.65 \\
 & 0.2  & 43.49 & 12.08 & 10.95 & 297.07 &  & 30.77 & 8.43 & 9.28  & 231.66  &  & 16.06 & 4.17 & 5.27  & 205.01   &  & 6.14  & 1.90 & 2.12 & 189.58 \\
 & 0.3  & 42.45 & 12.08 & 11.21 & 278.73 &  & 29.82 & 8.42 & 9.25  & 222.42  &  & 15.44 & 4.17 & 5.19  & 197.74   &  & 5.82  & 1.89 & 2.09 & 177.94 \\
 & 0.4  & 40.13 & 12.08 & 11.41 & 251.71 &  & 28.10 & 8.43 & 9.20  & 205.59  &  & 14.34 & 4.18 & 5.26  & 172.47   &  & 5.34  & 1.89 & 2.09 & 155.77 \\ \cline{2-21} 
  \multicolumn{1}{c}{0} & 0.5  & 36.50 & 12.08 & 11.86 & 207.83 &  & 25.23 & 8.42 & 9.09  & 177.41  &  & 12.50 & 4.16 & 5.06  & 147.18   &  & 4.61  & 1.88 & 2.04 & 126.29 \\ \cline{2-21} 
 & 0.6  & 31.89 & 12.09 & 12.16 & 162.37 &  & 21.32 & 8.43 & 8.95  & 138.07  &  & 10.31 & 4.17 & 4.85  & 112.49   &  & 3.67  & 1.88 & 1.99 & 84.58  \\
 & 0.7  & 27.04 & 12.10 & 12.32 & 119.53 &  & 17.23 & 8.43 & 8.81  & 95.50   &  & 7.90  & 4.17 & 4.57  & 72.78    &  & 2.84  & 1.89 & 1.96 & 44.85  \\
 & 0.8  & 21.97 & 12.09 & 12.06 & 82.11  &  & 13.69 & 8.43 & 8.67  & 57.96   &  & 6.09  & 4.18 & 4.41  & 38.20    &  & 2.32  & 1.89 & 1.92 & 20.42  \\
 & 0.9  & 18.00 & 12.10 & 12.11 & 48.60  &  & 11.16 & 8.43 & 8.64  & 29.06   &  & 5.03  & 4.17 & 4.29  & 17.31    &  & 2.07  & 1.89 & 1.92 & 8.32   \\
 & 1.0  & 15.37 & 12.09 & 12.07 & 27.32  &  & 9.71  & 8.43 & 8.52  & 13.97   &  & 4.55  & 4.19 & 4.24  & 7.36     &  & 1.95  & 1.89 & 1.89 & 3.00   \\
 & 1.1  & 13.75 & 12.10 & 12.13 & 13.36  &  & 9.01  & 8.42 & 8.45  & 6.53    &  & 4.36  & 4.19 & 4.23  & 3.20     &  & 1.91  & 1.89 & 1.87 & 2.18   \\
 & 1.2  & 12.88 & 12.09 &  12.08     & 6.65   &  & 8.74  & 8.44 & 8.42  & 3.78    &  & 4.33  & 4.20 & 4.24  & 2.23     &  & 1.92  & 1.90 & 1.89 & 1.97   \\ \hline
 &      &       &       &       &        &  &       &      &       &         &  &       &      &       &          &  &       &      &      &        \\ \hline
 & -0.2 & 15.50 & -2.25 & -8.38 & 84.89  &  & 14.41 & 0.66 & -3.60 & 300.20  &  & 12.90 & 2.56 & -0.09 & 13834.33 &  & 10.93 & 3.01 & 2.18 & 400.54 \\
 & -0.1 & 15.65 & -2.24 & -8.21 & 90.58  &  & 14.47 & 0.66 & -3.78 & 283.08  &  & 12.98 & 2.56 & -0.16 & 7966.49  &  & 10.97 & 3.01 & 2.15 & 410.69 \\
 & 0.0  & 15.19 & -2.25 & -7.92 & 91.86  &  & 14.20 & 0.66 & -3.72 & 281.65  &  & 12.83 & 2.55 & -0.17 & 7432.74  &  & 10.88 & 3.01 & 2.19 & 396.56 \\
 & 0.1  & 14.35 & -2.24 & -7.43 & 93.08  &  & 13.40 & 0.66 & -3.47 & 286.12  &  & 12.36 & 2.55 & -0.08 & 14628.76 &  & 10.69 & 3.01 & 2.21 & 383.96 \\
 & 0.2  & 12.35 & -2.24 & -6.74 & 83.05  &  & 11.67 & 0.66 & -2.82 & 313.86  &  & 11.43 & 2.56 & 0.18  & 6182.65  &  & 10.10 & 3.01 & 2.21 & 357.94 \\
 & 0.3  & 9.54  & -2.23 & -5.51 & 73.04  &  & 9.49  & 0.66 & -2.00 & 373.77  &  & 9.73  & 2.55 & 0.75  & 1199.28  &  & 8.87  & 3.01 & 2.38 & 273.09 \\
 & 0.4  & 6.33  & -2.24 & -4.22 & 49.85  &  & 6.79  & 0.66 & -1.05 & 547.99  &  & 7.53  & 2.56 & 1.40  & 438.16   &  & 7.15  & 3.01 & 2.71 & 163.48 \\ \cline{2-21} 
 \multicolumn{1}{c}{0.5}  & 0.5  & 3.47  & -2.23 & -3.44 & 0.83   &  & 4.41  & 0.66 & -0.16 & 2683.50 &  & 5.29  & 2.55 & 1.87  & 182.76   &  & 5.36  & 3.01 & 2.93 & 83.22  \\ \cline{2-21} 
 & 0.6  & 1.08  & -2.23 & -3.01 & -64.13 &  & 2.75  & 0.65 & 0.36  & 669.70  &  & 3.89  & 2.55 & 2.25  & 73.17    &  & 4.07  & 3.01 & 2.97 & 36.98  \\
 & 0.7  & -0.41 & -2.22 & -2.76 & -85.13 &  & 1.76  & 0.66 & 0.56  & 216.75  &  & 3.12  & 2.55 & 2.44  & 27.78    &  & 3.47  & 3.01 & 3.04 & 14.09  \\
 & 0.8  & -1.37 & -2.24 & -2.69 & -49.04 &  & 1.27  & 0.66 & 0.66  & 92.05   &  & 2.83  & 2.55 & 2.53  & 11.85    &  & 3.24  & 3.01 & 3.04 & 6.45   \\
 & 0.9  & -1.81 & -2.23 & -2.54 & -28.66 &  & 1.02  & 0.66 & 0.70  & 45.54   &  & 2.75  & 2.55 & 2.56  & 7.21     &  & 3.13  & 3.01 & 3.02 & 3.51   \\
 & 1.0  & -1.87 & -2.24 & -2.38 & -21.41 &  & 1.05  & 0.66 & 0.68  & 54.67   &  & 2.73  & 2.56 & 2.57  & 6.19     &  & 3.12  & 3.01 & 3.02 & 3.38   \\
 & 1.1  & -1.84 & -2.23 & -2.31 & -20.49 &  & 1.06  & 0.66 & 0.71  & 49.18   &  & 2.73  & 2.56 & 2.55  & 6.99     &  & 3.12  & 3.01 & 2.99 & 4.26   \\
 & 1.2  & -1.76 & -2.23 & -2.26 & -22.26 &  & 1.11  & 0.67 & 0.73  & 53.33   &  & 2.78  & 2.55 & 2.54  & 9.37     &  & 3.11  & 3.01 & 3.00 & 3.72    \\ \hline
\end{tabular}
}
\caption{(100 $\times$) Monte Carlo bias of the estimated AR coefficient for ARFIMA(1,$d_0$,0) of CSS estimator with unknown and known $\mu_0$ and the MCSS estimator.}
\label{tablear2}
\end{table}








\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{EEEEEEEEEEEEEEEEEEEEEE}
\hline
\multicolumn{1}{c}{$\varphi_0$} &
 &
 \multicolumn{1}{c}{MSE($\hat{d}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{\mu_0}$) } &
  \multicolumn{1}{c}{MSE($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \ &
 \multicolumn{1}{c}{MSE($\hat{d}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{\mu_0}$) } &
  \multicolumn{1}{c}{MSE($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \   &
 \multicolumn{1}{c}{MSE($\hat{d}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{\mu_0}$) } &
  \multicolumn{1}{c}{MSE($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \   &
  \multicolumn{1}{c}{MSE($\hat{d}$)} &
  \multicolumn{1}{c}{MSE($\hat{d}_{\mu_0}$) } &
  \multicolumn{1}{c}{MSE($\hat{d}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \ & \multicolumn{1}{c}{$d_0$ \textbackslash{} $T$}  & \multicolumn{4}{c}{32} & & \multicolumn{4}{c}{64} &  & \multicolumn{4}{c}{128} &  & \multicolumn{4}{c}{256} \\ \hline
  & -0.2 & 38.58 & 7.89  & 14.57 & 164.83 &  & 7.04  & 2.21 & 3.37  & 108.65 &  & 1.50  & 0.86 & 1.10 & 36.42  &  & 0.54 & 0.39 & 0.45 & 18.83  \\
 & -0.1 & 40.16 & 7.89  & 14.52 & 176.55 &  & 7.17  & 2.21 & 3.42  & 109.90 &  & 1.52  & 0.86 & 1.10 & 38.22  &  & 0.54 & 0.39 & 0.45 & 19.14  \\
 & 0.0  & 41.46 & 7.89  & 14.69 & 182.33 &  & 7.12  & 2.21 & 3.45  & 106.42 &  & 1.54  & 0.86 & 1.10 & 39.89  &  & 0.54 & 0.39 & 0.45 & 19.45  \\
 & 0.1  & 42.41 & 7.89  & 14.54 & 191.68 &  & 7.36  & 2.21 & 3.57  & 106.19 &  & 1.56  & 0.86 & 1.10 & 42.15  &  & 0.54 & 0.39 & 0.45 & 19.73  \\
 & 0.2  & 43.39 & 7.89  & 14.55 & 198.14 &  & 7.60  & 2.21 & 3.50  & 117.52 &  & 1.55  & 0.86 & 1.09 & 42.69  &  & 0.54 & 0.39 & 0.45 & 19.93  \\
 & 0.3  & 43.27 & 7.89  & 14.08 & 207.44 &  & 7.68  & 2.21 & 3.45  & 122.40 &  & 1.51  & 0.86 & 1.07 & 40.28  &  & 0.53 & 0.39 & 0.45 & 19.95  \\
 & 0.4  & 42.77 & 7.89  & 13.52 & 216.46 &  & 7.60  & 2.21 & 3.29  & 130.76 &  & 1.49  & 0.86 & 1.05 & 42.51  &  & 0.52 & 0.39 & 0.44 & 19.42  \\ \cline{2-21} 
\multicolumn{1}{c}{$-0.5$} & 0.5  & 42.53 & 7.89  & 12.89 & 230.00 &  & 7.24  & 2.21 & 3.06  & 136.20 &  & 1.43  & 0.86 & 1.01 & 40.59  &  & 0.50 & 0.39 & 0.43 & 17.67  \\ \cline{2-21} 
 & 0.6  & 40.63 & 7.89  & 12.70 & 219.98 &  & 6.45  & 2.21 & 2.90  & 122.43 &  & 1.31  & 0.86 & 0.97 & 34.27  &  & 0.48 & 0.39 & 0.42 & 14.16  \\
 & 0.7  & 36.63 & 7.89  & 11.93 & 207.16 &  & 5.64  & 2.21 & 2.76  & 104.61 &  & 1.16  & 0.86 & 0.94 & 23.98  &  & 0.45 & 0.39 & 0.41 & 9.69   \\
 & 0.8  & 31.57 & 7.89  & 10.76 & 193.36 &  & 4.83  & 2.21 & 2.58  & 86.96  &  & 1.03  & 0.86 & 0.90 & 14.40  &  & 0.42 & 0.39 & 0.40 & 5.81   \\
 & 0.9  & 26.42 & 7.89  & 9.91  & 166.44 &  & 3.98  & 2.21 & 2.41  & 65.47  &  & 0.98  & 0.86 & 0.88 & 10.69  &  & 0.41 & 0.39 & 0.39 & 3.22   \\
 & 1.0  & 21.39 & 7.90  & 8.97  & 138.38 &  & 3.44  & 2.21 & 2.32  & 47.86  &  & 0.91  & 0.86 & 0.87 & 4.69   &  & 0.40 & 0.39 & 0.39 & 1.71   \\
 & 1.1  & 17.14 & 7.89  & 8.81  & 94.52  &  & 2.88  & 2.21 & 2.27  & 26.62  &  & 0.89  & 0.86 & 0.87 & 2.38   &  & 0.39 & 0.39 & 0.39 & 0.86   \\
 & 1.2  & 14.10 & 7.89  & 8.63  & 63.37  &  & 2.56  & 2.21 & 2.27  & 12.70  &  & 0.87  & 0.86 & 0.86 & 1.08   &  & 0.39 & 0.39 & 0.39 & 0.36   \\  \hline
 &      &       &       &       &        &  &       &      &       &        &  &       &      &      &        &  &      &      &      &        \\  \hline
 & -0.2 & 54.39 & 16.06 & 21.94 & 147.94 &  & 29.91 & 8.56 & 11.69 & 155.87 &  & 11.74 & 3.23 & 4.87 & 141.09 &  & 2.79 & 1.05 & 1.41 & 97.75  \\
 & -0.1 & 56.32 & 16.06 & 22.01 & 155.85 &  & 30.85 & 8.54 & 11.71 & 163.49 &  & 12.04 & 3.20 & 4.87 & 147.28 &  & 2.84 & 1.06 & 1.43 & 98.77  \\
 & 0.0  & 58.19 & 16.06 & 21.87 & 166.08 &  & 31.56 & 8.55 & 11.75 & 168.62 &  & 12.35 & 3.21 & 4.89 & 152.54 &  & 2.88 & 1.05 & 1.43 & 101.76 \\
 & 0.1  & 59.83 & 16.07 & 21.56 & 177.55 &  & 31.86 & 8.56 & 11.57 & 175.36 &  & 12.62 & 3.22 & 4.89 & 157.84 &  & 2.93 & 1.06 & 1.41 & 107.90 \\
 & 0.2  & 60.51 & 16.07 & 20.92 & 189.20 &  & 31.96 & 8.56 & 11.44 & 179.41 &  & 12.68 & 3.21 & 4.89 & 159.47 &  & 2.88 & 1.06 & 1.40 & 105.33 \\
 & 0.3  & 60.54 & 16.06 & 20.45 & 196.03 &  & 31.58 & 8.55 & 11.22 & 181.60 &  & 12.37 & 3.21 & 4.73 & 161.39 &  & 2.77 & 1.05 & 1.37 & 101.69 \\
 & 0.4  & 59.17 & 16.06 & 19.82 & 198.56 &  & 30.57 & 8.56 & 10.87 & 181.31 &  & 11.80 & 3.22 & 4.75 & 148.55 &  & 2.68 & 1.05 & 1.34 & 99.97  \\ \cline{2-21} 
 \multicolumn{1}{c}{0} & 0.5  & 55.90 & 16.06 & 19.39 & 188.28 &  & 28.37 & 8.55 & 10.40 & 172.73 &  & 10.61 & 3.20 & 4.41 & 140.38 &  & 2.48 & 1.05 & 1.24 & 99.57  \\ \cline{2-21} 
 & 0.6  & 50.68 & 16.07 & 18.75 & 170.28 &  & 24.71 & 8.56 & 9.95  & 148.45 &  & 9.09  & 3.21 & 4.13 & 120.19 &  & 2.09 & 1.05 & 1.16 & 79.60  \\
 & 0.7  & 44.09 & 16.07 & 18.04 & 144.38 &  & 20.53 & 8.56 & 9.50  & 116.02 &  & 7.08  & 3.21 & 3.75 & 88.63  &  & 1.67 & 1.05 & 1.12 & 49.09  \\
 & 0.8  & 36.42 & 16.06 & 17.10 & 112.94 &  & 16.35 & 8.56 & 9.15  & 78.80  &  & 5.37  & 3.23 & 3.52 & 52.39  &  & 1.38 & 1.05 & 1.09 & 26.65  \\
 & 0.9  & 29.58 & 16.07 & 16.80 & 76.14  &  & 13.01 & 8.56 & 9.00  & 44.56  &  & 4.25  & 3.21 & 3.35 & 26.85  &  & 1.24 & 1.05 & 1.08 & 14.15  \\
 & 1.0  & 24.57 & 16.07 & 16.51 & 48.81  &  & 10.90 & 8.56 & 8.79  & 24.01  &  & 3.72  & 3.23 & 3.30 & 12.71  &  & 1.12 & 1.05 & 1.06 & 6.13   \\
 & 1.1  & 20.98 & 16.08 & 16.49 & 27.23  &  & 9.69  & 8.56 & 8.66  & 11.84  &  & 3.45  & 3.23 & 3.28 & 5.28   &  & 1.07 & 1.05 & 1.03 & 3.95   \\
 & 1.2  & 18.70 & 16.07 & 16.36 & 14.28  &  & 9.05  & 8.56 & 8.59  & 5.34   &  & 3.37  & 3.23 & 3.30 & 1.95   &  & 1.06 & 1.06 & 1.05 & 1.20   \\  \hline
 &      &       &       &       &        &  &       &      &       &        &  &       &      &      &        &  &      &      &      &        \\  \hline
 & -0.2 & 18.68 & 9.55  & 12.60 & 48.33  &  & 11.81 & 6.57 & 7.58  & 55.91  &  & 8.17  & 4.57 & 5.05 & 61.73  &  & 5.48 & 2.95 & 3.43 & 60.01  \\
 & -0.1 & 19.25 & 9.55  & 12.37 & 55.59  &  & 12.03 & 6.57 & 7.55  & 59.35  &  & 8.28  & 4.57 & 5.02 & 65.00  &  & 5.53 & 2.95 & 3.42 & 61.79  \\
 & 0.0  & 19.73 & 9.55  & 11.91 & 65.71  &  & 12.20 & 6.57 & 7.45  & 63.68  &  & 8.34  & 4.57 & 5.00 & 66.82  &  & 5.53 & 2.95 & 3.43 & 61.53  \\
 & 0.1  & 20.11 & 9.55  & 11.24 & 79.03  &  & 12.28 & 6.57 & 7.23  & 69.89  &  & 8.39  & 4.57 & 4.94 & 69.67  &  & 5.56 & 2.95 & 3.41 & 62.94  \\
 & 0.2  & 19.91 & 9.55  & 10.54 & 88.96  &  & 12.09 & 6.57 & 6.91  & 75.04  &  & 8.29  & 4.57 & 4.84 & 71.17  &  & 5.50 & 2.95 & 3.35 & 64.22  \\
 & 0.3  & 19.18 & 9.55  & 9.92  & 93.28  &  & 11.56 & 6.57 & 6.59  & 75.38  &  & 7.89  & 4.57 & 4.73 & 66.86  &  & 5.22 & 2.95 & 3.27 & 59.64  \\
 & 0.4  & 17.74 & 9.55  & 9.55  & 85.77  &  & 10.49 & 6.57 & 6.41  & 63.58  &  & 7.12  & 4.57 & 4.61 & 54.63  &  & 4.71 & 2.95 & 3.20 & 47.53  \\ \cline{2-21} 
\multicolumn{1}{c}{0.5} & 0.5  & 15.91 & 9.55  & 9.39  & 69.42  &  & 9.18  & 6.57 & 6.39  & 43.54  &  & 6.10  & 4.57 & 4.53 & 34.67  &  & 4.03 & 2.95 & 3.10 & 30.02  \\ \cline{2-21} 
 & 0.6  & 13.86 & 9.55  & 9.32  & 48.72  &  & 8.04  & 6.57 & 6.48  & 24.16  &  & 5.35  & 4.57 & 4.53 & 17.89  &  & 3.46 & 2.95 & 3.02 & 14.80  \\
 & 0.7  & 12.04 & 9.55  & 9.34  & 28.99  &  & 7.29  & 6.57 & 6.52  & 11.74  &  & 4.88  & 4.57 & 4.57 & 6.87   &  & 3.16 & 2.95 & 3.00 & 5.31   \\
 & 0.8  & 10.89 & 9.55  & 9.42  & 15.57  &  & 6.90  & 6.57 & 6.61  & 4.28   &  & 4.67  & 4.57 & 4.58 & 1.81   &  & 3.03 & 2.95 & 2.98 & 1.60   \\
 & 0.9  & 10.10 & 9.55  & 9.49  & 6.44   &  & 6.67  & 6.57 & 6.61  & 0.87   &  & 4.59  & 4.57 & 4.60 & -0.19  &  & 2.96 & 2.95 & 2.97 & -0.28  \\
 & 1.0  & 9.74  & 9.55  & 9.62  & 1.25   &  & 6.56  & 6.57 & 6.62  & -0.83  &  & 4.54  & 4.57 & 4.59 & -0.99  &  & 2.94 & 2.95 & 2.96 & -0.72  \\
 & 1.1  & 9.56  & 9.55  & 9.61  & -0.51  &  & 6.50  & 6.57 & 6.62  & -1.77  &  & 4.52  & 4.57 & 4.59 & -1.54  &  & 2.93 & 2.95 & 2.95 & -0.66  \\
 & 1.2  & 9.41  & 9.55  & 9.64  & -2.35  &  & 6.46  & 6.57 & 6.60  & -1.99  &  & 4.53  & 4.57 & 4.59 & -1.33  &  & 2.92 & 2.95 & 2.96 & -1.10  \\ \hline
\end{tabular}
}
\caption{(100 $\times$) Empirical MSE of the estimated memory parameter for ARFIMA(1,$d_0$,0) of CSS estimator with unknown and known $\mu_0$ and the MCSS estimator.}
\label{tablear3}
\end{table}



\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{EEEEEEEEEEEEEEEEEEEEEE}
\hline
\multicolumn{1}{c}{$\varphi_0$} &
&
  \multicolumn{1}{c}{MSE($\hat{\varphi}$)} &
  \multicolumn{1}{c}{MSE($\hat{\varphi}_{\mu_0}$) } &
  \multicolumn{1}{c}{MSE($\hat{\varphi}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \ &
  \multicolumn{1}{c}{MSE($\hat{\varphi}$)} &
  \multicolumn{1}{c}{MSE($\hat{\varphi}_{\mu_0}$) } &
  \multicolumn{1}{c}{MSE($\hat{\varphi}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \   &
 \multicolumn{1}{c}{MSE($\hat{\varphi}$)} &
  \multicolumn{1}{c}{MSE($\hat{\varphi}_{\mu_0}$) } &
  \multicolumn{1}{c}{MSE($\hat{\varphi}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \   &
   \multicolumn{1}{c}{MSE($\hat{\varphi}$)} &
  \multicolumn{1}{c}{MSE($\hat{\varphi}_{\mu_0}$) } &
  \multicolumn{1}{c}{MSE($\hat{\varphi}_{m}$)} &
  \multicolumn{1}{c}{$\Delta \ & \multicolumn{1}{c}{$d_0$ \textbackslash{} $T$}  & \multicolumn{4}{c}{32} & & \multicolumn{4}{c}{64} &  & \multicolumn{4}{c}{128} &  & \multicolumn{4}{c}{256} \\ \hline
 & -0.2 & 28.23 & 7.91  & 10.40 & 171.49 &  & 6.00  & 2.46 & 2.92  & 105.29 &  & 1.32  & 0.98 & 1.05 & 26.59  &  & 0.52 & 0.45 & 0.46 & 11.95 \\
 & -0.1 & 29.35 & 7.91  & 10.35 & 183.70 &  & 6.10  & 2.46 & 2.95  & 106.59 &  & 1.35  & 0.98 & 1.05 & 28.51  &  & 0.52 & 0.45 & 0.46 & 12.10 \\
 & 0.0  & 30.10 & 7.91  & 10.43 & 188.64 &  & 6.00  & 2.46 & 2.98  & 101.49 &  & 1.35  & 0.98 & 1.05 & 29.41  &  & 0.52 & 0.45 & 0.46 & 12.21 \\
 & 0.1  & 30.52 & 7.91  & 10.23 & 198.33 &  & 6.16  & 2.46 & 3.09  & 99.60  &  & 1.37  & 0.98 & 1.05 & 30.75  &  & 0.52 & 0.45 & 0.46 & 12.22 \\
 & 0.2  & 31.28 & 7.91  & 10.32 & 202.98 &  & 6.41  & 2.46 & 3.04  & 110.82 &  & 1.36  & 0.98 & 1.04 & 30.67  &  & 0.52 & 0.45 & 0.46 & 12.07 \\
 & 0.3  & 31.19 & 7.91  & 10.07 & 209.76 &  & 6.46  & 2.46 & 3.03  & 113.64 &  & 1.32  & 0.98 & 1.04 & 26.87  &  & 0.51 & 0.45 & 0.46 & 11.61 \\
 & 0.4  & 30.82 & 7.91  & 9.81  & 214.04 &  & 6.42  & 2.46 & 2.93  & 119.38 &  & 1.32  & 0.98 & 1.03 & 28.24  &  & 0.51 & 0.45 & 0.46 & 10.62 \\ \cline{2-21} 
\multicolumn{1}{c}{$-0.5$} & 0.5  & 30.61 & 7.91  & 9.48  & 222.79 &  & 6.19  & 2.46 & 2.80  & 120.77 &  & 1.28  & 0.98 & 1.02 & 25.36  &  & 0.50 & 0.45 & 0.46 & 8.84  \\ \cline{2-21} 
 & 0.6  & 29.10 & 7.91  & 9.50  & 206.32 &  & 5.53  & 2.46 & 2.73  & 102.23 &  & 1.21  & 0.98 & 1.01 & 19.70  &  & 0.48 & 0.45 & 0.45 & 6.34  \\
 & 0.7  & 26.22 & 7.90  & 9.26  & 183.24 &  & 4.93  & 2.46 & 2.70  & 82.52  &  & 1.12  & 0.98 & 1.00 & 11.73  &  & 0.47 & 0.45 & 0.45 & 3.75  \\
 & 0.8  & 22.88 & 7.91  & 8.73  & 162.01 &  & 4.35  & 2.46 & 2.62  & 66.02  &  & 1.05  & 0.98 & 0.99 & 5.74   &  & 0.46 & 0.45 & 0.45 & 1.78  \\
 & 0.9  & 19.22 & 7.91  & 8.37  & 129.68 &  & 3.72  & 2.46 & 2.52  & 47.62  &  & 1.03  & 0.98 & 0.99 & 4.31   &  & 0.45 & 0.45 & 0.45 & 0.60  \\
 & 1.0  & 15.81 & 7.90  & 7.98  & 98.09  &  & 3.34  & 2.46 & 2.49  & 34.10  &  & 0.99  & 0.98 & 0.98 & 0.84   &  & 0.45 & 0.45 & 0.45 & -0.01 \\
 & 1.1  & 13.17 & 7.91  & 8.18  & 61.05  &  & 2.89  & 2.46 & 2.46  & 17.31  &  & 0.98  & 0.98 & 0.98 & -0.07  &  & 0.44 & 0.45 & 0.45 & -0.28 \\
 & 1.2  & 11.24 & 7.91  & 8.18  & 37.43  &  & 2.66  & 2.46 & 2.48  & 7.04   &  & 0.98  & 0.98 & 0.98 & -0.44  &  & 0.44 & 0.45 & 0.45 & -0.38 \\  \hline
 &      &       &       &       &        &  &       &      &       &        &  &       &      &      &        &  &      &      &      &       \\  \hline
 & -0.2 & 34.38 & 15.17 & 15.24 & 125.60 &  & 24.50 & 9.12 & 10.47 & 134.08 &  & 11.24 & 3.91 & 5.10 & 120.16 &  & 3.09 & 1.44 & 1.74 & 77.98 \\
 & -0.1 & 35.39 & 15.18 & 15.22 & 132.58 &  & 25.13 & 9.11 & 10.48 & 139.84 &  & 11.47 & 3.89 & 5.10 & 124.88 &  & 3.14 & 1.45 & 1.76 & 78.67 \\
 & 0.0  & 36.33 & 15.18 & 15.10 & 140.55 &  & 25.59 & 9.11 & 10.48 & 144.28 &  & 11.74 & 3.90 & 5.12 & 129.15 &  & 3.18 & 1.44 & 1.75 & 81.17 \\
 & 0.1  & 37.13 & 15.19 & 14.97 & 148.08 &  & 25.73 & 9.12 & 10.31 & 149.47 &  & 11.96 & 3.91 & 5.13 & 133.33 &  & 3.22 & 1.45 & 1.73 & 85.65 \\
 & 0.2  & 37.41 & 15.18 & 14.71 & 154.32 &  & 25.73 & 9.12 & 10.23 & 151.63 &  & 12.00 & 3.90 & 5.11 & 134.85 &  & 3.16 & 1.45 & 1.72 & 83.48 \\
 & 0.3  & 37.24 & 15.18 & 14.67 & 153.90 &  & 25.38 & 9.11 & 10.12 & 150.92 &  & 11.71 & 3.90 & 4.98 & 135.36 &  & 3.06 & 1.44 & 1.70 & 80.42 \\
 & 0.4  & 36.08 & 15.18 & 14.55 & 147.91 &  & 24.52 & 9.12 & 9.91  & 147.40 &  & 11.19 & 3.91 & 5.02 & 122.88 &  & 2.96 & 1.44 & 1.67 & 77.75 \\ \cline{2-21} 
\multicolumn{1}{c}{0} & 0.5  & 33.88 & 15.18 & 14.66 & 131.05 &  & 22.72 & 9.12 & 9.66  & 135.32 &  & 10.12 & 3.89 & 4.77 & 112.32 &  & 2.77 & 1.44 & 1.59 & 74.58 \\ \cline{2-21} 
 & 0.6  & 30.81 & 15.19 & 14.77 & 108.56 &  & 19.88 & 9.12 & 9.45  & 110.36 &  & 8.79  & 3.90 & 4.55 & 93.16  &  & 2.39 & 1.44 & 1.52 & 57.20 \\
 & 0.7  & 27.44 & 15.18 & 14.92 & 83.82  &  & 16.80 & 9.12 & 9.30  & 80.52  &  & 7.03  & 3.90 & 4.27 & 64.81  &  & 2.00 & 1.45 & 1.49 & 33.84 \\
 & 0.8  & 23.61 & 15.18 & 14.83 & 59.27  &  & 13.92 & 9.12 & 9.24  & 50.72  &  & 5.60  & 3.91 & 4.12 & 36.02  &  & 1.73 & 1.44 & 1.47 & 17.87 \\
 & 0.9  & 20.62 & 15.19 & 15.03 & 37.16  &  & 11.79 & 9.12 & 9.27  & 27.08  &  & 4.70  & 3.90 & 4.01 & 17.39  &  & 1.60 & 1.44 & 1.47 & 9.12  \\
 & 1.0  & 18.51 & 15.19 & 15.15 & 22.15  &  & 10.50 & 9.12 & 9.21  & 14.00  &  & 4.31  & 3.92 & 3.98 & 8.28   &  & 1.50 & 1.44 & 1.45 & 3.90  \\
 & 1.1  & 17.14 & 15.19 & 15.28 & 12.13  &  & 9.84  & 9.12 & 9.18  & 7.19   &  & 4.11  & 3.92 & 3.97 & 3.64   &  & 1.47 & 1.44 & 1.43 & 2.87  \\
 & 1.2  & 16.32 & 15.19 & 15.27 & 6.87   &  & 9.52  & 9.13 & 9.15  & 4.01   &  & 4.06  & 3.92 & 3.99 & 1.89   &  & 1.46 & 1.45 & 1.44 & 1.21  \\  \hline
 &      &       &       &       &        &  &       &      &       &        &  &       &      &      &        &  &      &      &      &       \\  \hline
 & -0.2 & 6.69  & 9.11  & 10.56 & -36.62 &  & 6.41  & 6.22 & 6.86  & -6.59  &  & 5.52  & 4.30 & 4.68 & 18.11  &  & 4.20 & 2.83 & 3.14 & 33.75 \\
 & -0.1 & 6.87  & 9.11  & 10.43 & -34.14 &  & 6.49  & 6.22 & 6.86  & -5.41  &  & 5.59  & 4.30 & 4.66 & 19.93  &  & 4.23 & 2.83 & 3.13 & 34.89 \\
 & 0.0  & 7.07  & 9.11  & 10.27 & -31.17 &  & 6.59  & 6.22 & 6.80  & -3.15  &  & 5.62  & 4.30 & 4.64 & 21.25  &  & 4.24 & 2.83 & 3.14 & 34.99 \\
 & 0.1  & 7.29  & 9.11  & 10.06 & -27.49 &  & 6.70  & 6.22 & 6.67  & 0.46   &  & 5.67  & 4.30 & 4.59 & 23.50  &  & 4.27 & 2.83 & 3.13 & 36.23 \\
 & 0.2  & 7.64  & 9.11  & 9.80  & -22.05 &  & 6.78  & 6.22 & 6.48  & 4.62   &  & 5.67  & 4.30 & 4.50 & 25.98  &  & 4.24 & 2.83 & 3.08 & 37.67 \\
 & 0.3  & 8.11  & 9.11  & 9.51  & -14.76 &  & 6.79  & 6.22 & 6.32  & 7.43   &  & 5.51  & 4.30 & 4.40 & 25.17  &  & 4.08 & 2.83 & 3.02 & 34.97 \\
 & 0.4  & 8.55  & 9.11  & 9.27  & -7.81  &  & 6.72  & 6.22 & 6.23  & 7.92   &  & 5.22  & 4.30 & 4.32 & 20.74  &  & 3.78 & 2.83 & 2.96 & 27.46 \\ \cline{2-21} 
\multicolumn{1}{c}{0.5} & 0.5  & 8.89  & 9.10  & 9.19  & -3.27  &  & 6.57  & 6.22 & 6.18  & 6.16   &  & 4.83  & 4.30 & 4.28 & 12.87  &  & 3.40 & 2.83 & 2.91 & 16.87 \\ \cline{2-21} 
 & 0.6  & 9.09  & 9.10  & 9.20  & -1.10  &  & 6.42  & 6.22 & 6.22  & 3.20   &  & 4.56  & 4.30 & 4.29 & 6.20   &  & 3.09 & 2.83 & 2.86 & 8.04  \\
 & 0.7  & 9.15  & 9.10  & 9.21  & -0.65  &  & 6.32  & 6.22 & 6.24  & 1.27   &  & 4.40  & 4.30 & 4.31 & 2.08   &  & 2.94 & 2.83 & 2.86 & 2.63  \\
 & 0.8  & 9.20  & 9.11  & 9.23  & -0.33  &  & 6.26  & 6.22 & 6.29  & -0.34  &  & 4.32  & 4.30 & 4.32 & 0.14   &  & 2.87 & 2.83 & 2.85 & 0.70  \\
 & 0.9  & 9.17  & 9.10  & 9.24  & -0.80  &  & 6.22  & 6.22 & 6.27  & -0.76  &  & 4.30  & 4.30 & 4.33 & -0.67  &  & 2.83 & 2.83 & 2.84 & -0.38 \\
 & 1.0  & 9.10  & 9.11  & 9.25  & -1.65  &  & 6.17  & 6.22 & 6.27  & -1.57  &  & 4.27  & 4.30 & 4.32 & -1.04  &  & 2.82 & 2.83 & 2.84 & -0.62 \\
 & 1.1  & 9.03  & 9.10  & 9.23  & -2.12  &  & 6.14  & 6.22 & 6.27  & -1.98  &  & 4.25  & 4.30 & 4.32 & -1.45  &  & 2.82 & 2.83 & 2.83 & -0.58 \\
 & 1.2  & 8.96  & 9.10  & 9.22  & -2.78  &  & 6.11  & 6.22 & 6.25  & -2.30  &  & 4.26  & 4.30 & 4.32 & -1.34  &  & 2.81 & 2.83 & 2.83 & -0.95  \\ \hline
\end{tabular}
}
\caption{(100 $\times$) Empirical MSE of the estimated AR coefficient for ARFIMA(1,$d_0$,0) of CSS estimator with unknown and known $\mu_0$ and the MCSS estimator.}
\label{tablear4}
\end{table}




\begin{figure}[H]
    \centering
   
\subfloat[Density plot of $d$ estimates with $T$ = 32]{
  \includegraphics[width=0.48\textwidth]{HistdT32_cropped.pdf}
}
\subfloat[Density plot of $\varphi$ estimates with  $T$ = 32]{
  \includegraphics[width=0.48\textwidth]{HistalphaT32_cropped.pdf}
}
\hspace{0mm}
\subfloat[Density plot of $d$ estimates with $T$ = 256]{
  \includegraphics[width=0.48\textwidth]{HistdT256_cropped.pdf}
}
\subfloat[Density plot of $\varphi$ estimates with $T$ = 256]{
  \includegraphics[width=0.48\textwidth]{HistalphaT256_cropped.pdf}
}
    \caption{ Density plots of the CSS estimator with unknown and known level parameter and of the MCSS estimator for the ARFIMA(1,$d_0$,0) model with $T$ = 32 (upper panel) and 256 (lower panel) and $d_0 = -0.2$ and $\varphi_0 = 0.5$. The density estimates use a normal kernel. }    \label{fig711}\end{figure}




\section{Empirical examples} \label{illustrations}

As an illustration of the results discussed in Sections \ref{S2} and \ref{secgen}, we now present three empirical applications, reconsidering the long-memory modelling of classical datasets: First, we examine the long-memory
properties of U.S.\ post-Second World War real GNP. Secondly, we test for a unit root in the time series considered by \textcite{nelson1982trends}. Last, we re-examine the issues of long memory and structural breaks in the
well-known Nile data. What all three applications have in common is that the datasets consist of short time series of 79 to 171 observations each, warranting the use of our MCSS estimator to correct the small-sample bias of the
received ML or CSS estimators\footnote{The computations are again performed using MATLAB 2019a with the code to reproduce the empirical examples available on request.}.


\subsection{Post-second World War real GNP}
\textcite{sowell1992modeling} conducted a well-known empirical analysis of the long-memory behaviour of U.S.\ post-Second World War quarterly, seasonally adjusted, log real GNP. The data\footnote{We use the data provided by
  \textcite{potter1995nonlinear} in the JAE Data Archive who mentions Citibase as his source, as does \textcite{sowell1992modeling}. The dataset can be downloaded from
  \url{https://journaldata.zbw.eu/dataset/a-nonlinear-approach-to-us-gnp}.} comprise observations from 1947:2 to 1989:4 and are displayed in panel (a) of Figure \ref{RGNPnile}.


\begin{figure}[H]
  \centering
  \subfloat[Post-Second World War real GNP data]{
    \includegraphics[width=0.48\textwidth]{rgnp_cropped.pdf}
  }
  \subfloat[Nile data]{
    \includegraphics[width=0.48\textwidth]{niledata_cropped.pdf}
  }
  \caption{Panel (a) 171 quarterly observations on first differences of log quarterly U.S.\ real GNP for the time period 1947:2 to 1989:4, as in \textcite{sowell1992modeling}. Panel (b) displays 100 annual observations of the
    volume of the Nile for the time period 1871 to 1970.}
  \label{RGNPnile}\end{figure}


\textcite{sowell1992modeling} estimates an ARFIMA(3,$d$,2) type-I model of mean-adjusted first differences using full maximum likelihood (ML), basing the lag order on the Akaike information criterion.  He obtains an estimated
memory parameter of $-0.59$. However, \textcite{smith1997fractional} assert that Sowell's results are substantially biased and especially the memory parameter is strongly underestimated. They propose a simulation-based bias
correction of the profile maximum likelihood (BC-PML) estimator, resulting in $\hat{d} = -0.46$. The BC-PML estimator relies on the assumption that the bias is a linear function in the parameters. However,
\textcite{lieberman2005expansions}\footnote{\textcite{lieberman2005expansions} consider the profile plug-in maximum likelihood estimator instead of the profile maximum likelihood estimator for tractability reasons.}  show this not
to be the case for a simple ARFIMA(0,$d$,0) type-I model. We circumvent this problem by using our MCSS estimator, which does not require the bias to be linear in the parameters. Table \ref{trgnp} presents the CSS and MCSS
estimates of $d$ for the ARFIMA(3,$d$,2) type-II model in \eqref{genq1}, along with the ML estimate of \textcite{sowell1992modeling} and the profile maximum likelihood (PML) as well as BC-PML estimate of
\textcite{smith1997fractional}. It can be noted, first, that the CSS estimate is of similar order of magnitude as the maximum likelihood estimates, compare e.g.\ CSS and (P)ML. Secondly, the bias-correction increases both the CSS
and PML estimates substantially, cf.\ MCSS and BC-PML. In fact, the CSS estimate is increased by a larger margin than the PML estimate. Thirdly, the type-II estimates are less significant than the type-I estimates, and the
significance is reduced by the bias-correction. In conclusion, our results indicate that the long memory parameter is closer to zero than previously thought, even relative to its standard error.


\begin{table}[H]
\centering
\begin{tabular}{l|EEE|EE}\hline
          & \multicolumn{3}{c}{type-I} & \multicolumn{2}{|c}{type-II}                                                                               \\ \cline{2-6}
          & \multicolumn{1}{c}{ML}     & \multicolumn{1}{c}{PML} & \multicolumn{1}{c}{BC-PML} & \multicolumn{1}{|c}{CSS} & \multicolumn{1}{c}{MCSS} \\ \hline
$\hat{d}$ & -0.59                      & -0.61                   & -0.46                      & -0.53                   & -0.26                    \\
SE        & 0.35                       & 0.29                    & 0.29                       & 0.37                    & 0.30                     \\ \hline
\end{tabular}
\caption{The memory parameter estimates for the ARFIMA(3,$d$,2) model and their standard errors. The standard errors are calculated using the inverse of the empirical Hessian matrix.}
\label{trgnp}
\end{table}



\subsection{Extended Nelson-Plosser dataset}

There is a long-standing controversy on whether it is apt to describe the 14 time series in the well-known \textcite{nelson1982trends} dataset, as extended by \textcite{schotman1991bayesian}\footnote{The dataset can be downloaded
  from \url{http://korora.econ.yale.edu/phillips/data/np&enp.dat} and is included in the R package `tseries'.}, by unit root processes. More recently, the literature on long memory processes has broadened the debate by considering
a fractional integration parameter $d$ that can take any value on the real line instead of merely zero or one. Yet the test statistics for the null hypothesis of $d=1$ tend to be close to their critical values, impeding strong
conclusions. Prominent papers are, amongst others, \textcite{crato1994fractional}, \textcite{gil1997testing}, \textcite{shimotsu2010exact} and \textcite{la2019saddlepoint}.

Our enquiry proceeds in two stages: First, we revisit \textcite{crato1994fractional}\footnote{Unfortunately, we did not succeed in replicating the results of \textcite{crato1994fractional}. They use Sowell's Fortan program
  GQSTRFRAC, which is not available to us. Also, \textcite[p.\ 110]{hassler2019time} mentions an error in the autocovariance formula of \textcite[eq.\ (8)]{sowell1992modeling}. We hence exercise caution in interpreting their
  results.} who use profile maximum likelihood (PML) to estimate an ARFIMA type-I model. We compare their PML to the CSS and MCSS estimates of $d$ in our type-II setting, using either the model in \eqref{q1} or
\eqref{genq1}. Secondly, we conduct unit root tests and relate them to the results obtained in the frequency-domain setting considered by \textcite{gil1997testing} and \textcite{shimotsu2010exact}. This comparison is of interest
because the MCSS estimator shares one interesting characteristic with frequency-domain estimators, namely that the leading bias of the estimator is not altered by an inclusion of a level parameter, a feature not present in PML or
CSS.



\begin{figure}[H] 
\centering
\includegraphics[scale=1]{Npplotspdf.pdf}
\caption{ The extended Nelson-Plosser data in levels. All of the series are in logs, except for the bond yield.}
\label{fignp}
\end{figure} 





\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{l|cc|RRR|RRR|RRR} \hline
                      &     &           & \multicolumn{3}{c|}{PML}      & \multicolumn{3}{c|}{CSS} & \multicolumn{3}{c}{MCSS}                                                                                                                                                                        \\
series                & $T$ & BIC       & \multicolumn{1}{c}{$\hat{d}$} & \multicolumn{1}{c}{SE}   & \multicolumn{1}{c|}{$t$} & \multicolumn{1}{c}{$\hat{d}$} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c|}{$t$} & \multicolumn{1}{c}{$\hat{d}$} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{$t$} \\ \hline
real GNP              & 79  & (1,$d$,0) & -0.41                         & 0.21                     & -1.95                    & -0.43                         & 0.21                   & -2.11                    & -0.32                         & 0.23                   & -1.42                   \\
nominal GNP           & 79  & (1,$d$,0) & -0.19                         & 0.24                     & -0.80                    & -0.21                         & 0.25                   & -0.85                    & -0.07                         & 0.26                   & -0.27                   \\
real per capita GNP   & 79  & (1,$d$,0) & -0.43                         & 0.22                     & -1.96                    & -0.44                         & 0.21                   & -2.10                    & -0.33                         & 0.23                   & -1.41                   \\
industrial production & 128 & (1,$d$,0) & -0.64                         & 0.33                     & -1.95                    & -0.59                         & 0.24                   & -2.42                    & -0.46                         & 0.21                   & -2.16                   \\
employment            & 98  & (0,$d$,1) & -0.19                         & 0.12                     & -1.60                    & -0.20                         & 0.12                   & -1.65                    & -0.14                         & 0.13                   & -1.09                   \\
unemployment rate     & 98  & (0,$d$,1) & -0.58                         & 0.11                     & -5.14                    & -0.57                         & 0.11                   & -5.17                    & -0.52                         & 0.11                   & -4.62                   \\
GNP price deflator    & 99  & (1,$d$,0) & -0.39                         & 0.21                     & -1.88                    & -0.40                         & 0.20                   & -1.95                    & 0.22                          & 0.27                  & 0.79                    \\
CPI                   & 128 & (0,$d$,1) & 0.19                          & 0.08                     & 2.24                     & 0.21                          & 0.09                   & 2.40                     & 0.24                          & 0.09                   & 2.60                    \\
real wage             & 88  & (0,$d$,0) & 0.12                          & 0.10                     & 1.16                     & 0.13                          & 0.11                   & 1.19                     & 0.17                          & 0.11                   & 1.59                    \\
nominal wage          & 88  & (1,$d$,0) & -0.21                         & 0.25                     & -0.85                    & -0.23                         & 0.25                   & -0.91                    & -0.07                         & 0.28                   & -0.24                   \\
money                 & 99  & (1,$d$,1) & -0.50                         & 0.22                     & -2.26                    & -0.52                         & 0.21                   & -2.49                    & -0.44                         & 0.26                   & -1.71                   \\
velocity              & 119 & (0,$d$,0) & 0.04                          & 0.08                     & 0.47                     & 0.04                          & 0.08                   & 0.46                     & 0.07                          & 0.08                   & 0.81                    \\
bond yields           & 88  & (0,$d$,1) & -0.19                         & 0.10                     & -1.81                    & -0.20                         & 0.10                   & -1.92                    & -0.15                         & 0.11                   & -1.42                   \\
SP500 index           & 117 & (0,$d$,1) & -0.21                         & 0.10                     & -2.21                    & -0.21                         & 0.09                   & -2.21                    & -0.17                         & 0.10                   & -1.76                   \\ \hline
\end{tabular}
}
\caption{Estimated ARFIMA models of the extended Nelson-Plosser data. The time series are transformed into log-differences, merely bond yields are only in differences. The second column shows the length $T$ of the individual series, the third
  column the model specifications based on the BIC for the profile maximum likelihood (PML) estimator. Subsequent columns then list the estimates of the memory parameter for PML, conditional sum-of-squares (CSS), and modified
      conditional sum-of-squares (MCSS). The empirical Hessian is used to calculate the standard errors, and the $t$-statistics are computed for the unit root null $H_0 \colon d=0$. The PML estimates are computed in R using the ‘arfima’ Package, see \textcite{R}.}
\label{fullsamplenp}
\end{table}

The extended Nelson-Plosser dataset consist of 14 annual macroeconomic series, starting between 1860 and 1909 and running to 1988, and are displayed in Figure \ref{fignp}. For the analysis, all of the series are log-differenced\footnote{The ``differencing and adding back'' technique, a
  commonly used method to simplify estimation by removing drift through differencing, has been found to deliver inconsistent CSS estimates in type-II models when the data in levels exhibit a memory parameter of less than 0. As a
  solution to this problem, \textcite{hualde2020truncated} recommend modelling the data in levels instead of first-differences or, alternatively, employing a single dummy variable to capture the initial observation. Implementing this latter approach, our results remain qualitatively the same.}, except for the bond yield, which is merely in differences. Table \ref{fullsamplenp} displays the PML, CSS and MCSS estimates of the memory parameter as well as their respective standard error and the $t$-statistics for testing the
unit root null that $d = 0$. Following \textcite{crato1994fractional}, the model selection is based on the BIC\footnote{\textcite{huang2022consistent} have recently shown the BIC criterion to provide consistent selection of the short-run
  dynamics when based on the CSS estimator in ARFIMA models without constant term.} of the PML estimator. The table reveals that (a) four of the PML $t$-statistics are larger than the 5\further five being borderline cases, (b) the MCSS estimates are consistently larger than the PML and CSS ones, and (c) of the MCSS $t$-statistics, only three lie in or close to the critical region. Another interesting point to note in Table
\ref{fullsamplenp} is that, for the GNP price deflator, PML and CSS provide a long memory estimate of $-0.39$ and $-0.40$, respectively, while the MCSS estimator yields a value of $+0.22$. This disparity may be attributed to the
fact that CSS strongly underestimates the memory parameter when positive AR(1) dynamics are present whereas the MCSS estimator eliminates the bias, as demonstrated in Theorem \ref{t54} and the simulation study presented in Section \ref{Ssimgen}. In summary,
we find greater evidence than in the previous literature in favour of the unit root hypothesis in 11 out of the 14 Nelson-Plosser series.

Let us now turn to the second issue of interest, i.e.\ the comparison of our time-domain estimation to the frequency-domain approaches in \textcite{gil1997testing} and \textcite{shimotsu2010exact}. For the unit root null
hypothesis, \textcite{gil1997testing} employ \citeauthor{robinson1994efficient}'s (\citeyear{robinson1994efficient}) LM-type test based on the Whittle (W) estimator, while \textcite{shimotsu2010exact} uses a $t$-type statistic based on
the extended local Whittle (ELW) objective function. Table \ref{tests} compares the results of the unit root test based on the frequency domain estimators W and ELW with those based on the time domain estimators in Table \ref{fullsamplenp}, a checkmark indicating that $H_0$ is (almost) rejected at the 5\be made from this table. First, the tests of \textcite{gil1997testing} and \textcite{shimotsu2010exact} give completely different outcomes, confirming the impression that there is presently no consensus in the literature on the unit root
issue. A discussion of the relative merits of the W and ELW estimators is provided in, for instance, \textcite{hualde2011gaussian}.
Secondly, the test decisions of \textcite{gil1997testing} are consistent with the majority of the MCSS tests. They only differ for real GNP, the unemployment rate and CPI, the reason for which could be that \textcite{gil1997testing} capture the short-run
dynamics solely through AR($k$) components, which may be somewhat restrictive considering that the BIC also discovers MA lags.


\begin{table}[H]
\centering
\begin{tabular}{l|ccc|cc}
\hline
                      & \multicolumn{5}{c}{rejection of unit root hypothesis }                                                                                   \\ \cline{2-6} 
                      & \multicolumn{3}{c|}{time domain} & \multicolumn{2}{c}{frequency domain}                                                                  \\
series                & \multicolumn{1}{c}{PML}          & \multicolumn{1}{c}{CSS} & \multicolumn{1}{c|}{MCSS} & \multicolumn{1}{c}{W} & \multicolumn{1}{c}{ELW} \\ \hline
real GNP              & (\checkmark)                     & (\checkmark)              &                           & \checkmark            &                         \\
nominal GNP           &                                  &                         &                           &                       & \checkmark              \\
real per capita GNP   & (\checkmark)                       & (\checkmark)              &                           &    (\checkmark)                     &                         \\
industrial production & (\checkmark)                     & \checkmark              & \checkmark                & \checkmark            &                         \\
employment            &                                  &                         &                           &                       &                         \\
unemployment rate     & \checkmark                       & \checkmark              & \checkmark                &                       &                         \\
GNP price deflator    & (\checkmark)                     &   (\checkmark)                        &                           &                       & \checkmark              \\
CPI                   & \checkmark                       & \checkmark              & \checkmark                &                       & \checkmark              \\
real wage             &                                  &                         &                           &                       &                         \\
nominal wage          &                                  &                         &                           &                       & \checkmark              \\
money                 & \checkmark                       & \checkmark              &                           &                       & \checkmark              \\
velocity              &                                  &                         &                           &                       &                         \\
bond yields           & (\checkmark)                     & (\checkmark)              &                           &                       &                         \\
SP500 index           & \checkmark                       & \checkmark              &                           &                       &                         \\ \hline
\end{tabular}
\caption{Summary of the unit root tests, based on time-domain and frequency-domain estimators.  W denotes the LM-type test of \textcite{gil1997testing} based on the Whittle estimator, while ELW is the LM-type test of
  \textcite{shimotsu2010exact} based on the extended local Whittle estimator.  The presence of a checkmark shows that the null hypothesis of a unit root is rejected at a 5\  alternative. A checkmark in parentheses means that the $t$-statistic is just outside the critical region.}
\label{tests}
\end{table}


  
\subsection{Nile data} \label{nilesec}

We now present an empirical application to the classical dataset\footnote{ The dataset used in this analysis can be obtained from the R package `datasets'.} on the annual water flow volume of the Nile for the years 1871 to 1970. The 100 time-series observations are displayed in panel (b) of  Figure \ref{RGNPnile}. Several
studies have analysed this dataset either in a long memory or short memory framework, with or without the presence of a break in the time series. \textcite{hosking1984modeling} and \textcite{boes1989parameter} focus on long memory without
considering a break. \textcite{macneill1991search}, \textcite{wu2007inference}, \textcite{macneill2020multiple} examine breaks in a short memory time series context. \textcite{atkinson1997detecting} look at breaks in a unit root
model. \textcite{shao2011simple} and \textcite{bet17} address the testing and estimation of a break using a procedure that is robust to long memory although, after identifying a break, they do not proceed to estimating the fractional
parameter. In summary, while there appears to be a consensus on including a break in the model, there is disagreement on whether the dynamics are better described by short or long memory. In particular, the literature currently
does not consider the estimation of the memory parameter that is robust to a break. This is what we aim to achieve.


To that end, we proceed in two steps: First, we extend our model in \eqref{genq1} to incorporate a break, i.e.\ $\mu$ in \eqref{genq1} is replaced by
\begin{align}
    \mu_t(\tau) = \mu + \beta I(t \leq \lfloor \tau T \rfloor),  \label{determinNIle}
\end{align}
where the break fraction $\tau \in (0,1)$ is assumed unknown. $\mu_t (\tau)$ can be consistently estimated in a type-II fractionally integrated model with $|d_0| < 1/2$, as shown by \textcite{chang2016inference} and
\textcite{iacone2019testing}. It is, however, necessary to generalise our Assumption \ref{A2} such that $q > 1/(1+2d_0)$ moments exist, see \textcite[Theorem 2]{johansen2012necessary}. In a second step, we employ the filtered observations
$\hat{x}_t = x_t - \hat{\mu}_t(\hat{\tau})$ to obtain the CSS estimates $\hat{\vartheta}$ in \eqref{genCSS} and the MCSS estimate $\hat{\vartheta}_m$ in \eqref{MCSSgen1}. The consistency of $\hat{\vartheta}$ in this model follows from similar arguments as in
\textcite[Proposition 1]{robinson2015efficient}, that of $\hat \vartheta_m$ in this model is easily obtained because of its asymptotic equivalence to the CSS estimator, see \eqref{geneq111} in Lemma \ref{gen:lm:md}. The model selection procedure
suggested by \textcite{hualde2011gaussian} is employed, consisting in a preliminary estimator $\tilde{d}$ of $d$ obtained by local Whittle estimation as in \textcite{robinson1995gaussian} before the procedure by \textcite{box1990time} is
applied for selecting the short-run dynamics of $\Delta^{\tilde{d}} \left\{\hat{x}_t\right\}$. The \textcite{lobato1998nonparametric} automatic selection rule of the bandwidth $m$ is used.


In the first step, we find that $\hat \tau = 0.27 $, translating into an estimated break in 1898. This is similar to what most of the aforementioned papers find, and it coincides with the beginning of the
construction of the Lower Aswan Dam in 1899. The estimates of the level and break magnitudes of, respectively, $\hat \mu (\hat \tau) = 849.97$ and $\hat \beta (\hat \tau) = 247.78$ imply that the flow volume was reduced by
22\

In the second step, we find a bandwidth of $m = 22$, resulting in a preliminary estimate of $\tilde{d} = -0.05$. The Box-Jenkins procedure indicates that the short-run dynamics are best described by a MA(1) model. The resulting
CSS and MCSS estimates are reported in Table \ref{tableNile}, along with their standard errors and $t$-statistics. The results are unambiguous: the MCSS estimate does not provide evidence of long memory in the Nile data once the
break is incorporated, with the point estimate of the memory parameter being $-0.12$.  The CSS estimate supports this conclusion, with an estimate of $-0.18$. In terms of short-run dynamics, however, CSS and MCSS differ: CSS
estimates the MA coefficient to be $0.30$, an estimate that is statistically significant at the 5\conclusion is that after incorporating the break, the Nile data is characterised by IID shocks. This finding aligns with that of \textcite{atkinson1997detecting}, supporting their argument that the series can be adequately described
by a white noise process once the break is taken into consideration\footnote{ \textcite{atkinson1997detecting} also identifies an outlier in the year 1913. However, even after removing this outlier, our results remain robust.}.

To corroborate our conclusion regarding the memory parameter, we employ the semi-parametric $t$-type statistic in \textcite{iacone2022semiparametric} to test the null hypothesis $H_0 \colon d_0 = 0$ against the alternative hypothesis
$H_1 \colon d_0 \neq 0$. This test is designed to be robust against breaks and has the advantage that a parametric specification of the shocks is not needed. The test result, omitted to conserve space, is conclusive and supports our
finding: after taking into account the break, there is no evidence that the Nile data exhibits long memory.


\begin{table}[H]
\centering
\begin{tabular}{c|EEE|EEE} \hline
          & \multicolumn{3}{c|}{CSS}      & \multicolumn{3}{c}{MCSS}                                                                                                             \\
Es          & \multicolumn{1}{c}{estimate} & \multicolumn{1}{c}{SE}   & \multicolumn{1}{c|}{$t$} & \multicolumn{1}{c}{estimate} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{$t$} \\ \hline
$d$       & -0.18                        & 0.13                     & -1.44                   & -0.12                        & 0.14                   & -0.81                   \\
$\varphi$ & 0.30                         & 0.13                     & 2.36                    & 0.26                         & 0.14                   & 1.88                   \\ \hline
\end{tabular}
\caption{CSS and MCSS estimates of the ARFIMA(0,$d$,1) model for the filtered observations $\hat x_t$ of the Nile data. The MA(1) coefficient is denoted by $\varphi$. The empirical Hessian matrix's inverse is used to calculate the
  standard errors.}
\label{tableNile}
\end{table}







\section{Final comments}\label{S5}



Practitioners like the CSS estimator due to its simplicity and effectiveness in estimating both stationary and non-stationary ARFIMA models. Recent work by \textcite{hualde2020truncated,hualde2021truncated} provides the asymptotic
justification for using the CSS estimator to estimate models that include deterministic components such as level and trend components. However, our analysis reveals that incorporating the level parameter into the model introduces
an additional bias in the CSS estimator. This bias is due to a biased score which is particularly pronounced when the data is stationary. To address this issue, we propose modifying the CSS profile objective function to create an
unbiased score, resulting in a new estimator which we call the modified CSS (MCSS) estimator. This new estimator is straightforward to compute and implement, enabling practitioners to obtain more accurate estimates and less
distorted tests and confidence intervals. We illustrate the MCSS estimator by three classical empirical applications. Our analysis is for the general ARFIMA($p_1$,$d$,$p_2$) model including a constant term. Various extensions are
conceivable and of interest, yet beyond the scope of this paper:

First, further deterministic components could be included in the model, e.g.\ a linear time trend: Denoting by $X$ a $T$ $\times$ 2 matrix of a constant and a linear trend then it can be shown that the modification term for the
MCSS objective function turns out to be
 \begin{align*}
          m(d,\varphi) = \left|(\phi(L;\varphi)\Delta_+^{d}X)'(\phi(L;\varphi)\Delta_+^{d}X)\right|^{\frac{1}{T-2}}. 
\end{align*}  
This modification term is again simple to calculate. Notably, it is equivalent to that in \eqref{genmodificationterm} if $X$ is only a vector of ones and the degrees of freedom in the power term is replaced to $T-1$. We expect
that the above modification term, corrected for the appropriate degrees of freedom, also holds for more general deterministic components in $X$. We conjecture that this MCSS estimator should improve on the CSS estimator and that
its bias should be the same as that of the CSS estimator with known parameters of the deterministic components.

Secondly, while our paper focused solely on univariate fractional time series, the topic takes on added interest when extended to a panel setting. For instance, \textcite{robinson2015efficient} extend the model presented in
equations \eqref{genq1}-\eqref{genq2} to a panel framework. While the CSS estimator in a panel setting is consistent under large-$T$ asymptotics, its finite sample properties are deficient due to the presence of fixed effects. To
address this issue, the authors propose a bias correction that depends on the true parameters, necessitating the use of estimates to render this correction feasible. However, as the finite sample properties of the CSS estimator is
unsatisfactory, substituting the true values with estimated ones leads to similarly non-optimal estimates. As an alternative to improving the small-sample properties of the CSS estimator a similar modification to the CSS objective
can be made as in Section \ref{theorygen}. The advantage of this approach is highlighted in the recent work of \textcite{schumann2023role}.
  





\printbibliography
\newpage

\appendix
\counterwithin*{equation}{section} \renewcommand\theequation{\thesection.\arabic{equation}} 
\section{Proof of the results in Section \ref{secgen}} \label{Appendixgeneraldgen}

In this appendix, we give the proofs of the results in Section \ref{secgen}, i.e.\ the general model in \eqref{genq1}-\eqref{genq2}. The results outlined in this Section \ref{S2} are special cases of that in Section \ref{secgen}. As such, they are also implicitly covered in this appendix. 

The setup of this appendix is as follows: In Appendix \ref{generalderivgen}, we find expressions for the first three derivatives of the profile objective functions, namely $L^*(\vartheta)$, $L_{\mu_0}^*(\vartheta)$ and
$L_{m}^*(\vartheta)$, evaluated at $\vartheta = \vartheta_0$. Appendix \ref{genlemmaap} presents some preliminary results that play a central role in approximating these derivatives. In Appendix \ref{ap3}, we analyse the terms involved in the derivatives and conclude with an asymptotic approximation of the derivatives. This approximation is divided into two parts: the non-stationary region, i.e.\ $d_0 > 1/2$, detailed in Appendix \ref{gennon1}, and the stationary region, i.e.\ $d_0 < 1/2$,
detailed in Appendix \ref{genstat1}. The decision to partition the analysis is rooted in the dependency of the convergence order of these terms on their respective regions. We
exclude the boundary case $d_0 = 0.5$ as it would necessitate a separate analysis which is beyond the scope of the present paper. Lastly, in Appendix \ref{generalizationsappendix}, we present the proofs of the main results in Section \ref{secgen}.


\subsection{Derivatives of the objective functions} \label{generalderivgen}

We first analyse the residuals $\epsilon_t(d, \varphi, \mu) = \phi(L;\varphi)\Delta_{+}^{d} x_t- \mu c_t(d,\varphi)$  for $t \geq 1$ and introduce some notations. We use a subscript zero to represent the true parameters. Clearly, inserting the DGP in \eqref{genq1} into the expression $\epsilon_t(d, \varphi, \mu)$ yields
\begin{align}
    \epsilon_t(d, \varphi, \mu) &= \phi(L;\varphi)\Delta_{+}^{d}(\mu_0 + \Delta_+^{-d_0} u_t) - \mu c_{t}(d,\varphi) \nonumber \\
    &= \phi(L;\varphi)\Delta_{+}^{d-d_0} u_t - c_{t}(d,\varphi) \left( \mu - \mu_0 \right) \nonumber \\
    &= S^+_t(\vartheta)  - c_{t}(\vartheta) \left( \mu - \mu_0 \right), \label{GenRDEF} 
\end{align}
where the stochastic term $S^+_t(\vartheta)$ is defined as 
\begin{align}
    S^+_t(\vartheta) = \phi(L;\varphi)\Delta_{+}^{d-d_0} u_t \label{Sterm}
\end{align}
and the deterministic term $c_{t}(\vartheta)$, see \eqref{convcoef}, is defined as 
\begin{align}
 c_{t}(\vartheta) = \phi(L;\varphi)\Delta_{+}^{d} I(t \geq 1) = \phi(L;\varphi) \kappa_{0t}(d) = \sum_{j = 0}^{t-1} \phi_j(\varphi) \kappa_{0(t-j)}(d),  \label{detc}
\end{align}
where $\kappa_{0t}(d)$ is defined in \eqref{definitionkappa}. 


The derivative of $\epsilon_t(d, \varphi, \mu)$ with respect to $i \in \{\vartheta_k,\vartheta_k \vartheta_j, \vartheta_k \vartheta_j,\vartheta_k \vartheta_j \vartheta_l \}$, for $k, j, l = 1,\ldots, p+1$, evaluated at $\vartheta = \vartheta_0$, are of the form 
\begin{align}
    D_{i}  \epsilon_t(d_0, \varphi_0, \mu) =  S^+_{i t}(\vartheta_0)  - c_{ i t}(\vartheta_0)\left( \mu - \mu_0 \right), \label{firstd1}
\end{align}
where 
\begin{align}
     S^+_{i t}(\vartheta_0) = D_{i} S^+_t(\vartheta_0), \label{firststoch}
\end{align}
and
\begin{align}
    c_{ i t}(\vartheta_0) = D_{i}  c_{t}(\vartheta_0).  \label{firstdet} 
\end{align}    

Throughout the appendix, we simplify notation by suppressing the dependence on $\vartheta_0$. For instance, we write $S^+_{i t}$ instead of $S^+_{i t}(\vartheta_0)$. We follow the following convention: the derivative of a function $f(x, y(x))$ with respect to $x$ is written as $D_x f(x, y(x))$, and the partial derivative with respect to $x$ is written as $f_x(x, y(x))$.

The following lemma provides simple analytic expressions for the derivatives of the stochastic terms given in \eqref{firststoch}.
\begin{lemma} \label{explicitforms} Assume that Assumption \ref{A5} holds, then 
    \begin{align}
        S_t^+ &= \epsilon_t, \label{exp0}\\
        S^+_{m t} &= (-1)^{m^*} \sum_{k = 0}^{t-1} D_m \pi_{k}(0) \epsilon_{t-k},  \label{exp1}\\
        S^+_{z t} &= \sum_{i = 1}^{t-1} b_{zi}(\varphi_0)  \epsilon_{t-i},\label{exp2} \\
        S^+_{ d z  t} &= -\sum_{i = 2}^{t-1} h_{dzi}(\varphi_0)  \epsilon_{t-i},\label{exp5} 
    \end{align}
    where $m \in \{d,dd,ddd \}$ and $m^*$ denotes the number of times $S^+_t(\vartheta)$ is differenced with respect to $d$ and where $z \in \{ \varphi_k, \varphi_k \varphi_j, \varphi_k \varphi_j \varphi_l \}$ for $k,j,l = 1,\ldots,p$ and 
    \begin{align}
        h_{dzi}(\varphi_0) = \sum_{s = 1}^{i-1} (i-s)^{-1} b_{zs}(\varphi_0), \label{defh} \\
        b_{zi}(\varphi_0) = \sum_{s = 0}^{i-1} \omega_s(\varphi_0) D_{z} \phi_{i-s}(\varphi_0). \label{defb}
    \end{align}
    Also,
    \begin{align}
        D_d \pi_j(0) &= j^{-1} I(j \geq 1), \label{dpi1} \\
        D_{dd} \pi_j(0) &= 2j^{-1} a_{j-1} I(j \geq 2), \label{dpi2}
    \end{align}
    where 
    \begin{align}
        a_j = I(j \geq 1) \sum_{k = 1}^j k^{-1}.
    \end{align}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{genderivatesLstar}.]


Define $z_t(\varphi) = \phi(L;\varphi) { u_t I (t \geq 1) }$ and evaluating this expression at $\varphi = \varphi_0$ results in $z_t(\varphi_0) = \epsilon_t - \sum_{j = t}^{\infty} \phi_j(\varphi_0) u_{t-j} = \epsilon_t$, which follows from Assumption \ref{A5}. Moreover, by observing that $S^+_t(d, \varphi) = \Delta_+^{d-d_0} z_t(\varphi)$, which can be used to conclude the proof of \eqref{exp0} and \eqref{exp1}. Additionally, note that $z_t(\varphi) = \phi(L;\varphi) \omega(L;\varphi_0) \phi(L;\varphi_0) { u_t I (t \geq 1) } = \phi(L;\varphi) \omega(L;\varphi_0){\epsilon_t I(t \geq 1)}$, which can be used to establish \eqref{exp2}. By employing similar arguments and considering that \eqref{dpi1} and \eqref{dpi2} are provided in \textcite[Lemma A.4]{johansen2016role}, the remaining expression follows.
\end{proof}

Next, we find expressions for the first three derivatives of $L^*(\vartheta)$, $L^*_{\mu_0}(\vartheta)$ and $L^*_{m}(\vartheta)$ and evaluate them at $\vartheta = \vartheta_0$. We present them in the same order. 

Recall that $L^*(\vartheta)$ in \eqref{genL1} equals $L(\vartheta,\mu(\vartheta))$, where $L(\vartheta,\mu) $ is given by 
\begin{align*}
      L(\vartheta,\mu) = \frac{1}{2} \sum_{t = 1}^T \left(  \phi(L;\varphi)\Delta_{+}^{d} x_t- \mu  c_t(d,\varphi)  \right)^2,
\end{align*}
and $\mu(\vartheta) = \hat{\mu}(\vartheta)$ is given in \eqref{genmu1}. The first derivative of $L^*(\vartheta)$ with respect to $\vartheta_k$ equals
\begin{align*}
    D_{\vartheta_k} L^*(\vartheta) =  L_{\vartheta_k}(\vartheta,\mu(\vartheta)) +  L_{\mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_k}(\vartheta). 
\end{align*}
We simplify this expression by noticing that $\hat{\mu}(\vartheta)$ is determined from $L_{\mu}(\vartheta,\mu(\vartheta)) = 0$ such that 
\begin{align}
    D_{\vartheta_k} L^*(\vartheta) =  L_{\vartheta_k}(\vartheta,\mu(\vartheta)).  \label{genproofqw12}
\end{align}
Next, we take the derivative of \eqref{genproofqw12} with respect to $ \vartheta_j$ to get an expression for $D_{\vartheta_k \vartheta_j} L^*(\vartheta)$. Using the chain rule we have that 
\begin{align}
   D_{\vartheta_k \vartheta_j } L^*(\vartheta) &=  L_{\vartheta_k  \vartheta_j}(\vartheta,\mu(\vartheta)) + L_{\vartheta_k \mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_j}(\vartheta). \label{genproofqw13}
\end{align}
Taking on both sides the derivative with respect to $\vartheta_j$ of $L_{\mu}(\vartheta,\mu(\vartheta)) = 0$ implies $L_{ \vartheta_j \mu}(\vartheta,\mu(\vartheta)) + L_{\mu \mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_j}(\vartheta) = 0$ such that
\begin{align*}
    \mu_{\vartheta_j}(\vartheta) = -\frac{L_{\vartheta_j \mu}(\vartheta,\mu(\vartheta))}{L_{\mu \mu}(\vartheta,\mu(\vartheta)) }. 
\end{align*} 
Lastly, we take the derivative of \eqref{genproofqw13} with respect to $\vartheta_l$ to get an expression for $ D_{\vartheta_k \vartheta_j \vartheta_l } L^*(\vartheta)$. We get that 
\begin{align}
D_{\vartheta_k \vartheta_j \vartheta_l } L^*(\vartheta) = &L_{\vartheta_k  \vartheta_j \vartheta_l}(\vartheta,\mu(\vartheta)) +  L_{\vartheta_k  \vartheta_j \mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_l}(\vartheta) +  L_{\vartheta_k  \vartheta_l \mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_j}(\vartheta) \nonumber \\
&+   L_{\vartheta_k  \mu \mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_l}(\vartheta) \mu_{\vartheta_j}(\vartheta)
+ L_{\vartheta_k  \mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_j \vartheta_l}(\vartheta).  \label{genproofqw14}
\end{align}
An expression for $ \mu_{\vartheta_j \vartheta_l}(\vartheta)$ can then be easily found by taking on both sides the derivative with respect to $\vartheta_l$ of $L_{\vartheta_j \mu}(\vartheta,\mu(\vartheta)) + L_{\mu \mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_j}(\vartheta) = 0$. We find that
\begin{align*}
  0 = &L_{\vartheta_j \vartheta_l \mu }(\vartheta,\mu(\vartheta)) + L_{ \vartheta_j \mu \mu  }(\vartheta,\mu(\vartheta)) \mu_{\vartheta_l}(\vartheta) \\ &+ 
   \left(  L_{\vartheta_l \mu \mu}(\vartheta,\mu(\vartheta)) +  L_{\mu \mu \mu }(\vartheta,\mu(\vartheta))  \mu_{\vartheta_l}(\vartheta) \right) \mu_{\vartheta_j}(\vartheta) 
   + L_{\mu \mu}(\vartheta,\mu(\vartheta)) \mu_{\vartheta_j \vartheta_l}(\vartheta), 
\end{align*}
and by rewriting 
\begin{align*}
    \mu_{\vartheta_j \vartheta_l}(\vartheta) = &-\frac{L_{ \vartheta_j \vartheta_l \mu}(\vartheta,\mu(\vartheta))}{L_{\mu \mu}(\vartheta,\mu(\vartheta))} -  \mu_{\vartheta_l}(\vartheta)  \frac{  L_{ \vartheta_j  \mu \mu }(\vartheta,\mu(\vartheta))}{L_{\mu \mu}(\vartheta,\mu(\vartheta))}  - \mu_{\vartheta_j}(\vartheta)  \frac{  L_{\vartheta_l \mu \mu }(\vartheta,\mu(\vartheta)) }{L_{\mu \mu}(\vartheta,\mu(\vartheta))} \\
    &-  \mu_{\vartheta_j}(\vartheta) \mu_{\vartheta_l}(\vartheta)  \frac{  L_{\mu \mu \mu }(\vartheta,\mu(\vartheta)) }{L_{\mu \mu}(\vartheta,\mu(\vartheta))}.  
\end{align*}

Next, we find expressions for the first three derivatives of $L^*(\vartheta)$, given in \eqref{genL1}, and present them in the following lemma. The derivatives are evaluated at $\vartheta = \vartheta_0$, and recall that we omit the explicit dependence. 
\begin{lemma} \label{genderivatesLstar}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and assume that Assumption \ref{A5} holds. Then the derivatives of $L^*(\vartheta)$, see \eqref{genL1}, evaluated at $\vartheta = \vartheta_0$ are given by
\begin{align}
      D_{\vartheta_k} L^* &=  L_{\vartheta_k}, \label{genqw11}\\
    D_{\vartheta_k \vartheta_j } L^* &=  L_{\vartheta_k  \vartheta_j} + L_{\vartheta_k \mu}\mu_{\vartheta_j} , \label{genqw12}\\
    D_{\vartheta_k \vartheta_j \vartheta_l } L^* &= L_{\vartheta_k  \vartheta_j \vartheta_l} +  L_{\vartheta_k  \vartheta_j \mu} \mu_{\vartheta_l} +  L_{\vartheta_k  \vartheta_l \mu} \mu_{\vartheta_j} \nonumber \\
&\ \  \  +   L_{\vartheta_k  \mu \mu} \mu_{\vartheta_l} \mu_{\vartheta_j}
+ L_{\vartheta_k  \mu} \mu_{\vartheta_j \vartheta_l} , \label{genqw13}
\end{align}
where 
\begin{align*}
     \mu_{\vartheta_j} &= -\frac{L_{ \vartheta_j \mu}}{L_{\mu \mu} }, \\
      \mu_{\vartheta_j \vartheta_l} &= -\frac{L_{\vartheta_j \vartheta_l \mu }}{L_{\mu \mu}} -  \mu_{\vartheta_l} \frac{  L_{ \vartheta_j \mu \mu  }}{L_{\mu \mu}}  - \mu_{\vartheta_j}  \frac{  L_{ \vartheta_l \mu \mu} }{L_{\mu \mu}} 
    -  \mu_{\vartheta_j} \mu_{\vartheta_l}  \frac{  L_{\mu \mu \mu } }{L_{\mu \mu}}.
\end{align*} 
for $k,j,l = 1,\ldots, p+1$.
The partial derivatives of $ L(\vartheta,\mu(\vartheta))$ evaluated at $\vartheta = \vartheta_0$ can be expressed as 
\begin{align*}
    L_{\vartheta_k} 
    &= \sum_{t = 1}^T \left(S_{t}^+ - c_{t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{\vartheta_k t}^+ -  c_{\vartheta_k t}\left(\mu(\vartheta_0)-\mu_0\right)\right), \\
     L_{\vartheta_k \vartheta_j } &= \sum_{t = 1}^T \left( S_{\vartheta_j t}^+ -  c_{\vartheta_j t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{\vartheta_k t}^+ -  c_{\vartheta_k t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    &\ \ \ + \sum_{t = 1}^T  \left(S_{t}^+ - c_{t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{ \vartheta_k  \vartheta_j t}^+ -  c_{\vartheta_k  \vartheta_j t}\left(\mu(\vartheta_0)-\mu_0\right)\right), \\
      L_{\vartheta_k \vartheta_j \vartheta_l} &= \sum_{t = 1}^T \left( S_{\vartheta_j \vartheta_l t}^+ -  c_{\vartheta_j \vartheta_l t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{\vartheta_k t}^+ -  c_{\vartheta_k t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    &\ \ \ + \sum_{t = 1}^T  \left(S_{\vartheta_j }^+ - c_{\vartheta_j t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{ \vartheta_k \vartheta_l t}^+ -  c_{\vartheta_k \vartheta_l t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \\ 
     &\ \ \ + \sum_{t = 1}^T  \left(S_{\vartheta_l t}^+ - c_{\vartheta_l t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{ \vartheta_k \vartheta_j t}^+ -  c_{\vartheta_k \vartheta_j t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
      &\ \ \ + \sum_{t = 1}^T  \left(S_{t}^+ - c_{ t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{ \vartheta_k \vartheta_j \vartheta_l t}^+ -  c_{\vartheta_k \vartheta_j \vartheta_l t}\left(\mu(\vartheta_0)-\mu_0\right)\right), \\
    L_{\vartheta_k \mu}&= - \sum_{t = 1}^T  c_{ t} \left( S_{\vartheta_k t}^+ -  c_{\vartheta_k t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    &\ \ \ -  \sum_{t = 1}^T  \left( S_{0 t}^+ -  c_{ t} \left(\mu(\vartheta_0)-\mu_0\right)\right) c_{\vartheta_k t},  \\
    L_{\vartheta_k \mu \mu }&=  2 \sum_{t = 1}^T  c_{ t}  c_{\vartheta_k t}, \\
    L_{\vartheta_k \vartheta_j \mu} &= -\sum_{t = 1}^T  c_{\vartheta_j t} \left( S_{\vartheta_k t}^+ -  c_{\vartheta_k t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    &\ \ \ - \sum_{t = 1}^T  c_{t} \left( S_{ \vartheta_k \vartheta_j t}^+ -  c_{\vartheta_k \vartheta_j t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \\ 
     &\ \ \ - \sum_{t = 1}^T  \left(S_{\vartheta_j t}^+ - c_{\vartheta_j t}\left(\mu(\vartheta_0)-\mu_0\right)\right) c_{\vartheta_k  t} \\
      &\ \ \ - \sum_{t = 1}^T  \left(S_{ t}^+ - c_{t}\left(\mu(\vartheta_0)-\mu_0\right)\right)  c_{\vartheta_k \vartheta_j t}, \\
         L_{\mu} &= -\sum_{t = 1}^T \left(S_{t}^+ - c_{t}\left(\mu(\vartheta_0)-\mu_0\right)\right)  c_{t}, \\
            L_{\mu \mu}&= \sum_{t = 1}^T  c^2_{t}, \\
          L_{\mu \mu \mu}&= 0.    
\end{align*}
Here, $\mu(\vartheta_0) = \hat{\mu}(\vartheta_0)$ and the stochastic term $S_{t}^+$ is defined in \eqref{Sterm} and its derivatives are given in \eqref{firststoch}. The deterministic term $c_{t} $ is defined in \eqref{detc} and its derivatives are given in \eqref{firstdet}.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{genderivatesLstar}.]
The proof of \eqref{genqw11}, \eqref{genqw12} and \eqref{genqw13} is given in \eqref{genproofqw12},\eqref{genproofqw13} and \eqref{genproofqw14}, respectively. The partial derivatives of  $L(\vartheta,\mu(\vartheta))$ follow from the relationship 
\begin{align*}
    L(\vartheta,\mu) = \frac{1}{2} \sum_{t = 1}^T\epsilon^2_t(d, \varphi, \mu),
\end{align*}
where $\epsilon_t(d, \varphi, \mu)$ is given in \eqref{GenRDEF} and its derivatives are provided in \eqref{firstd1}. The proof follows easily by using these derivatives.
\end{proof}



Next, we find expressions for the first three derivatives of $L_{\mu_0}^*(\vartheta)$, given in \eqref{genlikmu1known}, and evaluate them at $\vartheta = \vartheta_0$.  

\begin{lemma} \label{genderivatesLstarmu}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and assume that Assumption \ref{A5} holds. Then the derivatives of  $L_{\mu_0}^*(\vartheta)$, see \eqref{genlikmu1known}, evaluated at $\vartheta = \vartheta_0$ are  given by
\begin{align}
     D_{\vartheta_k} L_{\mu_0}^* &= \sum_{t = 1}^T S_{t}^+ S_{\vartheta_k t}^+ , \label{genmuqw11}\\
     D_{\vartheta_k \vartheta_j} L_{\mu_0}^* &= \sum_{t = 1}^T  S_{\vartheta_j t}^+ S_{\vartheta_k t}^+ + \sum_{t = 1}^T  S_{t}^+  S_{ \vartheta_k  \vartheta_j t}^+,  \label{genmuqw12} \\ D_{\vartheta_k \vartheta_j \vartheta_l} L_{\mu_0}^* &= \sum_{t = 1}^T  S_{t}^+  S_{ \vartheta_k \vartheta_j \vartheta_l t}^+   + \sum_{t = 1}^T  S_{\vartheta_j \vartheta_l t}^+ S_{\vartheta_k t}^+ +  \sum_{t = 1}^T S_{\vartheta_j }^+ S_{ \vartheta_k \vartheta_l t}^+ + \sum_{t = 1}^T  S_{\vartheta_l t}^+ S_{ \vartheta_k \vartheta_j t}^+, \label{genmuqw13}
\end{align}
for $k,j = 1,\ldots, p+1$.  Here, the stochastic term $S_{t}^+$ is defined in \eqref{Sterm}, and its derivatives are given in \eqref{firststoch}.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{genderivatesLstarmu}.]
Recall that
\begin{align*}
     L_{\mu_0}^*(\vartheta) = \frac{1}{2} \sum_{t = 1}^T \epsilon^2_t(d, \varphi, \mu_0),
\end{align*}
where  $\epsilon_t(d, \varphi, \mu)$ is given in \eqref{GenRDEF}. The second term in \eqref{GenRDEF} becomes zero when $\mu$ is equal to $\mu_0$. This simplifies the proof, which can now be easily derived using \eqref{firstd1}.
\end{proof}



Finally, we find expressions for the first three derivatives of $L_m^*(\vartheta)$, see \eqref{genmlik}, and evaluate them $\vartheta = \vartheta_0$. 

\begin{lemma} \label{genderivatesLstarMCSS}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and assume that the assumption \ref{A5} holds. Then the derivatives of  $L_m^*(\vartheta)$, given in \eqref{genmlik}, evaluated at $\vartheta = \vartheta_0$ are  given by
\begin{align}
    D_{\vartheta_k} L^* &=  m D_{\vartheta_k} L^*   + m_{\vartheta_k} L^*, \label{genmcssqw11}\\
    D_{\vartheta_k \vartheta_j} L^* &=  m D_{\vartheta_k \vartheta_j} L^* + m_{\vartheta_j} D_{\vartheta_k} L^* + m_{\vartheta_k \vartheta_j} L^* +  m_{\vartheta_k} D_{\vartheta_j}L^* ,  \label{genmcssqw12} \\
    D_{\vartheta_k \vartheta_j \vartheta_l} L^* &=  m D_{\vartheta_k \vartheta_j \vartheta_l} L^* +  m_{\vartheta_l} D_{\vartheta_k \vartheta_j} L^* + m_{\vartheta_j \vartheta_l} D_{\vartheta_k} L^* + m_{\vartheta_j} D_{\vartheta_k \vartheta_l} L^*  \nonumber \\
    &\ \ \ + m_{\vartheta_k \vartheta_j \vartheta_l} L^* +  m_{\vartheta_k \vartheta_j } D_{\vartheta_l}L^* + m_{\vartheta_k \vartheta_l} D_{\vartheta_j}L^* + m_{\vartheta_k} D_{\vartheta_j \vartheta_l}L^*, \label{genmcssqw13}
\end{align}
where expression for the derivatives of $L^*$ are given in Lemma \ref{genderivatesLstar} and the modification term $m(\vartheta)$ is given in \eqref{genmodificationterm} and the derivatives of $m(\vartheta)$, evaluated at $\vartheta = \vartheta_0$, are given by
\begin{align*}
   m &= \left( \sum_{t = 1}^T c_t^2 \right)^{\frac{1}{T-1}},  \\
    m_{\vartheta_k} &= \frac{2}{T-1}  \left( \sum_{t = 1}^T c_t^2  \right)^{-\frac{T-2}{T-1}} \sum_{t = 1}^T  c_t  c_{\vartheta_k t},\\
   m_{\vartheta_k \vartheta_j}  &=  \frac{2}{T-1}  \left( \sum_{t = 1}^T c_t^2  \right)^{-\frac{T-2}{T-1}} \sum_{t = 1}^T \left( c_{\vartheta_j t}  c_{\vartheta_k t} + c_t  c_{\vartheta_k \vartheta_j  t}  \right)  \\ &{\ \ } - 4\frac{T-2}{(T-1)^2}  \left( \sum_{t = 1}^T c_t^2 \right)^{-\frac{2T-3}{T-1}} \sum_{t = 1}^T  c_t  c_{\vartheta_k t} \sum_{t = 1}^T  c_t  c_{\vartheta_j t} , \\
    m_{\vartheta_k \vartheta_j  \vartheta_l }  &=  \frac{2}{T-1}  \left( \sum_{t = 1}^T c_t^2  \right)^{-\frac{T-2}{T-1}} \sum_{t = 1}^T \left( c_{\vartheta_j \vartheta_l t}  c_{\vartheta_k t} +  c_{\vartheta_j t}  c_{\vartheta_k \vartheta_l t} + c_{\vartheta_l t}  c_{\vartheta_k \vartheta_j  t} + c_t  c_{\vartheta_k \vartheta_j \vartheta_l t}  \right)  \\ 
    &\ \ \  - 4\frac{T-2}{(T-1)^2}  \left( \sum_{t = 1}^T c_t^2  \right)^{-\frac{2T-3}{T-1}}  \sum_{t = 1}^T c_t c_{\vartheta_l  t} \sum_{t = 1}^T \left( c_{\vartheta_j t}  c_{\vartheta_k t} + c_t  c_{\vartheta_k \vartheta_j  t}  \right) \\
    &\ \ \ - 4\frac{T-2}{(T-1)^2}  \left( \sum_{t = 1}^T c_t^2 \right)^{-\frac{2T-3}{T-1}} \left(\left( \sum_{t = 1}^T  c_{\vartheta_l t}  c_{\vartheta_k t} + \sum_{t = 1}^T  c_t  c_{\vartheta_k \vartheta_l t} \right) \sum_{t = 1}^T  c_t  c_{\vartheta_j t}  \right.\\ 
    &\left.\ \ \ +  \sum_{t = 1}^T  c_t  c_{\vartheta_k t} \left( \sum_{t = 1}^T  c_{\vartheta_l t}  c_{\vartheta_j t} + \sum_{t = 1}^T  c_t  c_{\vartheta_j \vartheta_l t}  \right) \right) \\
     &\ \ \ + 8\frac{(T-2)(2T-3)}{(T-1)^3}  \left( \sum_{t = 1}^T c_t^2 \right)^{-\frac{3T-4}{T-1}}  \sum_{t = 1}^T c_t c_{\vartheta_l t} \sum_{t = 1}^T  c_t  c_{\vartheta_k t} \sum_{t = 1}^T  c_t  c_{\vartheta_j t}.
\end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{genderivatesLstarMCSS}.]
Proof is straightforward due to the multiplicative form of the MCSS objective function, see \eqref{genmlik}.
\end{proof}








\subsection{Preliminary results} \label{genlemmaap}

In this section, we present findings that play a central role in the approximation of the derivatives. Appendix \ref{UB} contains usefull bounds, while Appendix \ref{UB1} presents results related to fractional coefficients in \eqref{fracpar}, and their derivatives, and the weights of the lag polynomial in \eqref{repmainf} and the inverse of the lag polynomial in \eqref{inverselag} along with their derivatives. In Appendix \ref{UB2}, we investigate the limiting behavior of the centered product moments, which are particularly relevant in the later expression of the biases. Lastly, Appendix \ref{UB3} focuses on the expectation of the CSS score function, which is a part of the bias term in the CSS estimator.



\subsubsection{Useful bounds}\label{UB} 
In this section, we provide some general results that are useful for finding the approximation of the derivatives. We sometimes apply them in the remainder without special reference.

\begin{lemma} \label{series}
    For any $d>-1$, as $T\rightarrow \infty$,
    \begin{align}
        \frac{1}{T^{d+1}} \sum_{t = 1}^T t^d \rightarrow \frac{1}{d+1}.\label{series1}
    \end{align}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{series}] See \textcite[Lemma S.10]{hualde2020truncated}. 
\end{proof}



Next, we present some useful bounds that are frequently used in the remainder of the appendix.

\begin{lemma} \label{genbounds}
For $m \geq 0$ and $c < \infty$, 
\begin{align}
    \sum_{n = 1}^N (1+\log(n))^m n^{\alpha} &\leq c (1+\log(N))^m N^{\alpha + 1} \text{ if } \alpha > -1, \label{lA1} \\
    \sum_{n = N}^{\infty} (1+\log(n))^m n^{\alpha} &\leq c (1+\log(N))^m N^{\alpha + 1} \text{ if } \alpha < -1.\label{lA2}
\end{align}
For $\alpha< 0$, and any $\beta$ it holds that 
\begin{align}
    \sum_{n = 1}^{t-1} n^{\alpha-1} (t-n)^{\beta - 1} \leq c t^{\max(\alpha-1,\beta-1)}. \label{lA3}
\end{align}
For $\alpha\geq 0$, and any $\beta$ it holds that 
\begin{align}
    \sum_{n = 1}^{t-1} n^{\alpha-1} (t-n)^{\beta - 1} \leq c (1 + \log(t)) t^{\max(\alpha+\beta-1,\alpha-1,\beta-1)}. \label{lA4}
\end{align}
For $\alpha+\beta < 1$ and $\beta > 0$ it holds that 
\begin{align}
    \sum_{k = 1}^{\infty} (k+h)^{\alpha-1} k^{\beta-1} (1+\log(k+h))^n \leq c h^{\alpha+\beta-1} (1+\log(h))^n. \label{lA5}
\end{align}

\end{lemma}
\begin{proof}[Proof of Lemma \ref{genbounds}] Proof of \eqref{lA1} and \eqref{lA2}: See \textcite[Lemma A.1]{johansen2016role}. \\
Proof of \eqref{lA3}: The proof follows a similar approach to the proof in \textcite[Lemma 1]{hualde2011gaussian}.
Cleary,
\begin{align*}
    \sum_{n = 1}^{t-1} n^{\alpha-1} (t-n)^{\beta - 1} &\leq  c \sum_{n = 1}^{\lfloor t/2 \rfloor} n^{\alpha-1} (t-n)^{\beta - 1} +  c \sum_{n = \lfloor t/2 \rfloor}^{t-1} n^{\alpha-1} (t-n)^{\beta - 1} \\
    &\leq c t^{\beta - 1} \sum_{n = 1}^{\lfloor t/2 \rfloor} n^{\alpha-1} +  c t^{\alpha-1} \sum_{n = \lfloor t/2 \rfloor}^{t-1} (t-n)^{\beta - 1}, \\
\end{align*}
because $\alpha < 0$ the first summand is $O(1)$ and the second summand is $O(1)$ if $\beta < 0$, $O(\log(t))$ if $\beta = 0$, and $O(t^\beta)$ if $\beta > 0$. \\
Proof of \eqref{lA4} and \eqref{lA5}: See \textcite[Lemma A.5]{johansen2016role}.

\end{proof}


\subsubsection{Bounds for the (derivates of) fractional coefficients and short-run dynamics} \label{UB1}

Next, we present findings concerning the fractional coefficients in \eqref{fracpar} and their derivatives, as well as the weights of the lag polynomial in \eqref{repmainf} and the weights of the inverse of the lag polynomial in \eqref{inverselag} and their derivatives.

\begin{lemma}\label{r11}
    For $m \geq 0$ and $j \geq 1$ it holds that 
    \begin{align}
        |D^m \pi_j(u)| \leq c (1+\log(j))^{m} j^{u-1} \label{r11_1}
    \end{align}
    Under Assumption \ref{A3} and \ref{A1} it follows, as $j \rightarrow \infty$,
    \begin{align}
    \underset{\varphi \in \Phi  }{\sup} \left| \omega_j(\varphi) \right|  &= O(j^{-1-\varsigma}), \label{r11_21}\\
     \underset{\varphi \in \Phi  }{\sup} \left| \phi_j(\varphi) \right|  &= O(j^{-1-\varsigma}), \label{r11_211}\\
     \underset{\varphi \in \Phi  }{\sup} \left| \frac{ \partial \phi_j(\varphi)}{\partial \varphi_i }\right|  &= O(j^{-1-\varsigma}),\label{r11_2} \\
    \underset{\varphi \in \Phi  }{\sup} \left| \frac{ \partial^2 \phi_j(\varphi)}{\partial \varphi_i \partial \varphi_l}\right|  &= O(j^{-1-\varsigma}),\label{r11_3} \\
    \underset{\varphi \in \Phi  }{\sup} \left| \frac{ \partial^3 \phi_j(\varphi)}{\partial \varphi_i \partial \varphi_l \partial \varphi_k}\right|  &= O(j^{-1-\varsigma}),\label{r11_4}
    \end{align}
for $i,l,k = 1,\ldots,p$ and where $1/2 < \varsigma  \leq 1$
\end{lemma}
\begin{proof}[Proof of Lemma \ref{r11}] Proof of \eqref{r11_1}:  See \textcite[Lemma A.3]{johansen2016role} \\
Proof of \eqref{r11_21}-\eqref{r11_4}: See \textcite[page 46]{zygmund1977trigonometric} and \textcite[page 3155 and page 3169]{hualde2011gaussian}.
\end{proof}

Next, we present bounds for the deterministic term in \eqref{detc} and their derivatives and the terms in \eqref{defh} and \eqref{defb}. 
\begin{lemma}\label{r12}
For any integer $m \geq 0$ and under Assumption \ref{A3} and \ref{A1},
\begin{align}
      \left|\frac{\partial^{m} c_t(\vartheta)}{ \partial d^m } \right| = O(t^{\max(-d,-1-\varsigma)} \log^m(t)), \label{r12_1}\\
      \left|\frac{\partial^{m+1} c_t(\vartheta)}{ \partial d^m \partial \varphi_i(\varphi)} \right| = O(t^{\max(-d,-1-\varsigma)} \log^m(t)), \label{r12_2} \\
     \left|\frac{\partial^{m+2} c_t(\vartheta)}{ \partial d^m \partial \varphi_i \partial \varphi_l} \right| = O(t^{\max(-d,-1-\varsigma)} \log^m(t)), \label{r12_3}\\
      \left|\frac{\partial^{m+3} c_t(\vartheta)}{ \partial d^m \partial \varphi_i \partial \varphi_l \partial \varphi_k } \right| = O(t^{\max(-d,-1-\varsigma)} \log^m(t)), \label{r12_4} 
\end{align}
for $i,l,k = 1,\ldots,p$ and where $1/2 < \varsigma  \leq 1$. Also, for $j \rightarrow \infty$ it holds that  
\begin{align}
|h_{dzj}(\varphi_0)| &= O(j^{-1}), \label{hproof}\\
|b_{zj}(\varphi_0)| &=  O(j^{-1-\varsigma}), \label{bproof}   
\end{align}
where $h_{dzi}(\varphi_0)$ and $b_{zi}(\varphi_0)$ are, respectively, defined in \eqref{defh} and \eqref{defb} and $z \in \{ \varphi_k, \varphi_k \varphi_j, \varphi_k \varphi_j \varphi_l \}$ for $k,j,l = 1,\ldots,p$. 

\end{lemma}
\begin{proof}[Proof of Lemma \ref{r12}] Proof of \eqref{r12_1}-\eqref{r12_4}: We give the proof of \eqref{r12_4} only, as the bounds on derivatives of the weight of the inverse lag polynomials are the same order according to Lemma \ref{r11}, resulting in a similar proof. From $k_{0t}(d) = \pi_{t-1}(1-d)$ and Lemma \ref{r11},
\begin{align*}
     \left|\frac{\partial^{m+3} c_t(\vartheta)}{ \partial d^m \partial \varphi_i \partial \varphi_l \partial \varphi_k } \right| &= \sum_{j = 0}^{t-1}  \left|\frac{\partial^3 \phi_j(\varphi)}{\partial  \partial \varphi_i \partial \varphi_l \partial \varphi_k} \right| \left| \frac{\partial^m \kappa_{0(t-j)}(d)}{\partial d^m} \right|, \\
     &\leq c \sum_{j = 1}^{t-1} \log^m(j) j^{-1-\varsigma} (t-j-1)^{-d}, \\
     &\leq c \log^m(t) \sum_{j = 1}^{t-1} j^{-1-\varsigma} (t-j-1)^{-d},
\end{align*}
because $\varsigma > 1/2$ this summand is $O(t^{\max(-d,-1-\varsigma)})$ from Lemma \ref{genbounds}. \\
Proof of \eqref{hproof} and \eqref{bproof}: From Lemma \ref{r11}, 
\begin{align*}
    |b_{zi}(\varphi_0)| \leq \sum_{s = 0}^{i-1} |\omega_s(\varphi_0) | |D_{z} \phi_{i-s}(\varphi_0)| = O(\sum_{s = 1}^{i-1}  s^{-1-\varsigma} (i-s)^{-1-\varsigma} ) = O(i^{-1-\varsigma}).
\end{align*}
The last equality follows from Lemma \ref{genbounds} because $\varsigma > 1/2$. 

Then from Lemma \ref{genbounds} and the bound above it follows that 
\begin{align*}
    |h_{dzi}(\varphi_0)| = O(\sum_{s = 1}^{i-1} (i-s)^{-1} s^{-1-\zeta} ) = O(i^{-1}).
\end{align*}
\end{proof}


\subsubsection{Limit behaviour of the centered product moments} \label{UB2}


Let $k,j,l = 1,\ldots, p+ 1$. Define the centered product moments of the derivative of the stochastic terms in \eqref{firststoch} as
\begin{align}
    M^+_{0,\vartheta_k  T} &= \sigma^{-2}_0 T^{-1/2} \sum_{t = 1}^T \left( S^+_{t} S^+_{\vartheta_k t} - E\left( S^+_{t} S^+_{\vartheta_k t} \right) \right), \label{genMta} \\
     M^+_{0,\vartheta_k \vartheta_j   T} &= \sigma^{-2}_0 T^{-1/2} \sum_{t = 1}^T \left( S^+_{t} S^+_{\vartheta_k \vartheta_j  t} - E\left( S^+_{t} S^+_{\vartheta_k \vartheta_j  t} \right) \right), \label{genMtb} \\
     M^+_{\vartheta_k,\vartheta_j   T} &= \sigma^{-2}_0 T^{-1/2} \sum_{t = 1}^T \left( S^+_{\vartheta_k t} S^+_{ \vartheta_j  t} - E\left( S^+_{\vartheta_k t} S^+_{\vartheta_j  t} \right) \right), \label{genMtc} \\
     M^+_{0,\vartheta_k \vartheta_j \vartheta_l  T} &= \sigma^{-2}_0 T^{-1/2} \sum_{t = 1}^T \left( S^+_{t} S^+_{ \vartheta_k \vartheta_j \vartheta_l  t} - E\left( S^+_{t} S^+_{\vartheta_k \vartheta_j \vartheta_l  t} \right) \right), \label{genMtd} \\
     M^+_{\vartheta_k,\vartheta_j \vartheta_l  T} &= \sigma^{-2}_0 T^{-1/2} \sum_{t = 1}^T \left( S^+_{\vartheta_k t} S^+_{ \vartheta_j \vartheta_l  t} - E\left( S^+_{\vartheta_k t} S^+_{\vartheta_j \vartheta_l  t} \right) \right), \label{genMte}
\end{align}
and define some of the corresponding vector forms of the centered product moments as 
\begin{align}
     M^+_{0,\vartheta T} &= (M^+_{0,\vartheta_1  T}, M^+_{0,\vartheta_2  T},\ldots, M^+_{0,\vartheta_{p+1}  T})' ,\label{genM1}\\
     M^+_{0,\vartheta_k \vartheta T} &= ( M^+_{0,\vartheta_k \vartheta_1 T} ,  M^+_{0,\vartheta_k \vartheta_2 T} ,\ldots,  M^+_{0,\vartheta_k \vartheta_{p+1} T} )' ,\label{genvM2}\\
      M^+_{\vartheta_k,\vartheta T} &= ( M^+_{\vartheta_k,\vartheta_1 T} ,   M^+_{\vartheta_k,\vartheta_2 T} ,\ldots,   M^+_{\vartheta_k,\vartheta_{p+1} T} )', \label{genvM3}
\end{align}
and matrix form as 
\begin{align}
 M^+_{0,\vartheta \vartheta' T} &=  ( M^+_{0,\vartheta_1 \vartheta T}, M^+_{0,\vartheta_2 \vartheta T}, \ldots,  M^+_{0,\vartheta_{p+1} \vartheta T}  ) ,\label{genM2}\\
       M^+_{\vartheta,\vartheta' T} &= (  M^+_{\vartheta_1,\vartheta T},  M^+_{\vartheta_2,\vartheta T}, \ldots,  M^+_{\vartheta_{p+1},\vartheta T}  ) .\label{genM3}
\end{align}


By Lemma \ref{explicitforms}, we have that $S_t^+ = \epsilon_t$. As a direct consequence, we can observe that $E\left( S^+_{t} S^+_{\vartheta_k t} \right) =E\left( S^+_{t} S^+_{\vartheta_k \vartheta_j t} \right) = E\left( S^+_{t} S^+_{\vartheta_k \vartheta_j \vartheta_l  t} \right) = 0$.


We next show the limiting behaviour of the centered product moments.
\begin{lemma} \label{genlemmma1}
Suppose that Assumptions \ref{A2}-\ref{A5} hold. Then, for $T\rightarrow \infty$, it holds that $M^+_{0,\vartheta T}$ is asymptotic normal with mean zero and the variance of $M^+_{0,\vartheta T}$ is 
\begin{align*}  
     E\left(M^+_{0,\vartheta T} (M^+_{0,\vartheta T})' \right) = A + O(T^{-1} \log(T)), 
\end{align*}
where $A$ is the inverse of the variance-covariance matrix given in \eqref{genA}.
Furthermore, $M^+_{0,\vartheta_k  T} = O_P(1)$, $M^+_{0,\vartheta_k \vartheta_j   T} = O_P(1)$, $ M^+_{\vartheta_k,\vartheta_j   T} = O_P(1)$, $ M^+_{0,\vartheta_k \vartheta_j \vartheta_l  T} = O_P(1)$ and $M^+_{\vartheta_k,\vartheta_j \vartheta_l  T} = O_P(1)$. 






\end{lemma}
\begin{proof}[Proof of Lemma \ref{genlemmma1}]
The proof of asymptotic normality of $M^+_{0,\vartheta T}$ and the limiting variance is given in \textcite[(2.54) and (2.55)]{hualde2011gaussian}. The order the rest term comes from the (1,1)-th element of the matrix $E\left(M^+_{0,\vartheta T} (M^+_{0,\vartheta T})' \right) $, 
\begin{align*}
    \sigma_0^{-2} T^{-1}  \sum_{t = 1}^T E\left(S^+_{\vartheta_1t}\right)^2 &= T^{-1}  \sum_{t = 1}^T \sum_{k = 1}^{t-1} \frac{1}{k^2} \\
    &= \sum_{k = 1}^{\infty} \frac{1}{k^2} - T^{-1}  \sum_{t = 1}^T \sum_{k = t}^{\infty} \frac{1}{k^2} \\
    &= \zeta_2 + O(T^{-1}\log(T)),
\end{align*}
using $\sum_{k = t}^{\infty} k^{-2} = O(t^{-1})$, see \eqref{lA2}. It can be straightforwardly shown that the other elements in this matrix have a rest term of $O(T^{-1})$. We have that $S_t^+ = \epsilon_t$ and therefore the proof of $M^+_{0,\vartheta_k  T} = O_P(1)$, $M^+_{0,\vartheta_k \vartheta_j   T} = O_P(1)$ and  $M^+_{0,\vartheta_k \vartheta_j  \vartheta_l T} = O_P(1)$ are straightforward and can be derived from Lemmata \ref{genbounds}, \ref{r11} and \ref{r12}. The proofs of $ M^+_{\vartheta_k,\vartheta_j   T} = O_P(1)$ and $M^+_{\vartheta_k,\vartheta_j \vartheta_l  T} = O_P(1)$ are analogous, so we will present the proof for $M^+_{\vartheta_k,\vartheta_j \vartheta_l  T} = O_P(1)$. It is sufficient to show that $E |M^+_{\vartheta_k,\vartheta_j \vartheta_l  T}| = O(1)$. Bounding the right-hand side of \eqref{genMte} by taking the modulus yields 
\begin{align*}
    \sigma^{-2}_0 T^{-1/2} \sum_{t = 1}^T \left|  S^+_{\vartheta_k t} S^+_{ \vartheta_j \vartheta_l  t} - E\left( S^+_{\vartheta_k t} S^+_{\vartheta_j \vartheta_l  t} \right) \right|.
\end{align*}
First,  
\begin{align*}
     S^+_{\vartheta_k t} S^+_{ \vartheta_j \vartheta_l  t} - E\left( S^+_{\vartheta_k t} S^+_{\vartheta_j \vartheta_l  t} \right) = \sum_{n = 1}^{t-1} \sum_{s = 1}^{t-1} v_{\vartheta_k n} w_{ \vartheta_j \vartheta_l  s} \left( \epsilon_{t-n} \epsilon_{t-s} - E \left(\epsilon_{t-n} \epsilon_{t-s} \right)  \right),
\end{align*}
where the expression $v_{\vartheta_k t}$ and $ w_{ \vartheta_j \vartheta_l  t}$ follow directly from Lemma \ref{explicitforms} and from Lemmata \ref{r11} and \ref{r12} it holds that  $|v_{\vartheta_k t}| = O(t^{-\delta_1})$ and $| w_{ \vartheta_j \vartheta_l  t}| = O(t^{-\delta_2})$ for some $\delta_1,\delta_2 > 0$. Then,
\begin{align*}
    E |M^+_{0,\vartheta_k \vartheta_j \vartheta_l  T} | \leq T^{-1/2} \sum_{t = 1}^T E \left| \sum_{n = 1}^{t-1} \sum_{s = 1}^{t-1} v_{\vartheta_k n} w_{ \vartheta_j \vartheta_l  s} \left( \epsilon_{t-n} \epsilon_{t-s} - E \left(\epsilon_{t-n} \epsilon_{t-s} \right)  \right) \right|,
\end{align*}
it follows readily that $\Var \left( \sum_{n = 1}^{t-1} \sum_{s = 1}^{t-1} v_{\vartheta_k n} w_{ \vartheta_j \vartheta_l  s} \epsilon_{t-n} \epsilon_{t-s} \right) = O(t)$, so
\begin{align*}
    |M^+_{0,\vartheta_k \vartheta_j \vartheta_l  T} | = O_P( T^{-1/2} \sum_{t = 1}^T t^{1/2}) = O_P(1).
\end{align*}


\end{proof}







\begin{lemma} \label{genlemmma1_1}
Suppose that Assumptions \ref{A2}-\ref{A5} holds. The covariances of  $M^+_{0,\vartheta_k  T}$ and  $M^+_{0,\vartheta_j \vartheta_l}$ are given by 
\begin{align*}
      E\left(  M^+_{0,\vartheta_k  T}  M^+_{0,\vartheta_j \vartheta_l   T} \right) = \sigma_0^{-2} T^{-1}  \sum_{t = 1}^T E \left( S^+_{\vartheta_k t}   S^+_{\vartheta_j \vartheta_l  t} \right), 
\end{align*}
for $k,j,l \in \{1,\ldots,p+1 \}$. 
For $T\rightarrow \infty$, it holds that 
\begin{align}
     \sigma_0^{-2} T^{-1}\sum_{t = 1}^T  E \left( S_{\vartheta_1 t}^+ S_{\vartheta_1 \vartheta_1 t}^+  \right) &= -2 \zeta_3 + O(T^{-1} \log^4(T) ), \label{l1_1a} \\
     \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_1 t}^+ S_{\vartheta_1 \vartheta_l t}^+  \right) &=    \sum_{i = 2}^{\infty} i^{-1} h_{\vartheta_1 \vartheta_l i}(\varphi_0) + O(T^{-1} \log(T)), \label{l1_1b}\\
     \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_1 t}^+ S_{\vartheta_j \vartheta_l t}^+  \right) &=  -    \sum_{i = 1}^{\infty}  i^{-1} b_{\vartheta_j \vartheta_l i}(\varphi_0) + O( T^{-1}), \label{l1_1c}\\
    \sigma_0^{-2} T^{-1}\sum_{t = 1}^T  E \left( S_{\vartheta_k t}^+ S_{\vartheta_1 \vartheta_1 t}^+  \right) &=  \sum_{i = 0}^{\infty} D_{dd} \pi_{i}(0)  b_{\vartheta_k i}(\varphi_0)  + O(T^{-1}), \label{l1_1d}\\
     \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_k t}^+ S_{\vartheta_1 \vartheta_l t}^+  \right) &= -    \sum_{i = 2}^{\infty}   b_{\vartheta_k i}(\varphi_0) h_{\vartheta_1 \vartheta_l i}(\varphi_0) + O( T^{-1}), \label{l1_1e} \\
     \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_k t}^+ S_{\vartheta_j \vartheta_l t}^+  \right) &=     \sum_{i = 1}^{\infty} b_{\vartheta_k i}(\varphi_0) b_{\vartheta_j \vartheta_l i}(\varphi_0) + O( T^{-1}), \label{l1_1f}
\end{align}
for $k,j,l \in \{2,\ldots,p+1 \}$.

\end{lemma}
\begin{proof}[Proof of Lemma \ref{genlemmma1_1}]
From Lemma \ref{explicitforms} we have that  $S^+_{t} = \epsilon_t$ and $ S^+_{\vartheta_k t}$, $S^+_{\vartheta_j \vartheta_l t}$ are weighted sums of $\epsilon_{1},\ldots,\epsilon_{t-1}$, so that  
\begin{align}
      E\left(  M^+_{0,\vartheta_k  T}  M^+_{0,\vartheta_j \vartheta_l   T} \right) &= 
      \sigma_0^{-4} T^{-1} E \left( \sum_{t = 1}^T  S^+_{t} S^+_{\vartheta_k t} \sum_{s = 1}^T S^+_{s} S^+_{\vartheta_j \vartheta_l  s} \right) \nonumber\\ 
      &=  \sigma_0^{-4} T^{-1}  \sum_{t = 1}^T E \left( \left(S^+_{t} \right)^2  S^+_{\vartheta_k t} S^+_{\vartheta_j \vartheta_l  t} \right) \nonumber \\
      &= \sigma_0^{-2} T^{-1}  \sum_{t = 1}^T E \left( S^+_{\vartheta_k t}   S^+_{\vartheta_j \vartheta_l  t} \right), \label{EXP1} 
\end{align}
where the last inequality uses the independence of $\left(S^+_{t} \right)^2$ and $S^+_{\vartheta_k t} S^+_{\vartheta_j \vartheta_l  t}$. 

Proof of \eqref{l1_1a}: Consider the case $k=j=l = 1$ for \eqref{EXP1}. We have 
\begin{align*}
      \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_1 t}^+ S_{\vartheta_1 \vartheta_1 t}^+  \right) &=  -  \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( \left( \sum_{k = 0}^{t-1} D_d \pi_{k}(0) \epsilon_{t-k} \right) \left( \sum_{k = 0}^{t-1} D_{dd} \pi_{k}(0) \epsilon_{t-k} \right) \right) \\
     &= - T^{-1} \sum_{t = 1}^T \sum_{k = 0}^{t-1} D_{dd} \pi_{k}(0)  D_d \pi_{k}(0) \\
     &= -  \sum_{k = 0}^{\infty} D_{dd} \pi_{k}(0)  D_d \pi_{k}(0) + T^{-1} \sum_{t = 1}^T \sum_{k = t}^{\infty} D_{dd} \pi_{k}(0)  D_d \pi_{k}(0), \\
\end{align*}
Then from \eqref{dpi1} and \eqref{dpi2}
\begin{align*}
    - \sum_{k = 0}^{\infty} D_{dd} \pi_{k}(0)  D_d \pi_{k}(0) &= -2 \sum_{ k = 2}^{\infty} k^{-2} \sum_{j = 1}^{k-1} j^{-1} \\
    &= -2 \zeta_3,
\end{align*}
where the last equality follows from \textcite[Lemma B.2]{johansen2016role}
and from Lemmata \ref{genbounds}  and \ref{r11} we have  
\begin{align*}
     T^{-1} \sum_{t = 1}^T \sum_{k = t}^{\infty} D_{dd} \pi_{k}(0)  D_d \pi_{k}(0) &=  O(T^{-1} \sum_{t = 1}^T \sum_{k = t}^{\infty} (1+\log(k))^3 k^{-2} ) \\
     &=  O(T^{-1} \sum_{t = 1}^T (1+\log(t))^3 t^{-1} ) =  O(T^{-1} \log^4(T) ).
\end{align*}

Proof of \eqref{l1_1b}: Consider the case $k=j =1$ and $l \geq 2$ for \eqref{EXP1}. Then 
\begin{align*}
    \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_1 t}^+ S_{\vartheta_1 \vartheta_l t}^+  \right) &=   \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( \left( \sum_{k = 0}^{t-1} D_d \pi_{k}(0) \epsilon_{t-k} \right) \left( \sum_{i = 2}^{t-1} h_{\vartheta_1 \vartheta_l i}(\varphi_0)  \epsilon_{t-i} \right) \right) \\
    &=    T^{-1} \sum_{t = 1}^T  \sum_{i = 2}^{t-1}  i^{-1}  h_{\vartheta_1 \vartheta_l i}(\varphi_0) \\
    &=     \sum_{i = 2}^{\infty} i^{-1} h_{\vartheta_1 \vartheta_l i}(\varphi_0) - T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}  i^{-1} h_{\vartheta_1 \vartheta_l i}(\varphi_0),
\end{align*}
and 
\begin{align*}
   T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty} i^{-1} h_{\vartheta_1 \vartheta_l i}(\varphi_0) &= O( T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}  i^{-2} ) \\
    &=  O( T^{-1} \sum_{t = 1}^T    t^{-1} ) =  O( T^{-1} \log(T)) .
\end{align*}


Proof of \eqref{l1_1c}: Consider the case $k= 1$ and $j,l \geq 2$ for \eqref{EXP1}. We have 
\begin{align*}
    \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_1 t}^+ S_{\vartheta_j \vartheta_l t}^+  \right) &=  -  \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( \left( \sum_{k = 0}^{t-1} D_d \pi_{k}(0) \epsilon_{t-k} \right) \left(\sum_{i = 1}^{t-1} b_{\vartheta_j \vartheta_l i}(\varphi_0)  \epsilon_{t-i} \right) \right) \\
    &=  -  T^{-1} \sum_{t = 1}^T  \sum_{i = 1}^{t-1}  i^{-1}  b_{\vartheta_k \vartheta_l i}(\varphi_0) \\
    &= -    \sum_{i = 1}^{\infty}  i^{-1} b_{\vartheta_j \vartheta_l i}(\varphi_0) +  T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}   i^{-1} b_{\vartheta_k \vartheta_l i}(\varphi_0) ,
\end{align*}
and 
\begin{align*}
 T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}   i^{-1} b_{\vartheta_j \vartheta_l i}(\varphi_0) &= O( T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}  i^{-2-\varsigma} ) \\
 &= O( T^{-1} \sum_{t = 1}^T    t^{-1-\varsigma} ) = O( T^{-1}) .
\end{align*}

Proof of \eqref{l1_1d}: Consider the case $k \geq 2$ and $j=l=1$ for \eqref{EXP1}. We have 
\begin{align*}
      \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_k t}^+ S_{\vartheta_1 \vartheta_1 t}^+  \right) &=    \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( \left( \sum_{i = 1}^{t-1} b_{\vartheta_k i}(\varphi_0)  \epsilon_{t-i} \right) \left( \sum_{k = 0}^{t-1} D_{dd} \pi_{k}(0) \epsilon_{t-k} \right) \right) \\
     &=  T^{-1} \sum_{t = 1}^T \sum_{i = 0}^{t-1} D_{dd} \pi_{i}(0)  b_{\vartheta_k i}(\varphi_0)  \\
     &= \sum_{i = 0}^{\infty} D_{dd} \pi_{i}(0)  b_{\vartheta_k i}(\varphi_0) - T^{-1} \sum_{t = 1}^T \sum_{i = t}^{\infty} D_{dd} \pi_{i}(0)  b_{\vartheta_k i}(\varphi_0), 
\end{align*}
and 
\begin{align*}
 T^{-1} \sum_{t = 1}^T \sum_{i = t}^{\infty} D_{dd} \pi_{i}(0)  b_{\vartheta_k i}(\varphi_0) &= O(T^{-1} \sum_{t = 1}^T \sum_{i = t}^{\infty} (1+\log(k))^2 i^{-2-\varsigma} )\\
 &= O(T^{-1} \sum_{t = 1}^T (1+\log(t))^2 t^{-1-\varsigma} ) =  O(T^{-1} \sum_{t = 1}^T t^{-1-\varsigma+2\lambda} )  = O(T^{-1} ),
\end{align*}
where we use the bound $1+\log(t) < t^\lambda$ for small $\lambda > 0$. 

Proof of \eqref{l1_1e}: Consider the case $k,l \geq 2$ and $j=1$ for \eqref{EXP1}. We have 
\begin{align*}
    \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_k t}^+ S_{\vartheta_1 \vartheta_l t}^+  \right) &=    -\sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( \left( \sum_{i = 1}^{t-1} b_{\vartheta_k i}(\varphi_0)  \epsilon_{t-i}  \right) \left( \sum_{i = 2}^{t-1} h_{\vartheta_1 \vartheta_l i}(\varphi_0)  \epsilon_{t-i} \right) \right) \\
    &=  -  T^{-1} \sum_{t = 1}^T  \sum_{i = 2}^{t-1}   b_{\vartheta_k i}(\varphi_0) h_{\vartheta_1 \vartheta_l i}(\varphi_0)  \\
    &=  -    \sum_{i = 2}^{\infty}   b_{\vartheta_k i}(\varphi_0) h_{\vartheta_1 \vartheta_l i}(\varphi_0) +  T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}    b_{\vartheta_k i}(\varphi_0) h_{\vartheta_1 \vartheta_l i}(\varphi_0),  
\end{align*}
and 
\begin{align*}
 T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}   b_{\vartheta_k i}(\varphi_0) h_{\vartheta_1 \vartheta_l i}(\varphi_0)   &= O( T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}  i^{-2-  \varsigma} ) \\
 &= O( T^{-1} \sum_{t = 1}^T    t^{-1-\varsigma} ) =   O( T^{-1}). 
\end{align*}

Proof of \eqref{l1_1f}: Consider the case $k,j,l \geq 2$ for \eqref{EXP1}. We have 
\begin{align*}
    \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( S_{\vartheta_k t}^+ S_{\vartheta_j \vartheta_l t}^+  \right) &=   \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E \left( \left( \sum_{i = 1}^{t-1} b_{\vartheta_k i}(\varphi_0)  \epsilon_{t-i}  \right) \left(\sum_{i = 1}^{t-1} b_{\vartheta_j \vartheta_l i}(\varphi_0)  \epsilon_{t-i} \right) \right) \\
    &=   T^{-1} \sum_{t = 1}^T  \sum_{i = 1}^{t-1}  b_{\vartheta_k i}(\varphi_0)   b_{\vartheta_j \vartheta_l i}(\varphi_0) \\
    &= \sum_{i = 1}^{\infty}   b_{\vartheta_k i}(\varphi_0)   b_{\vartheta_j \vartheta_l i}(\varphi_0) -  T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}   b_{\vartheta_k i}(\varphi_0)   b_{\vartheta_j \vartheta_l i}(\varphi_0), 
\end{align*}

and 
\begin{align*}
 T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}   b_{\vartheta_k i}(\varphi_0)   b_{\vartheta_j \vartheta_l i}(\varphi_0) &= O( T^{-1} \sum_{t = 1}^T  \sum_{i = t}^{\infty}  i^{-2-2\varsigma} ) \\
 &= O( T^{-1} \sum_{t = 1}^T    t^{-1-2\varsigma} ) =   O( T^{-1}) .
\end{align*}


\end{proof}



\begin{lemma} \label{genlemma891}
    Suppose that Assumptions \ref{A2}-\ref{A5} holds. The covariances of  $M^+_{0,\vartheta_1  T}$ and $M^+_{\vartheta_1 ,\vartheta_1}$ are given by 
\begin{align}
    E\left(  M^+_{0,\vartheta_1  T}  M^+_{\vartheta_1 ,\vartheta_1   T} \right) &= -4\zeta_3 + O(T^{-1} \log^2(T)), \label{ab11} \\
    E\left(  M^+_{0,\vartheta_1  T}  M^+_{\vartheta_1 ,\vartheta_l   T} \right) &= \sum_{k = 1}^{\infty} k^{-1} \sum_{s = 1}^{\infty} \left(  s^{-1} b_{\vartheta_l (s+k)}(\varphi_0) + (s+k)^{-1} b_{\vartheta_l s}(\varphi_0) \right) \nonumber \\
    &\ \ \ + O(T^{-1}\log(T)), \label{ab21}\\ 
    E\left(  M^+_{0,\vartheta_1  T}  M^+_{\vartheta_n ,\vartheta_l   T} \right) &= -\sum_{k = 1}^{\infty} k^{-1} \sum_{s = 1}^{\infty} \left(  b_{\vartheta_n s}(\varphi_0) b_{\vartheta_l (s+k)}(\varphi_0) + b_{\vartheta_n (s+k)}(\varphi_0) b_{\vartheta_l s}(\varphi_0) \right) \nonumber \\  
    &\ \ \ + O(T^{-1}\log(T)), \label{ab31}\\
    E\left(  M^+_{0,\vartheta_l  T}  M^+_{\vartheta_1 ,\vartheta_1   T} \right) &= 2 \sum^{\infty}_{k = 1} b_{\vartheta_l k}(\varphi_0) \sum_{s = 1}^{\infty} s^{-1} (s+k)^{-1} + O(T^{-1}\log(T)),\label{ab41}\\
    E\left(  M^+_{0,\vartheta_m  T}  M^+_{\vartheta_1 ,\vartheta_l   T} \right) &= -\sum_{k = 1}^{\infty} b_{\vartheta_m k}(\varphi_0)  \sum_{s = 1}^{\infty} \left(  s^{-1} b_{\vartheta_l (s+k)}(\varphi_0) + (s+k)^{-1} b_{\vartheta_l s}(\varphi_0) \right) \nonumber \\ 
    &\ \ \ + O(T^{-1}\log(T)),\label{ab51}\\ 
    E\left(  M^+_{0,\vartheta_m  T}  M^+_{\vartheta_n ,\vartheta_l   T} \right) &= \sum_{k = 1}^{\infty} b_{\vartheta_m k}(\varphi_0)  \sum_{s = 1}^{\infty} \left(  b_{\vartheta_n s}(\varphi_0) b_{\vartheta_l (s+k)}(\varphi_0) + b_{\vartheta_n (s+k)}(\varphi_0) b_{\vartheta_l s}(\varphi_0) \right) \nonumber \\
    &\ \ \ + O(T^{-1}\log(T)), \label{ab61}
\end{align}
for $k,j,l \in \{2,\ldots,p+1 \}$.

\end{lemma}

\begin{proof}[Proof of Lemma \ref{genlemma891}] We have that 
\begin{align*}
      E\left(  M^+_{0,\vartheta_k  T}  M^+_{\vartheta_j ,\vartheta_l   T} \right) =  \sigma^{-4}_0 T^{-1}  E\left(  \sum_{t = 1}^T  S^+_{t} S^+_{\vartheta_k t}  \sum_{s = 1}^T  S^+_{\vartheta_j s} S^+_{ \vartheta_l  s}  \right). 
\end{align*}
The expectation of $S^+_{t} S^+_{\vartheta_k t}  S^+_{\vartheta_j s} S^+_{ \vartheta_l}$ equals zero for $s \leq t$ so that what only matters is 
\begin{align}
\sigma^{-4}_0 T^{-1}  E\left(  \sum_{t = 1}^T  S^+_{t} S^+_{\vartheta_k t}  \sum_{s = t+1}^T  S^+_{\vartheta_j s} S^+_{ \vartheta_l  s}  \right). \label{abgen}
\end{align}
Now we consider the different cases. 

Proof of \eqref{ab11}: Consider the case $k,j,l = 1$ for \eqref{abgen}.  From Lemma \ref{explicitforms}
\begin{align*}
-\sigma^{-4}_0 T^{-1}  E\left(  \sum_{t = 1}^T  \epsilon_t  \sum_{k = 0}^{t-1} D_{d} \pi_{k}(0) \epsilon_{t-k}   \sum_{s = t+1}^T \sum_{n = 0}^{s-1} D_{d} \pi_{n}(0) \epsilon_{s-n} \sum_{a = 0}^{s-1} D_{d} \pi_{a}(0) \epsilon_{s-a}  \right).  
\end{align*}
Only the contributions of the form $\epsilon^2_t \epsilon^2_{t-k}$ are non-zero such that what only matter is if $s-n = t$ and $s-a = t-k$ or if $s-a = t$ and $s-n = t-k$ and since both contributions are equal we get 
\begin{align*}
-2\sigma^{-4}_0 T^{-1}   \sum_{t = 1}^T    \sum_{k = 0}^{t-1}  \sum_{s = t+1}^T  D_{d} \pi_{k}(0) D_{d} \pi_{s-t}(0) D_{d} \pi_{s-t+k}(0)  E \left( \epsilon^2_t \epsilon^2_{t-k}   \right). 
\end{align*}
Plugging in $D_{d} \pi_{k}(0) D_{d} = k^{-1} I(k \geq 1)$ and $D_{dd} \pi_j(0) = 2j^{-1} a_{j-1} I(j \geq 2)$, with $a_j = I(j \geq 1) \sum_{k = 1}^j k^{-1}$, see \eqref{dpi1} and \eqref{dpi2}, yields 
\begin{align*}
-2 T^{-1}   \sum_{t = 1}^T    \sum_{k = 1}^{t-1}   \sum_{s = t+1}^T  k^{-1}  (s-t)^{-1} (s-t+k)^{-1},  
\end{align*}
or, equivalently,
\begin{align*}
    -2 T^{-1}   \sum_{t = 1}^T   
\sum_{s = t+1}^T  \sum_{k = 1}^{t-1}   (t-k)^{-1}  (s-t)^{-1} (s-k)^{-1} ,
\end{align*}
which can be written as 
\begin{align*}
    &-2 T^{-1}   \sum_{t = 1}^T   
\sum_{s = t+1}^T  \sum_{k = -\infty}^{t-1}   (t-k)^{-1}  (s-t)^{-1} (s-k)^{-1} \\ &+ 2T^{-1}   \sum_{t = 1}^T   
\sum_{s = t+1}^T (s-t)^{-1}  \sum_{k = -\infty}^{0}  (t-k)^{-1}  (s-k)^{-1}. 
\end{align*}
For the first term, we have 
\begin{align*}
     -2 T^{-1}   \sum_{t = 1}^T   
\sum_{s = t+1}^T  \sum_{k = -\infty}^{t-1}   (t-k)^{-1}  (s-t)^{-1} (s-k)^{-1}  = -4\zeta_3,
\end{align*}
see \textcite[Lemma B.2]{johansen2016role}. For the second term, we have 
\begin{align*}
    O(T^{-1}   \sum_{t = 1}^T   
\sum_{s = t+1}^T (s-t)^{-1}  \sum_{k = -\infty}^{0}  (t-k)^{-1}  (s-k)^{-1} ) &=  O(T^{-1}   \sum_{t = 1}^T   
\sum_{s = t+1}^T (s-t)^{-1}  \sum_{k = -\infty}^{0}  (s-k)^{-2}  ) \\
&=  O(T^{-1}   \sum_{t = 1}^T   
\sum_{s = t+1}^T (s-t)^{-1}  \sum_{k = 0}^{\infty}  (s+k)^{-2}  ) \\
&=  O(T^{-1}   \sum_{t = 1}^T   
\sum_{s = t+1}^T (s-t)^{-1} s^{-1}  ) \\
&=O(T^{-1} \sum_{t = 1}^T   t^{-1}
\log(T)   ) \\
&= O(T^{-1}  \log^2(T) ). 
\end{align*}






Proof of \eqref{ab21}: Consider the case $k,j = 1$ and $l > 1$ for \eqref{abgen}. From Lemma \ref{explicitforms} 
\begin{align*}
\sigma^{-4}_0 T^{-1}  E\left(  \sum_{t = 1}^T  \epsilon_t  \sum_{k = 0}^{t-1} D_{d} \pi_{k}(0) \epsilon_{t-k}   \sum_{s = t+1}^T \sum_{n = 0}^{s-1} D_{d} \pi_{n}(0) \epsilon_{s-n}\sum_{a = 1}^{s-1} b_{\vartheta_l a}(\varphi_0)  \epsilon_{s-a}  \right) . 
\end{align*}
Only the contributions of the form $\epsilon^2_t \epsilon^2_{t-k}$ are non-zero such that
\begin{align*}
\sigma^{-4}_0 T^{-1}  E\left(  \sum_{t = 1}^T  \epsilon_t  \sum_{k = 0}^{t-1} D_{d} \pi_{k}(0) \epsilon_{t-k}   \sum_{s = t+1}^T \left(  \pi_{s-t}(0) \epsilon_{t} b_{\vartheta_l (s-t+k)}(\varphi_0)  \epsilon_{t-k} + \pi_{s-t+k}(0) \epsilon_{t-k} b_{\vartheta_l (s-t)}(\varphi_0)  \epsilon_{t}, 
\right)   \right) 
\end{align*}
and plugging in the definition $D_{d} \pi_{k}(0)$, see Lemma \ref{explicitforms}, gives 
\begin{align}
T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = t+1}^T \left(  (s-t)^{-1} b_{\vartheta_l (s-t+k)}(\varphi_0)  + (s-t+k)^{-1} b_{\vartheta_l (s-t)}(\varphi_0) \label{abc2} 
\right) . 
\end{align}
The first term in \eqref{abc2} is
\begin{align*}
   T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = t+1}^T (s-t)^{-1} b_{\vartheta_l (s-t+k)}(\varphi_0) &=  T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = 1}^{T-t} s^{-1} b_{\vartheta_l (s+k)}(\varphi_0) \\
   &= \sum_{k = 1}^{\infty} k^{-1} \sum_{s = 1}^{\infty} s^{-1} b_{\vartheta_l (s+k)}(\varphi_0) \\&\ \ \ - T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = T-t+1}^{\infty} s^{-1} b_{\vartheta_l (s+k)}(\varphi_0) \\
   & \ \ \ - T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-1} \sum_{s = 1}^{\infty} s^{-1} b_{\vartheta_l (s+k)}(\varphi_0), 
\end{align*}
where
\begin{align*}
    T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = T-t+1}^{\infty} s^{-1} b_{\vartheta_l (s+k)}(\varphi_0) &= O(T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = T-t+1}^{\infty} s^{-1} (s+k)^{-1-\varsigma}) \\
    &= O(T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} ( T-t+1)^{-1} \sum_{s = 1}^{\infty} (s+k)^{-1-\varsigma}) \\
     &=O(T^{-1}  \sum_{t = 1}^T ( T-t+1)^{-1} \sum_{k = 1}^{t-1} k^{-1-\varsigma} ) \\
     &= O(T^{-1} \log(T)  ), \\
\end{align*}
and 
\begin{align*}
     T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-1} \sum_{s = 1}^{\infty} s^{-1} b_{\vartheta_l (s+k)}(\varphi_0) &= O(  T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-1} \sum_{s = 1}^{\infty} s^{-1} (s+k)^{-1-\varsigma}) \\
     &= O(  T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-1} \sum_{s = 1}^{\infty} s^{-1+\lambda} (s+k)^{-1-\varsigma}) \\
     &= O(  T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-2-\varsigma+\lambda}) \\ 
     &= O(  T^{-1}  \sum_{t = 1}^T   t^{-1-\varsigma+\lambda}) \\
    &= O(  T^{-1}  ),
\end{align*}
with $\lambda > 0$ is small constant. The second term in \eqref{abc2} is
\begin{align*}
T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = t+1}^T (s-t+k)^{-1} b_{\vartheta_l (s-t)}(\varphi_0)    &= T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = 1}^{T-t} (s+k)^{-1} b_{\vartheta_l s}(\varphi_0) \\
&= \sum_{k = 1}^{\infty} k^{-1} \sum_{s = 1}^{\infty} (s+k)^{-1} b_{\vartheta_l s}(\varphi_0) \\ 
& \ \ \ - T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = T-t+1}^{\infty} (s+k)^{-1} b_{\vartheta_l s}(\varphi_0) \\
& \ \ \ - T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-1} \sum_{s = 1}^{\infty} (s+k)^{-1} b_{\vartheta_l s}(\varphi_0),
\end{align*}
where 
\begin{align*}
    T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = T-t+1}^{\infty} (s+k)^{-1} b_{\vartheta_l s}(\varphi_0) &= O( T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} \sum_{s = T-t+1}^{\infty} (s+k)^{-1} s^{-1-\varsigma} ) \\
    &=  O( T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} (T-t+1)^{-1}\sum_{s = T-t+1}^{\infty} (s+k)^{-1} s^{-1+(1-\varsigma + \lambda)} ) \\
    &=  O( T^{-1}  \sum_{t = 1}^T  \sum_{k = 1}^{t-1} k^{-1} (T-t+1)^{-1} k^{-\varsigma + \lambda}) \\
    &= O( T^{-1}  \sum_{t = 1}^T t^{-1} ) \\
    &= O( T^{-1} \log(T)),
\end{align*}
with $\lambda > 0$ is small constant, and 
\begin{align*}
    T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-1} \sum_{s = 1}^{\infty} (s+k)^{-1} b_{\vartheta_l s}(\varphi_0)&= O(   T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-1} \sum_{s = 1}^{\infty} (s+k)^{-1} s^{-1-\varsigma}) \\
    &= O(   T^{-1}  \sum_{t = 1}^T  \sum_{k = t}^{\infty} k^{-2} \sum_{s = 1}^{\infty} s^{-1-\varsigma}) \\
    &= O(   T^{-1}  \sum_{t = 1}^T  t^{-1} ) \\
     &= O(   T^{-1}  \log(T) ). 
\end{align*}

Proof of \eqref{ab31}-\eqref{ab61}: The proof is omitted, as it follows a similar step as in the proof of \eqref{ab21}. 

\end{proof}

\subsubsection{Expectation of the score function}\label{UB3}

The following lemma will be used to calculate the expectation of the score function of $L^*(\vartheta)$. 

\begin{lemma} \label{genlemma88} Suppose that Assumptions \ref{A2}-\ref{A5} holds. Then 
\begin{align*}
   E\left( \sum_{s = 1}^T  c_{s} S_{s}^+ \sum_{t = 1}^T c_{t} S_{\vartheta_l t}^+ \right) = \sigma_0^2 \sum_{t = 1}^T  c_{t} c_{\vartheta_l t},
\end{align*}
for $l \in \{1,\ldots,p+1 \}$. 
\end{lemma}
\begin{proof}[Proof of Lemma \ref{genlemma88}]
We first show the proof for $l = 1$, i.e.\ $\vartheta_1 = d$. From $S_{s}^+= \epsilon_s $ and $S_{d t}^+ =  -\sum_{k = 0}^{t-1} k^{-1}  \epsilon_{t-k} $, see Lemma \ref{explicitforms}, we find 
\begin{align*}
    \sum_{t = 1}^T  c_{t} S_{d t}^+  &= - \sum_{t = 1}^{T-1} \epsilon_t \sum_{k = t + 1}^T  c_{k} \frac{1}{k-t} \\
    &= - \sum_{t = 1}^{T-1} \epsilon_t \sum_{k = 1}^T c_{k} D\pi_{k-t}(u)|_{u = 0} , 
\end{align*}
where  $D\pi_{k-t}(u)|_{u = 0} = (k-t)^{-1} I(k-t \geq 1)$ and hence 
\begin{align*}
E(\sum_{s = 1}^T  c_{s} S_{s}^+ \sum_{t = 1}^T  c_{t} S_{dt}^+) &= - \sigma^2_0 \sum_{t = 1}^{T-1} c_{t}\sum_{k = 1}^T c_{k} D\pi_{k-t}(u)|_{u = 0} \\
&=  - \sigma^2_0 \sum_{k = 1}^T c_{k}  \sum_{t = 1}^{T-1}  c_{t} D\pi_{k-t}(u)|_{u = 0},  \\
\end{align*}
Next, we show that $\sum_{t = 1}^{T-1}  c_{t} D\pi_{k-t}(u)|_{u = 0} = c_{dk}$. From $c_{t}= \sum_{j = 0}^{t-1} \phi_j(\varphi) \kappa_{0(t-j)}(d)$ we find that 
\begin{align*}
     \sum_{t = 1}^{T-1} c_{t} D\pi_{k-t}(u)|_{u = 0} &= \sum_{t = 1}^{T-1} D\pi_{k-t}(u)|_{u = 0} \sum_{j = 0}^{t-1} \phi_j(\varphi) \kappa_{0(t-j)}(d) \\
     &= \sum_{j = 0}^{t-1} \phi_j(\varphi)  \sum_{t = 1}^{T-1} D\pi_{k-t}(u)|_{u = 0}\kappa_{0(t-j)}(d) \\
    &= \sum_{j = 0}^{t-1} \phi_j(\varphi)  \sum_{t = 1}^{T-1} D\pi_{(k-j)-(t-j)}(u)|_{u = 0}\kappa_{0(t-j)}(d) \\
    &= \sum_{j = 0}^{t-1} \phi_j(\varphi)  \sum_{m = 1-j}^{T-1} D\pi_{(k-j)-m}(u)|_{u = 0}\kappa_{0m}(d) \\
    &= \sum_{j = 0}^{t-1} \phi_j(\varphi)  \sum_{m = 1}^{k-j} D\pi_{(k-j)-m}(u)|_{u = 0}\kappa_{0m}(d) \\     
     &= \sum_{j = 0}^{k-1} \phi_j(\varphi)  \kappa_{1(k-j)}(d) \\    
    &=  c_{d k},
\end{align*}
where the second last equality follows from \textcite[Lemma A.4]{johansen2016role}. We next give a proof for $l \in \{2,\ldots,p+1 \}$, i.e.\ $\varphi_{n}$ for $n \in \{1,\ldots,p \}$. From Lemma \ref{explicitforms} it follows
\begin{align*}
   \sum_{t = 1}^T c_{t} S_{\varphi_n t}^+  &=  \sum_{t = 1}^{T} \epsilon_t \sum_{s = t}^{T} c_{s} b_{\varphi_n(s-t)},  
\end{align*}
so that
\begin{align*}
E(  \sum_{t = 1}^T c_{t} S_{\varphi_n t}^+\sum_{s = 1}^T  c_{s} S_{s}^+  ) &=    \sum_{t = 1}^T  c_{t} E( S_{\varphi_n t}^+\sum_{s = 1}^T c_{s} S_{s}^+ )  \\
&=  \sigma^2_0 \sum_{t = 1}^{T} c_{t} \sum_{k = 1}^{t-1} c_k b_{n(t-k)}.   \\
\end{align*}
We need to show that $\sum_{k = 1}^{t-1} c_k b_{n(t-k)} = c_{\varphi_{n} t} $. We find that 
\begin{align*}
    \sum_{k = 1}^{t-1} c_k b_{n(t-k)}  &= \sum_{k = 1}^{t-1} \sum_{j = 1}^{k} \phi_{k-j}(\varphi) \kappa_{0j}(d) \sum_{i = 1}^{t-k} \omega_{t-k-i}(\varphi) D_{\varphi_{n}} \phi_{i}(\varphi) \\
    &=  \sum_{j = 1}^{t-1}  \kappa_{0j}(d) \sum_{i = 1}^{t-1}  D_{\varphi_{n}} \phi_{i}(\varphi) \sum_{k = 1}^{t-1}  \phi_{k-j}(\varphi)  \omega_{t-k-i}(\varphi)\\ 
     &=  \sum_{j = 1}^{t-1}  \kappa_{0j}(d) \sum_{i = 1}^{t-1}  D_{\varphi_{n}} \phi_{i}(\varphi) \sum_{k = j}^{t-i}  \phi_{k-j}(\varphi)  \omega_{t-k-i}(\varphi)\\ 
      &=  \sum_{j = 1}^{t-1}  \kappa_{0j}(d) \sum_{i = 0}^{t-j}  D_{\varphi_{n}} \phi_{i}(\varphi) \sum_{k = 0}^{t-j-i}  \phi_{k}(\varphi)  \omega_{(t-j-i)-k}(\varphi)\\ 
      &= \sum_{j = 1}^{t-1}  \kappa_{0j}(d)  D_{\varphi_{n}} \phi_{t-j}(\varphi),
\end{align*}
since $\sum_{k = 0}^{t-j-i}  \phi_{k}(\varphi)  \omega_{(t-j-i)-k}(\varphi) = 1$ and follows from the identity $\phi(L;\varphi)  \omega(L;\varphi) I(t \leq k) = 1$.


\end{proof}


The following lemma finds the expectation of $D L^*(\vartheta_0) )$  and $L^*(\vartheta_0)$. 


\begin{lemma} \label{genlemmaexpectations}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and let Assumptions \ref{A2}-\ref{A5} be satisfied. Then 
\begin{align}
  E\left( D_{\vartheta_k} L^*(\vartheta_0) \right) &= -\sigma^2_0 \frac{\sum_{t = 1}^T c_{t}(\vartheta_0) c_{\vartheta_k t}(\vartheta_0)}{\sum_{t = 1}^T c^2_{t}(\vartheta_0)}, \label{genEDLmu} \\
   E\left( L^*(\vartheta_0) \right) &=  \sigma^2_0 \frac{T-1}{2},  \label{genELmu}
\end{align}
for $k \in \{1,\ldots,p+1 \}$. 
\end{lemma}
\begin{proof}[Proof of Lemma \ref{genlemmaexpectations}.] The proofs are omitted since it follows straightforwardly from Lemmata \ref{genderivatesLstar} and \ref{genlemma88}.






\end{proof}
\subsection{Approximation of the derivatives} \label{ap3}

In this section, we provide approximations for the first three derivatives of $L^*(\vartheta)$, $L^*_{\mu_0}(\vartheta)$ and $L^*_{m}(\vartheta)$ evaluated at $\vartheta = \vartheta_0$. Before that, we present results that analyse the terms involved in these derivatives. Specifically, we examine the order of magnitude of functions that incorporate the derivatives of the deterministic term $c_{t}(\vartheta)$ and the derivatives of the stochastic term $S^+_t(\vartheta)$, as well as the product moments that contain these terms. This analysis is divided into two parts. In Section \ref{gennon1}, we focus on the non-stationary region, where $d_0 > 1/2$. Then, in Section \ref{genstat1}, we explore the stationary region, where $d_0 < 1/2$. The reason for conducting separate analyses is that the order of magnitude varies depending on the region. Each section concludes with an approximation of the derivatives.

\subsubsection{Non-stationary region}\label{gennon1}

In Lemmata \ref{genlemmaaaa2n} and \ref{genlemmaa99n}, we investigate the order of magnitude of functions involving the deterministic term $c_{t}(\vartheta)$ and its derivatives and the stochastic term $S^+_{t}$ and its derivatives and the product moments containing these. In Lemma \ref{genlemmaa1}, we investigate the order of magnitude involving the modification term $m(\vartheta)$ and derivatives of these. These lemmata are then used to find asymptotic results for the first three derivatives of $L^*$, $L^*_{\mu_0}$ and $L^*_{m}$ in Lemmata \ref{asyappgennon1}, \ref{asyappgennon2}, and \ref{asyappgennon3}, respectively.

\begin{lemma} \label{genlemmaaaa2n} Suppose that Assumptions \ref{A3}-\ref{A5} holds. Let $d > 1/2$, then we have that:
\begin{align}
     \sum_{t = 1}^T c^2_{t}(\vartheta) &= \sum_{t = 1}^{\infty} c^2_{t}(\vartheta) + O(T^{\max(1-2d,-1-2\varsigma) }), \label{2na} \\
    \sum_{t = 1}^T c_{t}(\vartheta) c_{\vartheta_k t}(\vartheta) &= \sum_{t = 1}^{\infty} c_{t}(\vartheta) c_{\vartheta_k t}(\vartheta) \nonumber \\  
    &\ \ \ + O(T^{\max(1-2d,-1-2\varsigma) }  \log(T) I( k = 1) + T^{\max(1-2d,-1-2\varsigma) }   I( k > 1)),\label{2nb} \\
    \sum_{t = 1}^T c_{s t}(\vartheta) c_{i t}(\vartheta) &= O(1), \label{2nc}
\end{align} 
where $s \in \{0,\vartheta_{\tilde{k}},\vartheta_{\tilde{k}} \vartheta_{\tilde{k}}, \vartheta_{\tilde{k}} \vartheta_j,\vartheta_{\tilde{k}} \vartheta_{\tilde{k}} \vartheta_{\tilde{l}} \}$, $i \in \{0,\vartheta_k,\vartheta_k \vartheta_j, \vartheta_k \vartheta_j,\vartheta_k \vartheta_j \vartheta_l \}$ and $\tilde{k},\tilde{j},\tilde{l},k,j,l = 1,\ldots, p+1$. Here, $c_{0t}(\vartheta)$ refers to $c_{t}(\vartheta)$. 

\end{lemma}
\begin{proof}[Proof of Lemma \ref{genlemmaaaa2n}]
Proof of \eqref{2na}: Given that 
\begin{align*}
     \sum_{t = 1}^T c^2_{t}(\vartheta) =  \sum_{t = 1}^{\infty} c^2_{t}(\vartheta) - \sum_{t = T+1}^{\infty} c^2_{t}(\vartheta),
\end{align*}
and using $c_t(\vartheta)  = O(t^{\max(-d,-1-\varsigma)})$, see \eqref{r12_1} in Lemma \ref{r12}, we can deduce that 
\begin{align*}
    \sum_{t = T+1}^{\infty} c^2_{t}(\vartheta) &= O\left(\sum_{t = T+1}^{\infty} t^{\max(-2d,-2-2\varsigma)} \right) \\
    &=  O\left(T^{\max(1-2d,-1-2\varsigma)}\right),
\end{align*}
where the last equality follows from \eqref{lA2} in Lemma \ref{genbounds}. \\
Proof of \eqref{2nb}: Given that
\begin{align*}
    \sum_{t = 1}^T c_{t}(\vartheta) c_{\vartheta_k t}(\vartheta)  = \sum_{t = 1}^{\infty} c_{t}(\vartheta) c_{\vartheta_k t}(\vartheta) - \sum_{t = T+1}^{\infty} c_{t}(\vartheta) c_{\vartheta_k t}(\vartheta). 
\end{align*}
For $\vartheta_1$, using $c_{\vartheta_1 t}(\vartheta)  = O(\log(T) t^{\max(-d,-1-\varsigma)})$, see \eqref{r12_1} in Lemma \ref{r12}, we can deduce that
\begin{align*}
    \sum_{t = T+1}^{\infty} c_{t}(\vartheta) c_{\vartheta_1 t}(\vartheta) &= O\left( \sum_{t = T+1}^{\infty} \log(t) t^{\max(-2d,-2-2\varsigma)} \right) \\
    &= O\left( \log(T) T^{\max(1-2d,-1-2\varsigma)} \right),
\end{align*}
where the last equality follows from \eqref{lA2} in Lemma \ref{genbounds}.
Regarding $\vartheta_s$, $s \geq 2$, it can be shown that $c_{\vartheta_s t}(\vartheta)  = O(t^{\max(-d,-1-\varsigma)})$, see \eqref{r12_2} in Lemma \ref{r12}. The proof follows similarly as in the proof of \eqref{2na}.\\
Proof of \eqref{2nc}: We observe that we can establish an upper bound for $|c_st(\vartheta)|$ as $c \log^3(t) t^{\max(-d,-1-\varsigma)}$, see Lemma \ref{r12}, where $c$ is a generic
arbitrarily large positive constant. Consequently, we proceed to evaluate the summation 
\begin{align*}
     \sum_{t = 1}^T c_{s t}(\vartheta) c_{i t}(\vartheta) &\leq c \sum_{t = 1}^T \log^6(t) t^{\max(-2d,-2-2\varsigma)} \\
     &\leq c \log^6(T) T^{\max(1-2d,-1-2\varsigma)},  
\end{align*}
where the last inequality follows from \eqref{lA2} in Lemma \ref{genbounds}. Since $d$ and $\varsigma$ are both greater than 1/2, this term is $O(1)$.

\end{proof}

\begin{lemma} \label{genlemmaa1} Suppose that Assumptions \ref{A3}-\ref{A5} holds. Let $d > 1/2$, then we have that:
\begin{align}
    m(\vartheta) &= 1 + O(T^{-1}), \label{genab1} \\
    m_{i}(\vartheta) &=   O(T^{-1}), \label{genab2} 
\end{align}
\end{lemma}
where $i \in \{\vartheta_k,\vartheta_k \vartheta_j, \vartheta_k \vartheta_j,\vartheta_k \vartheta_j \vartheta_l \}$ and $k,j,l = 1,\ldots, p+1$.
\begin{proof}[Proof of Lemma \ref{genlemmaa1}] Proof of \eqref{genab1}: The expression for $m(\vartheta)$, as provided in \eqref{genmodificationterm}, can be represented as 
\begin{align*}
    m(\vartheta) =  e^{\frac{1}{T-1} \left( \sum_{t = 1}^T c^2_t(\vartheta)  \right)}.
\end{align*}
By employing the expansion $e^{b} = \sum_{k = 0}^{\infty} \frac{b^k}{k !}$ and considering \eqref{2nc} in Lemma \ref{genlemmaaaa2n}, we have that
\begin{align*}
     m(\vartheta) = 1 + O\left(T^{-1}\right).
\end{align*}
Proof of \eqref{genab2}: The derivatives of $m(\vartheta)$ are given in Lemma \ref{genderivatesLstarMCSS}. Proof follows directly from \eqref{2nc} in Lemma \ref{genlemmaaaa2n}. 
\end{proof}

\begin{lemma} \label{genlemmaa99n}
Suppose that Assumptions \ref{A2}-\ref{A5} hold. Let $d_0 > \frac{1}{2}$. Then
\begin{align} 
    \sum_{t = 1}^T S_{st}^+ c_{it} = O_{P}(1) \label{qwn1} 
\end{align}
where $s \in \{0,\vartheta_{\tilde{k}},\vartheta_{\tilde{k}} \vartheta_{\tilde{k}}, \vartheta_{\tilde{k}} \vartheta_j,\vartheta_{\tilde{k}} \vartheta_{\tilde{k}} \vartheta_{\tilde{l}} \}$, $i \in \{0,\vartheta_k,\vartheta_k \vartheta_j, \vartheta_k \vartheta_j,\vartheta_k \vartheta_j \vartheta_l \}$ and $\tilde{k},\tilde{j},\tilde{l},k,j,l = 1,\ldots, p+1$. Here, $c_{0t}(\vartheta)$ refers to $c_{t}(\vartheta)$ and $S_{0t}^+$ to $S_{t}^+$. 
\end{lemma}



\begin{proof}[Proof of Lemma \ref{genlemmaa99n}] Proof of \eqref{qwn1}: Note that $S_{t}^+(\vartheta_0) = \epsilon_t $, and as a consequence, the results for $s = 0$ directly follow from \eqref{2nc} in Lemma \ref{genlemmaaaa2n}.
Next, we provide a general proof. To begin with, we observe that from Lemma \ref{explicitforms}, $S_{st}^+$ can be expressed as
\begin{align*}
   S_{st}^+ = \sum_{k = 1}^{t-1} v_{st} \epsilon_{t-k}, 
\end{align*}
where the weights $v_{st}$ depend on $s$. From \eqref{r11_1} in Lemma \ref{r11} and \eqref{bproof} in Lemma \ref{r12}, it follows that $|v_{st}| \leq c \log^3(t) t^{-1}$. Also, from the proof of \eqref{2nc}, we have established a bound for $|c_{st}(\vartheta)|$ as $c \log^3(t) t^{\max(-d,-1-\varsigma)}$. 

Firstly, we note that 
\begin{align*}
    \sum_{t = T+1}^{\infty} S_{s t}^+ c_{it}=  \sum_{k = 1}^{\infty} \epsilon_k \sum_{t = \max(T,k)+1}^{\infty}  c_{it} v_{s(t-k)}  
\end{align*}
For small $\delta>0$, we bound $\log^3(t) \leq c t^{\delta}$ and use the bounds $|c_{st}(\vartheta)| \leq c \log^3(t) t^{\max(-d,-1-\zeta)} \leq c t^{\max(-d,-1)+\delta}$ and $|v_{s k}| \leq c \log^3(k) k^{-1} \leq c k^{-1+\delta}$, $t^{\max(-d,-1)+\delta} \leq (t-k)^{-2\delta} k^{\max(-d,-1)+2\delta}$. Then, we obtain
\begin{align*}
    Var\left( \sum_{t = T+1}^{\infty} S_{s t}^+ c_{it} \right) &\leq  c \sum_{k = 1}^{\infty}  \left( \sum_{t = \max(T,k)+1}^{\infty}  (t-k)^{-\delta-1} k^{\max(-d,-1)+2\delta}  \right)^2 \\
    &\leq c \sum_{k = 1}^{\infty} k^{\max(-2d,-2)+4\delta}  \left( \sum_{t = \max(T,k)+1}^{\infty}  (t-k)^{-\delta-1}   \right)^2. 
\end{align*}
Since $\sum_{t = \max(T,k)+1}^{\infty} (t-k)^{-\delta-1} \rightarrow 0$ as $T \rightarrow \infty$ and because $\sum_{k = 1}^{\infty} k^{\max(-2d,-2)+4\delta} < \infty$, we conclude, by the dominated convergence theorem, that this variance converges to zero.
\end{proof}


\begin{lemma} \label{asyappgennon1}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and let Assumptions \ref{A2}-\ref{A5} be satisfied with $d_0 > 1/2$. Then the normalized derivatives of the likelihood function $L^*$, see \eqref{genL1}, satisfy
\begin{align}
    \sigma_0^{-2} T^{-1/2} D_{\vartheta} L^*(\vartheta_0)  &= A_{0} + T^{-1/2} A_{1}, \label{17a} \\
    \sigma_0^{-2} T^{-1} D_{\vartheta \vartheta'} L^*(\vartheta_0)  &= B_{0} + T^{-1/2}  B_{1} + O_P(T^{-1} \log(T) ), \label{17b}\\
    \sigma_0^{-2} T^{-1} D_{\vartheta_i \vartheta \vartheta'} L^*(\vartheta_0)  &= C_{0i} + O_P(T^{-1/2}), \label{17c}
\end{align}
for $i = 1,\ldots,p+1$ and where
\begin{align*}
A_{0} &= M_{0\vartheta}^{+}, \ \ \ \ E(A_{1}) = E(\sigma^{-2}_0 D_{\vartheta} L^*(\vartheta_0) ) = O(1), \\
B_{0} &= A, \ \ \ \ B_1 = M_{\vartheta,\vartheta' T}^{+} + M_{0,\vartheta \vartheta' T}^{+}, 
\end{align*}
Here,
$M_{0\vartheta}^{+}$, $M_{0,\vartheta \vartheta' T}^{+}$ and $M_{\vartheta,\vartheta' T}^{+}$ are given in \eqref{genM1}, \eqref{genM2}, \eqref{genM3} respectively,
and $A$ is the inverse of the variance-covariance matrix given in \eqref{genA}. The expression for $C_{0i}$, $i = 1,\ldots,p+1$, is given in \eqref{genC1} and \eqref{genCk}. 
\end{lemma}



\begin{proof}[Proof of Lemma \ref{asyappgennon1}]  Proof of \eqref{17a}:
From Lemma \ref{genderivatesLstar}, we have that 
\begin{align*}
\sigma_0^{-2} T^{-1/2} D_{\vartheta_k} L^*   &= \sigma_0^{-2} T^{-1/2} \sum_{t = 1}^T S_{t}^+  S_{\vartheta_k t}^+ - \sigma_0^{-2} T^{-1/2} \left(\mu(\vartheta_0)-\mu_0\right) \sum_{t = 1}^T S_{t}^+   c_{\vartheta_k t} \\
     &\ \ \ - \sigma_0^{-2} T^{-1/2} \left(\mu(\vartheta_0)-\mu_0\right)  \sum_{t = 1}^T  S_{\vartheta_k t}^+   c_{t} + \sigma_0^{-2} T^{-1/2} \left(\mu(\vartheta_0)-\mu_0\right)^2  \sum_{t = 1}^T c_{t}  c_{\vartheta_k t} \\
     &=  M_{0\vartheta_k}^{+} + T^{-1/2} A_{1k}, 
\end{align*}
with elements of $A_1$ given by
\begin{align*}
    A_{1}(k) &=  - \sigma_0^{-2}  \left(\mu(\vartheta_0)-\mu_0\right) \sum_{t = 1}^T S_{t}^+   c_{\vartheta_k t} \\
    &\ \ \ - \sigma_0^{-2} \left(\mu(\vartheta_0)-\mu_0\right)  \sum_{t = 1}^T  S_{\vartheta_k t}^+   c_{0t} + \sigma_0^{-2}\left(\mu(\vartheta_0)-\mu_0\right)^2  \sum_{t = 1}^T c_{t}  c_{\vartheta_k t}, 
\end{align*}
since $E( M_{0\vartheta_k}^{+}) = 0$ it follows that $E\left(A_{1}(k)\right) =   E\left(\sigma_0^{-2}  D_{\vartheta_k} L^*\right)$  and from Lemmata \ref{genlemmaexpectations} and \ref{genlemmaaaa2n} we find that $E\left(\sigma_0^{-2}  D_{\vartheta_k} L^*\right) = O(1)$.   



Proof of \eqref{17b}: From Lemma \ref{genderivatesLstar} we have that 
\begin{align*}
   \sigma_0^{-2} T^{-1} D_{\vartheta_k \vartheta_j } L^* &= \sigma_0^{-2} T^{-1} L_{\vartheta_k  \vartheta_j} -  \sigma_0^{-2} T^{-1} \frac{L_{\mu \vartheta_j} L_{\mu \vartheta_k}}{L_{\mu \mu} },
\end{align*}
where  $\sigma_0^{-2} T^{-1} L_{\vartheta_k \mu}\mu_{\vartheta_j}/L_{\mu \mu} = O_P(T^{-1})$ from Lemmata \ref{genlemmaaaa2n} and \ref{genlemmaa99n}. Thus we get 
\begin{align*}
     \sigma_0^{-2} T^{-1} D_{\vartheta_k \vartheta_j } L^* &=  \sigma_0^{-2} T^{-1} \sum_{t = 1}^T \left( S_{\vartheta_j t}^+ -  c_{\vartheta_j t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{\vartheta_k t}^+ -  c_{\vartheta_k t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    &\ \ \ +  \sigma_0^{-2} T^{-1}  \sum_{t = 1}^T  \left(S_{t}^+ - c_{t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{ \vartheta_k  \vartheta_j t}^+ -  c_{\vartheta_k  \vartheta_j t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    &\ \ \ +  O_P(T^{-1} ), 
\end{align*}
ignoring terms that are of order $T^{-1}$ we get
\begin{align*}
 \sigma_0^{-2} T^{-1} D_{\vartheta_k \vartheta_j } L^* &= \sigma_0^{-2} T^{-1} \sum_{t = 1}^T S_{\vartheta_j t}^+ S_{\vartheta_k t}^+ + \sigma_0^{-2} T^{-1} \sum_{t = 1}^T S_{t}^+ S_{ \vartheta_k  \vartheta_j t}^+ +   O_P(T^{-1}) \\
 &= \sigma_0^{-2} T^{-1} \sum_{t = 1}^T E S_{\vartheta_j t}^+ S_{\vartheta_k t}^+ + T^{-1/2} \left( M^+_{\vartheta_j,\vartheta_k T} + M^+_{0,\vartheta_j \vartheta_k T}\right) +  O_P(T^{-1} ).
\end{align*}

We notice that $\sigma_0^{-2} T^{-1} \sum_{t = 1}^T E  \left( S_{\vartheta_j t}^+ S_{\vartheta_k t}^+ \right)  = E \left( M_{0,\vartheta_j} M_{0,\vartheta_k} \right)$ and is already convered in Lemma \ref{genlemmma1}.

Proof of \eqref{17c}: For the third derivative it can be shown from Lemmata \ref{genlemmaaaa2n} and \ref{genlemmaa99n} that the extra terms involving derivatives $\mu_{\vartheta_k}$ and $\mu_{\vartheta_k \vartheta_k}$, see Lemma \ref{genderivatesLstar}, can be ignored and we find
\begin{align*}
    \sigma_0^{-2} T^{-1} D_{\vartheta_k \vartheta_j \vartheta_l} L^*  &= \sigma_0^{-2} T^{-1}  \sum_{t = 1}^T \left( S_{\vartheta_j \vartheta_l t}^+ -  c_{\vartheta_j \vartheta_l t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{\vartheta_k t}^+ -  c_{\vartheta_k t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    & \ \ \ + \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  \left(S_{\vartheta_j }^+ - c_{\vartheta_j t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{ \vartheta_k \vartheta_l t}^+ -  c_{\vartheta_k \vartheta_l t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \\ 
     &\ \ \ + \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  \left(S_{\vartheta_l t}^+ - c_{\vartheta_l t}\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{ \vartheta_k \vartheta_j t}^+ -  c_{\vartheta_k \vartheta_j t}\left(\mu(\vartheta_0)-\mu_0\right)\right)\\
     &\ \ \ + O_P(T^{-1}) \\ 
     &=  \sigma_0^{-2} T^{-1} \sum_{t = 1}^T E\left( S_{\vartheta_j \vartheta_l t}^+ S_{\vartheta_k t}^+ \right) +  \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E\left( S_{\vartheta_j }^+ S_{ \vartheta_k \vartheta_l t}^+ \right) \\ 
     &\ \ \ + \sigma_0^{-2} T^{-1} \sum_{t = 1}^T   E\left( S_{\vartheta_l t}^+ S_{ \vartheta_k \vartheta_j t}^+ \right) \\
     &\ \ \ + T^{-1/2} \left(  M^+_{0,\vartheta_k \vartheta_j \vartheta_l  T}  + M^+_{\vartheta_k ,\vartheta_{j}\vartheta_{l}   T} + M^+_{\vartheta_j ,\vartheta_{l}\vartheta_{k}   T} + M^+_{\vartheta_l ,\vartheta_{j}\vartheta_{k}   T} \right) + O_P(T^{-1}) \\
     &=  \sigma_0^{-2} T^{-1} \sum_{t = 1}^T E\left( S_{\vartheta_j \vartheta_l t}^+ S_{\vartheta_k t}^+ \right) +  \sigma_0^{-2} T^{-1} \sum_{t = 1}^T  E\left( S_{\vartheta_j }^+ S_{ \vartheta_k \vartheta_l t}^+ \right) \\ 
     &\ \ \ + \sigma_0^{-2} T^{-1} \sum_{t = 1}^T   E\left( S_{\vartheta_l t}^+ S_{ \vartheta_k \vartheta_j t}^+ \right) + O_P(T^{-1/2}),
\end{align*}
where the second-to-last equality uses Lemmata \ref{genlemmaaaa2n} and \ref{genlemmaa99n}  and the last equality uses Lemma \ref{genlemmma1}. The terms in this expression are given in Lemma \ref{genlemmma1_1}. In matrix notation, we can therefore  define $C_{0i}$ in \eqref{17c} as follows 
\begin{align}
C_{01} = \begin{pmatrix}
 C_{01}(1,1)  & C_{01}(1,2) \\
C_{01}(2,1) & C_{01}(2,2)
\end{pmatrix},  \label{genC1}
\end{align}
where the elements are given by
\begin{align*}
     C_{01}(1,1) &= -6 \zeta_3, \\
     C_{01}(1,2) &= 2 \sum_{i = 2}^{\infty} i^{-1} h_{d \varphi' i}(\varphi_0) + \sum_{i = 0}^{\infty} D_{dd} \pi_i(0) b_{\varphi'i}(\varphi_0),\\
     C_{01}(2,1) &= 2 \sum_{i = 2}^{\infty} i^{-1} h_{d \varphi i}(\varphi_0) + \sum_{i = 0}^{\infty} D_{dd} \pi_i(0) b_{\varphi i}(\varphi_0),\\
     C_{01}(2,2) &=  -\sum_{i = 1}^{\infty} i^{-1} b_{\varphi \varphi'i}(\varphi_0) -\sum_{i = 2}^{\infty}   b_{\vartheta i}(\varphi_0) h_{d \vartheta' i}(\varphi_0) - \left(\sum_{i = 2}^{\infty}   b_{\vartheta i}(\varphi_0) h_{d \vartheta' i}(\varphi_0)\right)',
\end{align*}
and for $k = 1,\dots, p$ we have that
\begin{align}
C_{0(k+1)} = \begin{pmatrix}
 C_{0(k+1)}(1,1)  & C_{0(k+1)}(1,2) \\
C_{0(k+1)}(2,1) & C_{0(k+1)}(2,2),
\end{pmatrix}  \label{genCk}
\end{align}
where the elements are given by
\begin{align*}
     C_{0(k+1)}(1,1) &= 2 \sum_{i = 2}^{\infty} i^{-1} h_{d \varphi_k i}(\varphi_0)  + \sum_{i = 0}^{\infty} D_{dd} \pi_i(0) b_{\varphi_k i}(\varphi_0),   \\
     C_{0(k+1)}(1,2) &= -\sum_{i = 1}^{\infty} i^{-1} b_{\varphi' \varphi_k i}(\varphi_0) -    \sum_{i = 2}^{\infty}   b_{\varphi_k i}(\varphi_0) h_{d \varphi'  i}(\varphi_0) -  \sum_{i = 2}^{\infty}   b_{\varphi'  i}(\varphi_0) h_{d \varphi_k  i}(\varphi_0),   \\
     C_{0(k+1)}(2,1) &= -\sum_{i = 1}^{\infty} i^{-1} b_{\varphi \varphi_k i}(\varphi_0) -    \sum_{i = 2}^{\infty}   b_{\varphi_k i}(\varphi_0) h_{d \varphi  i}(\varphi_0) -  \sum_{i = 2}^{\infty}   b_{\varphi  i}(\varphi_0) h_{d \varphi_k  i}(\varphi_0),  \\
    C_{0(k+1)}(2,2) &= \left( \sum_{i = 1}^{\infty}  b_{\varphi i}(\varphi_0) b_{\varphi' \varphi_k i}(\varphi_0) \right)' + \sum_{i = 1}^{\infty}  b_{\varphi  i}(\varphi_0) b_{\varphi' \varphi_k i}(\varphi_0)  + \sum_{i = 1}^{\infty}  b_{\varphi_k i}(\varphi_0) b_{\varphi \varphi' i}(\varphi_0).
\end{align*}










  

\end{proof}



















\begin{lemma} \label{asyappgennon2}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and let Assumptions \ref{A2}-\ref{A5} be satisfied with $d_0 > 1/2$. Then the normalized derivatives of the likelihood function $L_{\mu_0}^*$, see \eqref{genlikmu1known}, satisfy
\begin{align}
    \sigma_0^{-2} T^{-1/2} D_{\vartheta} L_{\mu_0}^*(\vartheta_0)  &= A_{0}, \\
    \sigma_0^{-2} T^{-1} D_{\vartheta \vartheta'} L_{\mu_0}^* (\vartheta_0)  &= B_{0} + T^{-1/2}  B_{1} + O_P(T^{-1} \log(T) ), \\
    \sigma_0^{-2} T^{-1} D_{\vartheta_i \vartheta \vartheta'} L^*(\vartheta_0)  &= C_{0i} + O_P(T^{-1/2}),
\end{align}
for $i = 1,\ldots,p+1$ and where
\begin{align*}
A_{0} &= M_{0\vartheta}^{+},\\
B_{0} &= A, \ \ \ \ B_1 = M_{\vartheta,\vartheta' T}^{+} + M_{0,\vartheta \vartheta' T}^{+}, \\
\end{align*}
Here,
$M_{0\vartheta}^{+}$, $M_{0,\vartheta \vartheta' T}^{+}$ and $M_{\vartheta,\vartheta' T}^{+}$ are given in \eqref{genM1}, \eqref{genM2} and \eqref{genM3}, respectively,
and $A$ is the inverse of the variance-covariance marix given in \eqref{genA}. The expression for $C_{0i}$, $i = 1,\ldots,p+1$, is given in \eqref{genC1} and \eqref{genCk}. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{asyappgennon2}]
The proof is omitted and follows from the same approach as in the proof of Lemma  \ref{asyappgennon1} but is much easier since the constant term is known. 
\end{proof}


\begin{lemma} \label{asyappgennon3}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and let Assumptions \ref{A2}-\ref{A5} be satisfied with $d_0 > 1/2$. Then the normalized derivatives of the likelihood function $L_m^*$, see \eqref{genmlik}, satisfy
\begin{align}
    \sigma_0^{-2} T^{-1/2} D_{\vartheta} L_m^*(\vartheta_0)  &= A_{0} + T^{-1/2} A_{1} +O(T^{-1}), \\
    \sigma_0^{-2} T^{-1} D_{\vartheta \vartheta'} L_m^*(\vartheta_0)  &= B_{0} + T^{-1/2}  B_{1} + O_P(T^{-1} \log(T) ), \\
     \sigma_0^{-2} T^{-1} D_{\vartheta_i \vartheta \vartheta'} L^*(\vartheta_0)  &= C_{0i} + O_P(T^{-1/2}), 
\end{align}
for $i = 1,\ldots,p+1$ and where
\begin{align*}
A_{0} &= M_{0\vartheta}^{+}, \ \ \ \ E(A_{1}) = E(\sigma^{-2}_0 D_{\vartheta} L^*(\vartheta_0) ) = 0, \\
B_{0} &= A, \ \ \ \ B_1 = M_{\vartheta,\vartheta' T}^{+} + M_{0,\vartheta \vartheta' T}^{+}, \\
\end{align*}
Here,
$M_{0\vartheta}^{+}$, $M_{0,\vartheta \vartheta' T}^{+}$ and $M_{\vartheta,\vartheta' T}^{+}$ are given in \eqref{genM1}, \eqref{genM2} and \eqref{genM3}, respectively,
and $A$ is the inverse of the variance-covariance matrix given in \eqref{genA}. The expression for $C_{0i}$, $i = 1,\ldots,p+1$, is given in \eqref{genC1} and \eqref{genCk}.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{asyappgennon3}]
The proof is omitted and follows from Lemma \ref{asyappgennon1} and the asymptotic behaviour of the modification term and its derivatives in Lemma \ref{genlemmaa1}. 
\end{proof}

\subsubsection{Stationary region}\label{genstat1}


In Lemmata \ref{genlemmaaaa2s} and \ref{genlemmaa99s}, we investigate the order of magnitude of functions involving the deterministic term $c_{t}(\vartheta)$ and its derivatives and the stochastic term $S^+_{t}$ and its derivatives and the product moments containing these. In Lemma \ref{genlemmaa1stat}, we investigate the order of magnitude involving the modification term $m(\vartheta)$ and derivatives of these. These lemmata are then used to find asymptotic results for the first three derivatives of $L^*$, $L^*_{\mu_0}$ and $L^*_{m}$ in Lemmata \ref{asyappgenstat1}, \ref{asyappgenstat2}, and \ref{asyappgenstat3}, respectively.

\begin{lemma} \label{genlemmaaaa2s} Suppose that Assumptions \ref{A3}-\ref{A5} holds. Let $d < 1/2$, then we have that:
\begin{align}
      \frac{1}{T^{1-2d} }\sum_{t = 1}^T c^2_{t}(\vartheta) &= \phi^2(1;\varphi)       \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa^2_{0t}(d) + o(1), \label{2sa}\\
    \frac{1}{T^{1-2d} }\sum_{t = 1}^T c_{t}(\vartheta) c_{d t}(\vartheta) &= \phi^2(1;\varphi) \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa_{0t}(d) \kappa_{1t}(d)   + o(1),  \label{2sb}\\
    \frac{1}{T^{1-2d} }\sum_{t = 1}^T c_{t}(\vartheta) c_{\varphi_{k} t}(\vartheta) &= \phi(1;\varphi) D_{\varphi_{k}}\phi(1;\varphi)  \frac{1}{T^{1-2d}}  \sum_{t = 1}^T \kappa^2_{0t}(d) + o(1), \label{2sc}\\
        \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa_{0t}(d) \kappa_{1t}(d)  &=  - \left( \log(T) - \Psi(1-d)\right) \frac{1}{T^{1-2d} }  \sum_{t = 1}^T \kappa^2_{0t}(d) \nonumber \\ 
    &\ \ \ + \frac{1}{\Gamma(1-d)^2 (1-2d)^2}  + o(1),   \label{2sc1}\\
     \frac{1}{T^{1-2d}}\sum_{t = 1}^T k^2_{0t}(d) &\rightarrow \frac{1}{\Gamma(1-d)^2 (1-2d)}, \label{2sc2} \\
     \sum_{t = 1}^T c_{t}(\vartheta) c_{\vartheta_k t}(\vartheta) &= O(T^{1-2d} \log(T)), \label{2sd1} \\
     \sum_{t = 1}^T c_{t}(\vartheta) c_{\vartheta_k \vartheta_j t}(\vartheta) &= O(T^{1-2d} \log^2(T)), \label{2sd2} \\
     \sum_{t = 1}^T c_{t}(\vartheta) c_{\vartheta_k \vartheta_j \vartheta_l t}(\vartheta) &= O(T^{1-2d} \log^3(T)), \label{2sd3} \\
      \sum_{t = 1}^T c_{\vartheta_k t}(\vartheta) c_{\vartheta_j t}(\vartheta) &= O(T^{1-2d} \log^2(T)), \label{2sd4} \\
      \sum_{t = 1}^T c_{\vartheta_k t}(\vartheta) c_{\vartheta_j \vartheta_l  t}(\vartheta) &= O(T^{1-2d} \log^3(T)), \label{2sd5}  
\end{align} 
for $k,j,l = 1,\ldots, p+1$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{genlemmaaaa2s}]
Proof of \eqref{2sa}: See \textcite[Lemma S.15]{hualde2020truncated}.

Proof of \eqref{2sb}: 
By summation by parts 
\begin{align*}
    c_t(\vartheta) &=  \sum_{j = 0}^{t-1} \phi_j(\varphi) \kappa_{0(t-j)}(d) = \kappa_{0t}(d) \sum_{j = 0}^{t-1} \phi_j(\varphi) \\
    &\ \ \ - \sum_{j = 0}^{t-2} \left(\kappa_{0(t-j)}(d) - \kappa_{0(t-j-1)}(d) \right)
    \sum_{k = j+1}^{t-1}  \phi_k(\varphi),
\end{align*}
From $\kappa_{0(t-j)}(d) - \kappa_{0(t-j-1)} = \pi_{t-j-1}(1-d) - \pi_{t-j-2}(1-d) = \pi_{t-j-1}(-d)$, see \textcite[Lemma A.4]{johansen2016role}, we have
\begin{align*}
     c_t(\vartheta) &=  \kappa_{0t}(d) \sum_{j = 0}^{\infty} \phi_j(\varphi) -  \kappa_{0t}(d) \sum_{j = t}^{\infty} \phi_j(\varphi) \\
    &\ \ \ - \sum_{j = 0}^{t-2} \pi_{t-j-1}(-d) 
    \sum_{k = j+1}^{t-1}  \phi_k(\varphi),
\end{align*}
Notice that 
\begin{align*}
     \sum_{j = 0}^{t-2} \pi_{t-j-1}(-d) 
    \sum_{k = j+1}^{t-1}  \phi_k(\varphi) = \sum_{j = 1}^{t-1} \pi_j(-d) \sum_{k = 1}^j  \phi_{t-k}(\varphi),
\end{align*}
therefore
\begin{align}
     c_t(\vartheta) &= \kappa_{0t}(d) \sum_{j = 0}^{\infty} \phi_j(\varphi) -  \kappa_{0t}(d) \sum_{j = t}^{\infty} \phi_j(\varphi) \nonumber \\
      &\ \ \ - \sum_{j = 1}^{t-1} \pi_j(-d) \sum_{k = 1}^j  \phi_{t-k}(\varphi), \label{2sbcc}
\end{align}
Taking the derivative of $c_t(\vartheta)$ with respect to $d$ gives
\begin{align*}
    D_{d}   c_t(\vartheta) &= \kappa_{1t}(d) \sum_{j = 0}^{\infty} \phi_j(\varphi) -  \kappa_{1t}(d) \sum_{j = t}^{\infty} \phi_j(\varphi) \\
      &\ \ \ - \sum_{j = 1}^{t-1} D_d \pi_j(-d) \sum_{k = 1}^j  \phi_{t-k}(\varphi).
\end{align*}

The first term of $c_t(\vartheta)$ is bounded by $O(t^{-d})$ from \eqref{r11_1}. The second term is bounded 
by $O(t^{-d-\varsigma})$ from \eqref{r11_1} and \eqref{r11_21} and from the same arguments the third term is bounded by $O\left(\sum_{j = 1}^{t-1}j^{-d-1} \sum_{k = 1}^j  (t-k)^{-1-\varsigma}  \right) = O(t^{-d-\varsigma})$ which also involves employing Lemma \ref{series}. 


The first term of $D_{d} c_t(\vartheta)$ is bounded by $O(\log(t) t^{-d})$ from \eqref{r11_1}. The second term is bounded by $O(\log(t)t^{-d-\varsigma})$ from \eqref{r11_1} and \eqref{r11_21} and from the same arguments the third term is bounded by $O\left(\sum_{j = 1}^{t-1} \log(j) j^{-d-1} \sum_{k = 1}^j  (t-k)^{-1-\varsigma}  \right) = O(\log(t) t^{-d-\varsigma})$ which also involves employing Lemma \ref{series}. 

The leading term of $\sum_{t = 1}^T c_{t}(\vartheta) c_{d t}(\vartheta)$ involves only the first term of $c_t(\vartheta)$ and $D_{d} c_t(\vartheta)$ and the remainder term is bounded by 
\begin{align*}
    O(\sum_{t=1}^T \log(t) t^{-2d-\varsigma} ) = O(\log(T)\sum_{t=1}^T t^{-2d-\varsigma} ) 
\end{align*}
This term is $O(\log(T))$ when $-2d-\varsigma < -1$, $O(\log^2(T))$ when  $-2d-\varsigma = -1$, and $O(\log(T)  T^{1-2d-\varsigma})$ when $-2d-\varsigma > -1$. The proof is now completed. 

Proof of \eqref{2sc}: 
Taking the derivative of $c_t(\vartheta)$ in \eqref{2sbcc} with respect to $\varphi_k$ gives
\begin{align*}
    D_{\varphi_k}   c_t(\vartheta) &= \kappa_{0t}(d) \sum_{j = 0}^{\infty}  D_{\varphi_k}  \phi_j(\varphi) -  \kappa_{0t}(d)  \sum_{j = t}^{\infty} D_{\varphi_k}  \phi_j(\varphi) \\
      &\ \ \ - \sum_{j = 1}^{t-1} \pi_j(-d) \sum_{k = 1}^j  D_{\varphi_k}  \phi_{t-k}(\varphi) .
\end{align*}
The first term of $ D_{\varphi_k}  c_t(\vartheta)$ is bounded by $O( t^{-d})$ from \eqref{r11_1} and \eqref{r11_2}. The second term is bounded by $O(t^{-d-\varsigma})$ from \eqref{r11_1} and \eqref{r11_2} and from the same arguments the third term is bounded by $O\left(\sum_{j = 1}^{t-1}  j^{-d-1} \sum_{k = 1}^j  (t-k)^{-1-\varsigma}  \right) = O( t^{-d-\varsigma})$ which also involves employing Lemma \ref{series}. 
The leading term of $\sum_{t = 1}^T c_{t}(\vartheta) c_{\varphi_k t}(\vartheta)$ involves only the first term of $c_t(\vartheta)$ and $D_{\varphi_k}   c_t(\vartheta)$ and the remainder term is bounded by 
\begin{align*}
    O(\sum_{t=1}^T  t^{-2d-\varsigma} ) = O(\sum_{t=1}^T t^{-2d-\varsigma} ) .
\end{align*}
This term is $O(1)$ when $-2d-\varsigma < -1$, $O(\log(T))$ when  $-2d-\varsigma = -1$, and $O(T^{1-2d-\varsigma})$ when $-2d-\varsigma > -1$. The proof is now completed. 

Proof of \eqref{2sc1}: We note that 
\begin{align*}
    \kappa_{1t}(d) = - \kappa_{0t}(d) \left( \Psi(t-d) - \Psi(1-d) \right),
\end{align*}
then 
\begin{align}
    \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa_{0t}(d) \kappa_{1t}(d)  =  - \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa^2_{0t}(d) \Psi(t-d) +  \Psi(1-d)\frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa^2_{0t}(d), \label{ert1}
\end{align}
We evaluate the first term in \eqref{ert1}. We have that 
\begin{align}
   \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa^2_{0t}(d) \Psi(t-d) &= \Psi(T-d)   \frac{1}{T^{1-2d} }  \sum_{t = 1}^T \kappa^2_{0t}(d) \nonumber \\
   &\ \ \ +   \frac{1}{T^{1-2d} }  \sum_{t = 1}^T k^2_{0t}(d) \left(\Psi(t-d)-\Psi(T-d)\right). \label{ert2}
\end{align}
For a fixed $d$, 
\begin{align*}
    \Psi(t + d) = \log(t) + O(t^{-1}),
\end{align*}
see \textcite[eqn.\ 6.3.18]{abramowitz1964handbook}, hence the second term in \eqref{ert2} is 
\begin{align*}
\frac{1}{T^{1-2d} } \sum_{t = 1}^{T} \kappa^2_{0t}(d) \left(  \Psi(t-d) - \Psi(T-d) \right) &= \frac{1}{T^{1-2d} }\sum_{t = 1}^{T} \kappa^2_{0t}(d) \log(t/T) + o(1). 
\end{align*}
The first term in \eqref{ert2} is  
\begin{align*}
     \Psi(T-d) \frac{1}{T^{1-2d} }  \sum_{t = 1}^T \kappa^2_{0t}(d) = \log(T) \frac{1}{T^{1-2d} }  \sum_{t = 1}^T \kappa^2_{0t}(d)   + o(1).
\end{align*}
Thus we find for \eqref{ert2} that 
\begin{align}
   \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa^2_{0t}(d) \Psi(t-d) &=  \log(T) \frac{1}{T^{1-2d} }  \sum_{t = 1}^T \kappa^2_{0t}(d) \nonumber \\
   &\ \ \ +   \frac{1}{T^{1-2d} }\sum_{t = 1}^{T} \kappa^2_{0t}(d) \log(t/T) + o(1).\label{ert3}
\end{align}
The second term in \eqref{ert3} is 
\begin{align*}
      \frac{1}{T^{1-2d}}\sum_{t = 1}^{T} \kappa^2_{0t}(d) \log(t/T) &= \frac{1}{\Gamma\left(1-d\right)^2} \frac{1}{T^{1-2d}} \sum_{t = 1}^T \log(t/T) t^{-2d} + o(1) \\
     &\rightarrow - \frac{1}{\Gamma(1-d)^2 (1-2d)^2},
\end{align*}
from Stirling's approximation, see \textcite[page 257 6.1.47]{abramowitz1964handbook},
\begin{align}
    \pi_t(d) \sim \frac{1}{\Gamma(d)} t^{d-1} + O(t^{d-2}), \label{stirl}
\end{align}
and the last line follows from \textcite[Lemma S.10]{hualde2020truncated}. Thus \eqref{ert3} equals
\begin{align}
    \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa^2_{0t}(d) \Psi(t-d) &=  \log(T) \frac{1}{T^{1-2d} }  \sum_{t = 1}^T \kappa^2_{0t}(d) \nonumber \\
   &\ \ \ - \frac{1}{\Gamma(1-d)^2 (1-2d)^2} + o(1). \label{ert4}
\end{align}
Plugging in \eqref{ert4} to \eqref{ert1} gives 
\begin{align*}
    \frac{1}{T^{1-2d} } \sum_{t = 1}^T \kappa_{0t}(d) \kappa_{1t}(d)  &=  - \left( \log(T) - \Psi(1-d)\right) \frac{1}{T^{1-2d} }  \sum_{t = 1}^T \kappa^2_{0t}(d) \nonumber \\ 
    &\ \ \ + \frac{1}{\Gamma(1-d)^2 (1-2d)^2}  + o(1), 
\end{align*}
completing the proof.

Proof of \eqref{2sc2}: 

From Stirling's approximation \eqref{stirl}
\begin{align*}
     \frac{1}{T^{1-2d}}\sum_{t = 1}^T k^2_{0t}(d) &= \frac{1}{\Gamma\left(1-d\right)^2} \frac{1}{T^{1-2d}} \sum_{t = 1}^T t^{-2d} + o(1) \\
     &\rightarrow \frac{1}{\Gamma(1-d)^2 (1-2d)},
\end{align*}
and the last line follows from Lemma \ref{series}.\\

Proof of \eqref{2sd1}-\eqref{2sd5}: The proofs can be straightforwardly deduced from the provided bounds in Lemma \ref{r12}, together with the application of Lemma \ref{series}.
\end{proof}

\begin{lemma} \label{genlemmaa1stat} Suppose that Assumptions \ref{A3}-\ref{A5} holds. Let $d < 1/2$, then we have that:
\begin{align}
    m(\vartheta) &= 1 + O(T^{-1} \log(T)), \label{genab1s} \\
    m_{\vartheta_k}(\vartheta) &=  O(T^{-1} \log(T)) \label{genab2s}, \\
     m_{\vartheta_k \vartheta_j}(\vartheta) &=  O(T^{-1}\log^2(T)), \label{genab3s} \\
     m_{\vartheta_k \vartheta_j \vartheta_l}(\vartheta) &=  O(T^{-1}\log^3(T)), \label{genab4s} 
\end{align}
for $k,j,l = 1,\ldots, p+1$.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{genlemmaa1stat}]
Proof of \eqref{genab1s}: 
The expression for $m(\vartheta)$, as provided in \eqref{genmodificationterm}, can be represented as 
\begin{align*}
    m(\vartheta) &= \left( \frac{1}{T^{1-2d}} \sum_{t = 1}^T c^2_t(\vartheta)  \right)^{\frac{1}{T-1}} \left(T^{1-2d}  \right)^{\frac{1}{T-1}}.
\end{align*}
By employing the expansion $e^{b} = \sum_{k = 0}^{\infty} \frac{b^k}{k !}$ and considering \eqref{2sa} in Lemma \ref{genlemmaaaa2s}, we have that
\begin{align*}
    \left( \frac{1}{T^{1-2d}} \sum_{t = 1}^T c^2_t(\vartheta)  \right)^{\frac{1}{T-1}} &= e^{ (T-1)^{-1} \log \left( T^{-1+2d} \sum_{t = 1}^T \kappa^2_{0t}(d) \right) } =   1 + O(T^{-1}).
\end{align*}
From the same expansion we have that 
\begin{align*}
    \left(T^{1-2d}  \right)^{\frac{1}{T-1}} &= e^{ (T-1)^{-1} (1-2d)  \log(T) } \\
                                            &= 1+O(T^{-1}\log(T)).                            
\end{align*}
We conclude that: 
\begin{align*}
    m(\vartheta) = (1+O(T^{-1}))(1+O(T^{-1}\log(T)))  = 1+O(T^{-1}\log(T)).
\end{align*}

Proof of \eqref{genab2s}-\eqref{genab4s}:  The derivatives of $m(\vartheta)$ are given in Lemma \ref{genderivatesLstarMCSS}. Proof follows directly from \eqref{2sd1}-\eqref{2sd5} in Lemma \ref{genlemmaaaa2s}. 










\end{proof}






\begin{lemma} \label{genlemmaa99s}
Suppose that Assumptions \ref{A2}-\ref{A5} hold. Let $d_0 < \frac{1}{2}$. Then
\begin{align} 
    &\sum_{t = 1}^T S_{t}^+ c_t = O_{P}(T^{1/2-d_0}), \ \ \ 
    \sum_{t = 1}^T S_{t}^+ c_{\vartheta_k t} = O_{P}(T^{1/2-d_0} \log(T)), \nonumber\\
    &\sum_{t = 1}^T S_{t}^+(\vartheta) c_{\vartheta_k \vartheta_j t} = O_{P}(T^{1/2-d_0} \log^2(T)), \label{qws1} \\
    & \sum_{t = 1}^T S_{\vartheta_l t}^+ c_t = O_{P}(T^{1/2-d_0} \log(T)),
    \sum_{t = 1}^T S_{\vartheta_l t}^+ c_{\vartheta_k t} = O_{P}(T^{1/2-d_0} \log^{2}(T)), \nonumber\\
    &\sum_{t = 1}^T S_{\vartheta_l t}^+ c_{\vartheta_k \vartheta_j t}= O_{P}(T^{1/2-d_0} \log^{3}(T)), \label{qws2}\\
     &\sum_{t = 1}^T S_{\vartheta_l \vartheta_n t}^+ c_t = O_{P}(T^{1/2-d_0} \log^{2}(T)),
    \sum_{t = 1}^T S_{\vartheta_l \vartheta_n t}^+ c_{\vartheta_k t} = O_{P}(T^{1/2-d_0} \log^{3}(T)), \nonumber \\
    &\sum_{t = 1}^T S_{\vartheta_l \vartheta_n t}^+ c_{\vartheta_k \vartheta_j t} = O_{P}(T^{1/2-d_0} \log^{4}(T)). \label{qws3}
\end{align}
for $l,n,k,j = 0,\ldots,p+1$.
\end{lemma}



\begin{proof}[Proof of Lemma \ref{genlemmaa99s}] Proof of \eqref{qws1}: Note that $S_{t}^+ = \epsilon_t $ such that the results follow from Lemma \ref{genlemmaaaa2s}.\\
Proof of \eqref{qws2}: Due to their similarity and relative simplicity, we exclusively show $\sum_{t = 1}^T S_{\vartheta_l t}^+ c_{\vartheta_z \vartheta_j t} = O_{P}(T^{1/2-d_0} \log^3(T))$, omitting the proofs for $\sum_{t = 1}^T S_{\vartheta_l t}^+ c_t  = O_{P}(T^{1/2-d_0} \log(T))$ and $\sum_{t = 1}^T S_{\vartheta_l t}^+ c_{\vartheta_z t} = O_{P}(T^{1/2-d_0} \log^{2}(T))$. First, consider $l = 1$. By Lemma \ref{explicitforms} 
\begin{align*}
S^{+}_{\vartheta_1 t} = - \sum_{k = 0}^{t-1} D\pi_k(0) \epsilon_{t-k}, 
\end{align*}
resulting in
\begin{align*}
    \sum_{t = 1}^{T} S_{\vartheta_1 t}^+(\vartheta)  c_{\vartheta_z \vartheta_j k} = -\sum_{t = 1}^{T-1} \epsilon_t \sum_{k = t+1}^T c_{\vartheta_z \vartheta_j k}  D\pi_{k-t}(0). 
\end{align*}

Now, we analyse three scenarios: the first case involves $z = 1$ and $j = 1$; the second case involves $z = 1$ and $j>1$; and the third case encompasses $z>1$ and $j > 1$.


\textit{Case I: z = 1 and j = 1.} First let $0 < d_0 < 1/2$. We use the following bounds $|c_{\vartheta_1 \vartheta_1 t}| \leq K t^{-d_0} \log^2(t)$ and $|D\pi_{t}(0)| \leq Kt^{-1} I(t \geq 1)$. Then 
\begin{align*}
    Var(\sum_{t = 1}^{T} S_{\vartheta_1 t}^+ c_{\vartheta_1 \vartheta_1 t}) &\leq K \sum_{t = 1}^{T-1} \left( \sum_{k =t+1}^T \log^2(k)k^{-d_0}  (k-t)^{-1} \right)^2 \\  
    &\leq K \sum_{t = 1}^{T-1} \left( \sum_{k = 1}^{T-t} \log^2(t+k) (t+k)^{-d_0}  k^{-1} \right)^2 \\  
    &\leq K \sum_{t = 1}^{T} t^{-2d_0} \log^4(T) \left( \sum_{k = 1}^{T}   k^{-1} \right)^2 \\ 
    &\leq K T^{1-2d_0} \log^6(T),
\end{align*}
because $\sum_{k = 1}^{T}   t^{-2d} = O(T^{1-2d})$ for $d< 1/2$. 

Second, let $d_0 \leq 0$. Then 
\begin{align*}
    Var(\sum_{t = 1}^{T} S_{\vartheta_1 t}^+ c_{\vartheta_1 \vartheta_1 t}) &\leq K \sum_{t = 1}^{T-1} \left( \sum_{k =t+1}^T \log^2(k)k^{-d_0}  (k-t)^{-1} \right)^2 \\  
    &\leq K \sum_{t = 1}^{T-1} \left( \sum_{k = 1}^{T-t} \log^2(t+k) (t+k)^{-d_0}  k^{-1} \right)^2 \\  
    &\leq K  T^{-2d_0}   \log^4(T) \sum_{t = 1}^{T} \left( \sum_{k = 1}^{T-t}   k^{-1} \right)^2 \\ 
    &\leq K  T^{-2d_0}   \log^4(T) \sum_{t = 1}^{T} \log^2(T-t+1) \\
     &\leq K  T^{1-2d_0}   \log^6(T), \\
\end{align*}
because 
\begin{align*}
     \sum_{t = 1}^{T} \log^2(T-t+1) &= \sum_{t = 1}^{T} \log^2(t) \\
                                &= \sum_{t = 1}^{T} \log^2(t/T) +  \sum_{t = 1}^{T} \log^2(T) \log^2(t)\\
                                &= O(T) + O(T\log^2(T)) \\
                                &= O(T\log^2(T)).
\end{align*}


This shows that $\sum_{t = 1}^{T} S_{\vartheta_1 t}^+(\vartheta)  c_{\vartheta_1 \vartheta_1 k} = O_P(T^{1/2-d_0} \log^3(T)) $.

\textit{Case II: z = 1 and j > 1.} The proof of this case follows in a similar way to that in \textit{Case I} but now with the bound $|c_{\vartheta_1 \vartheta_j t}| \leq K t^{-d_0} \log(t)$. To conclude, $\sum_{t = 1}^{T} S_{\vartheta_1 t}^+(\vartheta)  c_{\vartheta_1 \vartheta_j k} = O_P(T^{1/2-d_0} \log^2(T)) $.

\textit{Case III: z > 1 and j > 1.} The proof of this case follows in a similar way to that in \textit{Case I} but now with the bound $|c_{\vartheta_z \vartheta_j t}| \leq K t^{-d_0}$. To conclude, $\sum_{t = 1}^{T} S_{\vartheta_1 t}^+(\vartheta)  c_{\vartheta_z \vartheta_j k} = O_P(T^{1/2-d_0} \log(T)) $.


Finally, consider $l > 1$. By Lemma \ref{explicitforms} 
\begin{align*}
S^+_{\vartheta_l t} = \sum_{i = 1}^{t-1} b_{\vartheta_li} \epsilon_{t-i}, 
\end{align*}
resulting in
\begin{align*}
    \sum_{t = 1}^{T} S_{\vartheta_l t}^+  c_{\vartheta_z \vartheta_j  t} = \sum_{t = 1}^{T-1} \epsilon_t \sum_{k = t+1}^T c_{\vartheta_z \vartheta_j k}  b_{\vartheta_l(k-t)}.
\end{align*}


Now, we analyse the same three scenarios again: the first case involves $z = 1$ and $j = 1$; the second case involves $z = 1$ and $j>1$; and the third case encompasses $z>1$ and $j > 1$.

\textit{Case I: z = 1 and j = 1.} Given a small $\delta > 0$ to be chosen later, we use the following bounds $|c_{\vartheta_1 \vartheta_1 t}| \leq K t^{-d_0} \log^2(t)$ and $|b_{\vartheta_j t}| \leq Kt^{-1-\varsigma+\delta}$. Then the 
\begin{align*}
      Var(\sum_{t = 1}^{T} S_{\vartheta_l t}^+(\vartheta) c_{\vartheta_1 \vartheta_1 t}(\vartheta)) &\leq K \sum_{t = 1}^{T-1} \left( \sum_{k =t+1}^T \log^2(k) k^{-d_0}  (k-t)^{-1-\varsigma+\delta} \right)^2 \\  
    &\leq K \sum_{t = 1}^{T-1} \left( \sum_{k = 1}^{T-t} \log^2(t+k)(t+k)^{-d_0}  k^{-1-\varsigma+\delta} \right)^2 \\  
    &\leq K \sum_{t = 1}^{T} t^{-2d_0} \log^4(T) \left( \sum_{k = 1}^{T}   k^{-1-\varsigma+\delta} \right)^2 \\ 
    &\leq K  T^{1-2d_0} \log^4(T),
\end{align*}
because $\sum_{k = 1}^{T}   k^{-1-\varsigma+\delta} = O(1)$ since $-\varsigma+\delta < 0$ from choosing $\delta$ to satisfy $0< \delta < \varsigma < 1/2$. This show that $\sum_{t = 1}^{T} S_{\vartheta_l t}^+ c_{\vartheta_1 \vartheta_1 t} = O_P( T^{1/2-d_0} \log^2(T))$.

\textit{Case II: z = 1 and j > 1.} The proof of this case follows in a similar way to that in \textit{Case I} but now with the bound $|c_{\vartheta_1 \vartheta_j t}| \leq K t^{-d_0} \log(t)$. To conclude that $\sum_{t = 1}^{T} S_{\vartheta_l t}^+(\vartheta)  c_{\vartheta_1 \vartheta_j k} = O_P(T^{1/2-d_0} \log(T)) $.

\textit{Case III: z > 1 and j > 1.} The proof of this case follows in a similar way to that in \textit{Case I} but now with the bound $|c_{\vartheta_z \vartheta_j t}| \leq K t^{-d_0}$. To conclude that $\sum_{t = 1}^{T} S_{\vartheta_l t}^+  c_{\vartheta_z \vartheta_j k} = O_P(T^{1/2-d_0}) $.

Proof of \eqref{qws3}: The proofs follow from similar arguments as in the proof of \eqref{qws2} and are therefore omitted.


\end{proof}



\begin{lemma} \label{asyappgenstat1}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and let Assumptions \ref{A2}-\ref{A5} be satisfied with $d_0 < 1/2$. Then the normalized derivatives of the likelihood function $L^*$, see \eqref{genL1}, satisfy
\begin{align}
    \sigma_0^{-2} T^{-1/2} D_{\vartheta} L^*(\vartheta_0)  &= A_{0} + T^{-1/2} A_{1}, \label{23a}\\
    \sigma_0^{-2} T^{-1} D_{\vartheta \vartheta'} L^*(\vartheta_0)  &= B_{0} + T^{-1/2}  B_{1} + O_P(T^{-1} \log^2(T) ), \label{23b}\\
     \sigma_0^{-2} T^{-1} D_{\vartheta_i \vartheta \vartheta'} L^*(\vartheta_0)  &= C_{0i} + O_P(T^{-1/2}), \label{23c} 
\end{align}
for $i = 1,\ldots,p+1$ and where
\begin{align*}
A_{0} &= M_{0\vartheta}^{+}, \ \ \ \ E(A_{1}) = E(\sigma^{-2}_0 D_{\vartheta} L^*(\vartheta_0) ) = O(\log(T)), \\
B_{0} &= A, \ \ \ \ B_1 = M_{\vartheta,\vartheta' T}^{+} + M_{0,\vartheta \vartheta' T}^{+}, \\
\end{align*}
Here,
$M_{0\vartheta}^{+}$, $M_{0,\vartheta \vartheta' T}^{+}$ and $M_{\vartheta,\vartheta' T}^{+}$ are given in \eqref{genM1}, \eqref{genM2} and \eqref{genM3}, respectively,
and $A$ is the inverse of the variance-covariance matrix given in \eqref{genA}.The expression for $C_{0i}$, $i = 1,\ldots,p+1$, is given in \eqref{genC1} and \eqref{genCk}. 
\end{lemma}



\begin{proof}[Proof of Lemma \ref{asyappgenstat1}] Proof of \eqref{23a}:
From Lemma \ref{genderivatesLstar}, we have that 
\begin{align*}
\sigma_0^{-2} T^{-1/2} D_{\vartheta_k} L^*   &= \sigma_0^{-2} T^{-1/2} \sum_{t = 1}^T S_{t}^+  S_{\vartheta_k t}^+ - \sigma_0^{-2} T^{-1/2} \left(\mu(\vartheta_0)-\mu_0\right) \sum_{t = 1}^T S_{t}^+   c_{\vartheta_k t} \\
     & \ \ \ - \sigma_0^{-2} T^{-1/2} \left(\mu(\vartheta_0)-\mu_0\right)  \sum_{t = 1}^T  S_{\vartheta_k t}^+   c_{t} + \sigma_0^{-2} T^{-1/2} \left(\mu(\vartheta_0)-\mu_0\right)^2  \sum_{t = 1}^T c_{t}  c_{\vartheta_k t} \\
     &=  M_{0\vartheta_k}^{+} + T^{-1/2} A_1,
\end{align*}
with elements of $A_1$ given by
\begin{align*}
    A_{1}(k) &=  - \sigma_0^{-2}  \left(\mu(\vartheta_0)-\mu_0\right) \sum_{t = 1}^T S_{t}^+   c_{\vartheta_k t} \\
    &\ \ \ - \sigma_0^{-2} \left(\mu(\vartheta_0)-\mu_0\right)  \sum_{t = 1}^T  S_{\vartheta_k t}^+   c_{0t} + \sigma_0^{-2}\left(\mu(\vartheta_0)-\mu_0\right)^2  \sum_{t = 1}^T c_{t}  c_{\vartheta_k t}, 
\end{align*}
since $E( M_{0\vartheta_k}^{+}) = 0$ it follows that $E\left(A_{1}(k)\right) =   E\left(\sigma_0^{-2}  D_{\vartheta_k} L^*\right)$ and from Lemmata \ref{genlemmaexpectations} and \ref{genlemmaaaa2s} we find that $E(A) = O(\log(T))$.   



Proof of \eqref{23b}: From Lemma \ref{genderivatesLstar} we have that 
\begin{align*}
   \sigma_0^{-2} T^{-1} D_{\vartheta_k \vartheta_j } L^* &= \sigma_0^{-2} T^{-1} L_{\vartheta_k  \vartheta_j} -  \sigma_0^{-2} T^{-1} \frac{L_{\mu \vartheta_j} L_{\mu \vartheta_k}}{L_{\mu \mu} },
\end{align*}
where  $\sigma_0^{-2} T^{-1} L_{\vartheta_k \mu}\mu_{\vartheta_j}/L_{\mu \mu} = O_P(T^{-1}\log^2(T)$ from Lemmata \ref{genlemmaaaa2s} and \ref{genlemmaa99s}. Thus we get 
\begin{align*}
     \sigma_0^{-2} T^{-1} D_{\vartheta_k \vartheta_j } L^* &=  \sigma_0^{-2} T^{-1} \sum_{t = 1}^T \left( S_{\vartheta_j t}^+ -  c_{\vartheta_j t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{\vartheta_k t}^+ -  c_{\vartheta_k t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    &\ \ \ +  \sigma_0^{-2} T^{-1}  \sum_{t = 1}^T  \left(S_{t}^+ - c_{t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \left( S_{ \vartheta_k  \vartheta_j t}^+ -  c_{\vartheta_k  \vartheta_j t}(\vartheta_0)\left(\mu(\vartheta_0)-\mu_0\right)\right) \\
    &\ \ \ +  O_P(T^{-1} \log^2(T) ), \\
\end{align*}
ignoring terms that are of order $T^{-1} \log^2(T)$ we get
\begin{align*}
 \sigma_0^{-2} T^{-1} D_{\vartheta_k \vartheta_j } L^* &= \sigma_0^{-2} T^{-1} \sum_{t = 1}^T S_{\vartheta_j t}^+ S_{\vartheta_k t}^+ + \sigma_0^{-2} T^{-1} \sum_{t = 1}^T S_{t}^+ S_{ \vartheta_k  \vartheta_j t}^+ +   O_P(T^{-1}) \\
 &= \sigma_0^{-2} T^{-1} \sum_{t = 1}^T E S_{\vartheta_j t}^+ S_{\vartheta_k t}^+ + T^{-1/2} \left( M^+_{\vartheta_j,\vartheta_k T} + M^+_{0,\vartheta_j \vartheta_k T}\right) +  O_P(T^{-1} \log^2(T)).
\end{align*}

We notice that $\sigma_0^{-2} T^{-1} \sum_{t = 1}^T E  \left( S_{\vartheta_j t}^+ S_{\vartheta_k t}^+ \right)  = E \left( M_{0,\vartheta_j} M_{0,\vartheta_k} \right)$ and is already convered in Lemma \ref{genlemmma1}.

Proof of \eqref{23c}: The proof strategy closely resembles that used in the derivation of \eqref{17c} in Lemma \ref{asyappgennon1} and is thus omitted. It is worth noting that, for this approximation, the terms provided in Lemmata \ref{genlemmaaaa2s} and \ref{genlemmaa99s} can be considered negligible.





  

\end{proof}



















\begin{lemma} \label{asyappgenstat2}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and let Assumptions \ref{A2}-\ref{A5} be satisfied with $d_0 < 1/2$. Then the normalized derivatives of the likelihood function $L_{\mu_0}^*$, see \eqref{genlikmu1known}, satisfy
\begin{align}
    \sigma_0^{-2} T^{-1/2} D_{\vartheta} L_{\mu_0}^*(\vartheta_0)  &= A_{0}, \\
    \sigma_0^{-2} T^{-1} D_{\vartheta \vartheta'} L_{\mu_0}^* (\vartheta_0)  &= B_{0} + T^{-1/2}  B_{1} + O_P(T^{-1} \log(T) ), \\
     \sigma_0^{-2} T^{-1} D_{\vartheta_i \vartheta \vartheta'} L^*(\vartheta_0)  &= C_{0i} + O_P(T^{-1/2}), 
\end{align}
for $i = 1,\ldots,p+1$ and where
\begin{align*}
A_{0} &= M_{0\vartheta}^{+},\\
B_{0} &= A, \ \ \ \ B_1 = M_{\vartheta,\vartheta' T}^{+} + M_{0,\vartheta \vartheta' T}^{+}, \\
\end{align*}
Here,
$M_{0\vartheta}^{+}$, $M_{0,\vartheta \vartheta' T}^{+}$ and $M_{\vartheta,\vartheta' T}^{+}$ are given in \eqref{genM1}, \eqref{genM2} and \eqref{genM3}, respectively,
and $A$ is the inverse of the variance-covariance marix given in \eqref{genA}. The expression for $C_{0i}$, $i = 1,\ldots,p+1$, is given in \eqref{genC1} and \eqref{genCk}. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{asyappgenstat2}]
The proof is omitted and follows from the same approach as in the proof of Lemma  \ref{asyappgenstat1} but is much easier since the constant term is known. 
\end{proof}


\begin{lemma} \label{asyappgenstat3}
Let the model for the data $x_t$, t = 1,$\ldots$,T, be given by \eqref{genq1} and let Assumptions \ref{A2}-\ref{A5} be satisfied with $d_0 < 1/2$. Then the normalized derivatives of the likelihood function $L_m^*$, see \eqref{genmlik}, satisfy
\begin{align}
    \sigma_0^{-2} T^{-1/2} D_{\vartheta} L_m^*(\vartheta_0)  &= A_{0} + T^{-1/2} A_{1} +O_P(T^{-1} \log(T)), \\
    \sigma_0^{-2} T^{-1} D_{\vartheta \vartheta'} L_m^*(\vartheta_0)  &= B_{0} + T^{-1/2}  B_{1} + O_P(T^{-1} \log^2(T) ), \\
     \sigma_0^{-2} T^{-1} D_{\vartheta_i \vartheta \vartheta'} L^*(\vartheta_0)  &= C_{0i} + O_P(T^{-1/2}), 
\end{align}
for $i = 1,\ldots,p+1$ and where
\begin{align*}
A_{0} &= M_{0\vartheta}^{+}, \ \ \ \ E(A_{1}) = E(\sigma^{-2}_0 D_{\vartheta} L^*(\vartheta_0) ) = 0, \\
B_{0} &= A, \ \ \ \ B_1 = M_{\vartheta,\vartheta' T}^{+} + M_{0,\vartheta \vartheta' T}^{+}, \\
\end{align*}
Here,
$M_{0\vartheta}^{+}$, $M_{0,\vartheta \vartheta' T}^{+}$ and $M_{\vartheta,\vartheta' T}^{+}$ are given in \eqref{genM1}, \eqref{genM2} and \eqref{genM3}, respectively,
and $A$ is the inverse of the variance-covariance matrix given in \eqref{genA}. The expression for $C_{0i}$, $i = 1,\ldots,p+1$, is given in \eqref{genC1} and \eqref{genCk}. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{asyappgenstat3}]
The proof is omitted and follows from Lemma  \ref{asyappgenstat1} and the asymptotic behaviour of the modification term and its derivatives in Lemma \ref{genlemmaa1stat}. 
\end{proof}





\subsection{Proof of the main results} \label{generalizationsappendix}

In this section, we provide the proofs for the main results presented in Section \ref{secgen}.









\subsubsection{Proof of Theorem \ref{t52}} \label{proofthm52}


The proof for the CSS score follows directly from Lemmata \ref{genlemmaexpectations}, \ref{genlemmaaaa2n} and \ref{genlemmaaaa2s}. The proof for the CSS score with known $\mu_0$ follows directly from Lemma \ref{genderivatesLstar} and from $E( S_{t}^+ S_{\vartheta_k t}^+) = 0$. 





\subsubsection{Proof of Lemma \ref{gen:lm:md}} \label{lemma31}

The first property is readily established since 
\begin{align*}
    \sum_{t = 1}^T c^2_t(\vartheta) \geq c^2_1 = \phi_0(\varphi) \kappa_{01}(d) = 1,
\end{align*}
from $\kappa_{01}(d)  = 1$ and $\phi_0(\varphi) = 1$.

In a special case where the short-run dynamics $\varphi = 0$, we have $c_t(\vartheta) = \kappa_{0t}(d)$. Then
\begin{align*}
    \sum_{t = 1}^T  \kappa^2_{0t} \leq \kappa^2_{01} + \kappa^2_{02} = 1 + (1-d)^2,  
\end{align*}
because  $\kappa_{01} = 1$ for all $d$ and $\kappa_{02} = \pi_1(1-d) = 1-d$ for all $d$, see \textcite[Lemma A.4]{johansen2016role}. Thus $\kappa_{02} = 0$ only if $d = 1$ and from the recursive relationship $\pi_j(a) = \frac{j-1+a}{j} \pi_{j-1}(a)$ for $j \geq 1$ and for all $a$, see for instance p.96 in \textcite{hassler2019time}, it follows that $\kappa_{0n} = 0$ for all $n \geq 2$ when $d = 1$. Thus $m(d,\varphi) = 1$ if $d = 1$ and $\varphi = 0$.

The proof of the second property is given in Lemmata \ref{genlemmaa1} and \ref{genlemmaa1stat}.


\subsubsection{Proof of Theorem \ref{t51}} \label{proofthm51}

We note that the MCSS estimator is equal to 
\begin{align*}
    \hat{\vartheta}_{m} &= \operatorname*{argmin}_{\vartheta \in \Theta}  L^*_{m}(\vartheta) \\
                   &= \operatorname*{argmin}_{\vartheta \in \Theta}  \log \left(m(\vartheta)\frac{2}{T}L^*(\vartheta)\right),
\end{align*}
so that the objective function equals $\tilde{L}(\vartheta) = \log \left(m(\vartheta)\frac{2}{T}L^*(\vartheta)\right) = \log(m(\vartheta)) + \log\left(\frac{2}{T}L^*(\vartheta)\right)$. We also note that $R(\vartheta) = \frac{2}{T}L^*(\vartheta) $ is the same objective function as in \textcite{hualde2020truncated}. 
Fix $\epsilon > 0$ and let $M_{\epsilon} = \{\vartheta \in \Theta: |\vartheta-\vartheta_0| < \epsilon \}$
and $\bar{M}_{\epsilon} = \{\vartheta \in \Theta: |\vartheta-\vartheta_0| \geq \epsilon \}$. Then 
\begin{align*}
    \text{Pr}\left(\hat{\vartheta}_{m} \in \bar{M} \right) &= \text{Pr}\left(\inf_{\vartheta \in \bar{M}_{\epsilon}}\tilde{L}(\vartheta) \leq \inf_{\vartheta \in M_{\epsilon}} \tilde{L}(\vartheta) \right), \\ 
    &\leq \text{Pr}\left(\inf_{\vartheta \in \bar{M}_{\epsilon}}\tilde{L}(\vartheta) \leq  \tilde{L}(\vartheta_0) \right), \\
    &\leq \text{Pr}\left(\inf_{\vartheta \in \bar{M}_{\epsilon}} \log(R(\vartheta))-\log(R(\vartheta_0)) \leq  \log(m(\vartheta_0))- \inf_{\vartheta \in \Theta} \log(m(\vartheta)) \right), \\
\end{align*}
From \textcite{hualde2020truncated}, as $T \rightarrow \infty$, we have that  
\begin{align*}
    \text{Pr}\left(\inf_{\vartheta \in \bar{M}_{\epsilon}} \log(R(\vartheta))-\log(R(\vartheta_0)) \leq 0 \right) \rightarrow 0.
\end{align*}
So to prove consistency, it remains to show that  
\begin{align}
    \log(m(\vartheta_0))- \inf_{\vartheta \in \Theta} \log(m(\vartheta)) \rightarrow 0, \label{consistency}
\end{align}
which is already established in Lemmata \ref{genlemmaaaa2n} and \ref{genlemmaaaa2s}.



To show the asymptotic normality of $\hat{\vartheta}_{m}$, we proceed with a usual Taylor expansion of the score function, 
\begin{align*}
    0 =  D_{\vartheta}L_{m}^*(\hat{\vartheta}_{m}) =  D_{\vartheta} L_{m}^*(\vartheta_0) + \left(\hat{\vartheta}_{m}-\vartheta_0\right)  D_{\vartheta \vartheta'} L_{m}^*(\vartheta^*),
\end{align*}
where $\vartheta^*$ is an intermediate value satisfying $|\vartheta^*-\vartheta_0| \leq |\hat{\vartheta}_{m} - \vartheta_0| \overset{p}{\rightarrow} 0$. The product moments within $D_{\vartheta \vartheta'}L^*(\vartheta)$ have been demonstrated in \textcite[Lemma C.4]{johansen2010likelihood} and \textcite[ Lemma A.8(i)]{johansen2012necessary} to exhibit tightness or equicontinuity in a neighborhood of $\vartheta_0$. This allows us to apply \textcite[Lemma A.3]{johansen2010likelihood} and conclude that $D_{\vartheta \vartheta'} L_{m}^*(\vartheta^*) = D_{\vartheta \vartheta'}L_{m}^*(\vartheta_0) + o_P(1)$. Consequently, we proceed to analyse $D_{\vartheta} L_{m}^*(\vartheta_0)$ and $D_{\vartheta \vartheta'}L_{m}^*(\vartheta_0)$. According to Lemmata \ref{asyappgennon3} and \ref{asyappgenstat3}, we find that $ \sigma^2_0 T^{-1/2}D_{\vartheta} L_{m}^*(\vartheta_0) =  M_{0\vartheta}^{+} + O_P(T^{-1/2}\log(T))$ and $ \sigma^2_0 T^{-1}D_{\vartheta \vartheta'}L_{m}^*(\vartheta_0) = A + O_P(T^{-1/2})$ so that the final result follows from Lemma \ref{genlemmma1}.  






\subsubsection{Proof of Theorem \ref{t53}} \label{proofthm53}

First, we consider the bias of $\hat{\vartheta}$. A Taylor series expansion of $D_{\vartheta}L^*(\hat{\vartheta}) = 0$ around $\vartheta_0$ gives 
\begin{align*}
0 = D_{\vartheta}L^*(\hat{\vartheta}) = D_{\vartheta}L^*(\vartheta_0)   + D_{\vartheta \vartheta'}L^*(\vartheta_0) (\hat{\vartheta}-\vartheta_0) + \frac{1}{2} \begin{bmatrix}
(\hat{\vartheta}-\vartheta_0)' D_{\vartheta_1 \vartheta \vartheta'}L(\vartheta^*) (\hat{\vartheta}-\vartheta_0) \\
\vdots  \\
(\hat{\vartheta}-\vartheta_0)' D_{\vartheta_{p+1} \vartheta \vartheta'}L(\vartheta^*) (\hat{\vartheta}-\vartheta_0) 
\end{bmatrix},
\end{align*}
where $\vartheta^*$ is an intermediate value which is allowed to vary across the different rows of $D_{\vartheta_i \vartheta \vartheta'}L(\vartheta^*) $ for $i = 1,\ldots,p+1$ and satisfies $|\vartheta^* - \vartheta_0| \leq |\hat{\vartheta} - \vartheta_0| \xrightarrow[]{p} 0$. We then insert $\hat{\vartheta}-\vartheta_0 = T^{-1/2} G_{1T} + T^{-1} G_{2T} + O_p(T^{-3/2}) $ and find
\begin{align*}
    G_{1T} &= - T^{1/2} (D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1}  D_{\vartheta}L^*(\vartheta_0), \\
    G_{2T} &= - \frac{1}{2} T (D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} \begin{bmatrix}
D_{\vartheta}L^*(\vartheta_0)' (D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} D_{\vartheta_1 \vartheta \vartheta'}L(\vartheta^*) (D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} D_{\vartheta}L^*(\vartheta_0) \\
\vdots  \\
D_{\vartheta}L^*(\vartheta_0)' (D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} D_{\vartheta_{p+1} \vartheta \vartheta'}L(\vartheta^*) (D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} D_{\vartheta}L^*(\vartheta_0)
\end{bmatrix} ,
\end{align*}
which we write as 
\begin{align}
    T^{1/2} (\hat{\vartheta}-\vartheta_0) &= - (T^{-1} D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} T^{-1/2}D_{\vartheta}L^*(\vartheta_0) - \frac{1}{2} T^{-1/2} (T^{-1} D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1}  \nonumber \\
    &\begin{bmatrix}
T^{-1/2}D_{\vartheta}L^*(\vartheta_0)' ( T^{-1} D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} T^{-1} D_{\vartheta_1 \vartheta \vartheta'}L(\vartheta^*) (T^{-1} D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} T^{-1/2} D_{\vartheta}L^*(\vartheta_0) \\
\vdots  \\
T^{-1/2} D_{\vartheta}L^*(\vartheta_0)' (T^{-1} D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} T^{-1} D_{\vartheta_{p+1} \vartheta \vartheta'}L(\vartheta^*) (T^{-1} D_{\vartheta \vartheta'}L^*(\vartheta_0))^{-1} T^{-1/2}D_{\vartheta}L^*(\vartheta_0)
\end{bmatrix} \nonumber\\
&\ \ \ + o_P(T^{-1/2}).  \label{genexpension}
\end{align}


First we note that, as in Appendix \ref{proofthm51}, we can apply \textcite[Lemma A.3]{johansen2010likelihood} to conclude that $ D_{\vartheta_i \vartheta \vartheta'}L(\vartheta^*) =  D_{\vartheta_i \vartheta \vartheta'}L(\vartheta_0) + o_P(1)$ for $i = 1,\ldots, p+1$. Consequently, we plug in the derivatives in Lemma \ref{asyappgennon1} and \ref{asyappgenstat1} into the expansion \eqref{genexpension} and find
\begin{align}
    T^{1/2} (\hat{\vartheta}-\vartheta_0) &= - (B_{0} + T^{-1/2}  B_{1})^{-1} \left(A_{0} + T^{-1/2} A_{1}\right) - \frac{1}{2} T^{-1/2} (B_{0} + T^{-1/2}  B_{1})^{-1} \nonumber \\
    &\begin{bmatrix}
\left( A_{0} + T^{-1/2} A_{1} \right)' ( B_{0} + T^{-1/2}  B_{1} )^{-1}  C_{01}  (B_{0} + T^{-1/2}  B_{1})^{-1} \left( A_{0} + T^{-1/2} A_{1} \right) \nonumber \\
\vdots  \\
\left( A_{0} + T^{-1/2} A_{1} \right)'  (B_{0} + T^{-1/2}  B_{1})^{-1}  C_{0(p+1))} (B_{0} + T^{-1/2}  B_{1})^{-1} \left( A_{0} + T^{-1/2} A_{1} \right)
\end{bmatrix} \label{genexp}\\
&\ \ \ + o_P(T^{-1/2}).
\end{align}
Using the Woodbury matrix identity
\begin{align*}
    (B_0 + T^{-1/2} B_1)^{-1} &= B_0^{-1} - T^{-1/2} B_0^{-1} (I+T^{-1/2} B_1 B_0^{-1})^{-1} B_1 B_0^{-1} \\
    &= B_0^{-1} - T^{-1/2} B_0^{-1} B_1 B_0^{-1} + O_P(T^{-1}),
\end{align*}
and hence \eqref{genexp} reduces to 
\begin{align*}
    T^{1/2} (\hat{\vartheta}-\vartheta_0) &= -B_0^{-1} A_0 - T^{-1/2} \left(B_0^{-1}A_1 - B_0^{-1}B_1B_0^{-1} A_0 + \frac{1}{2} B_0^{-1} \begin{bmatrix}
A_0'B_0^{-1} C_{0,1} B_0^{-1} A_0 \\
\vdots  \\
A_0'B_0^{-1} C_{0,p+1} B_0^{-1} A_0
\end{bmatrix} \right) \\
&\ \ \ +  o_P(T^{-1/2}).
\end{align*}
We find that $E(A_{0}) = E( M_{0\vartheta}^{+}) =  0$ so that 
\begin{align}
    T E(\hat{\vartheta}-\vartheta_0) &= -\left(B_0^{-1} E(A_1) - B_0^{-1} E(B_1B_0^{-1} A_0) + \frac{1}{2} B_0^{-1} \begin{bmatrix}
E(A_0'B_0^{-1} C_{0,1} B_0^{-1} A_0) \\
\vdots  \\
E(A_0'B_0^{-1} C_{0,p+1} B_0^{-1} A_0)
\end{bmatrix} \right) \nonumber \\
&\ \ \ +  o(1). \label{exactbias1}
\end{align}
We rewrite
\begin{align*}
    E(A_0'B_0^{-1} C_{0,i} B_0^{-1} A_0) &= \iota' \left(\left(B_0^{-1} C_{0,i} B_0^{-1} \right) \odot E(A_0 A_0') \right) \iota,
\end{align*}
and from Lemma \ref{genlemmma1}  we have that 
\begin{align*}
    E(A_0 A_0') =  E\left(M^+_{0,\vartheta T} (M^+_{0,\vartheta T})' \right) = A + o(1)
\end{align*}
We also rewrite 
\begin{align*}
  E \left(B_1 B_0^{-1} A_0 \right)  &= 
  E \left(\left( M_{\vartheta,\vartheta' T}^{+} + M_{0,\vartheta \vartheta' T}^{+} \right )A^{-1} M^+_{0,\vartheta T} \right) \\
  &= E \left( M_{\vartheta,\vartheta' T}^{+} A^{-1} M^+_{0,\vartheta T} \right) + E \left( M_{0,\vartheta \vartheta' T}^{+} A^{-1} M^+_{0,\vartheta T} \right)\\
  &=
  \begin{bmatrix}
\iota' \left( A^{-1} \odot E\left(M_{\vartheta_1,\vartheta T}^{+} \left(M^+_{0,\vartheta T}\right)'  \right) \right)\iota \\
\vdots    \\
\iota' \left( A^{-1} \odot E\left(M_{\vartheta_{p+1},\vartheta T}^{+} \left(M^+_{0,\vartheta T}\right)'  \right) \right)\iota 
\end{bmatrix} \\
&\ \ \ +  \begin{bmatrix}
\iota' \left( A^{-1} \odot E\left(M_{0,\vartheta_1\vartheta T}^{+} \left(M^+_{0,\vartheta T}\right)'  \right) \right)\iota \\
\vdots    \\
\iota' \left( A^{-1} \odot E\left(M_{0,\vartheta_{p+1}\vartheta T}^{+} \left(M^+_{0,\vartheta T}\right)'  \right) \right)\iota 
\end{bmatrix}. \\
\end{align*}

From Lemma \ref{genlemmma1_1} we have that
\begin{align*}
     E\left(M_{0,\vartheta_k\vartheta T}^{+} \left(M^+_{0,\vartheta T}\right)'  \right) = F_{k} + o(1),  
\end{align*}
for $k = 1,\ldots,p+1$, with 
\begin{align}
    F_{1} = \begin{pmatrix}
-2 \zeta_3 &  \sum_{i = 0}^{\infty} D_{dd} \pi_i(0) b_{\varphi'i}(\varphi_0)  \\
 \sum_{i = 2}^{\infty} i^{-1} h_{d \varphi i}(\varphi_0)  &-    \sum_{i = 2}^{\infty}    h_{d \varphi i}(\varphi_0) b_{\varphi' i}(\varphi_0) 
\end{pmatrix},  \label{genF1}
\end{align}
and for $m = 1,\dots, p$ it follows that
\begin{align}
    F_{m+1} = \begin{pmatrix}
  \sum_{i = 2}^{\infty} i^{-1} h_{d \varphi_m i}(\varphi_0)    & - \sum_{i = 2}^{\infty}    h_{d \varphi_{m} i}(\varphi_0) b_{\varphi' i}(\varphi_0)  \\
 -\sum_{i = 1}^{\infty} i^{-1} b_{\varphi \varphi_m i}(\varphi_0)  & \sum_{i = 1}^{\infty}  b_{\varphi \varphi_m i}(\varphi_0) b_{\varphi' i}(\varphi_0) 
\end{pmatrix},  \label{genF2}
\end{align}
From Lemma \ref{genlemma891} we have that
\begin{align*}
     E\left(M_{\vartheta_k,\vartheta T}^{+} \left(M^+_{0,\vartheta T}\right)'  \right)  = G_{k} + o(1)  
\end{align*}
for $k = 1,\ldots,p+1$, with
\begin{align}
   G_{1} = \begin{pmatrix}
G_{1}(1,1) &  G_{1}(1,2) \\
G_{1}(2,1) & G_{1}(2,2) 
\end{pmatrix},  \label{genG1}
\end{align}
where the elements are given by
\begin{align*}
G_{1}(1,1) &=  -4 \zeta_3, \\
G_{1}(1,2) &=  2 \sum^{\infty}_{k = 1} b_{\varphi' k}(\varphi_0) \sum_{s = 1}^{\infty} s^{-1} (s+k)^{-1}, \\
G_{1}(2,1) &=  \sum_{k = 1}^{\infty} k^{-1} \sum_{s = 1}^{\infty} \left(  s^{-1} b_{\varphi (s+k)}(\varphi_0) + (s+k)^{-1} b_{\varphi s}(\varphi_0) \right), \\
G_{1}(2,2) &= -\sum_{k = 1}^{\infty} \sum_{s = 1}^{\infty} \left(  s^{-1} b_{\varphi (s+k)}(\varphi_0) + (s+k)^{-1} b_{\varphi s}(\varphi_0) \right)  b_{\varphi' k}(\varphi_0),
\end{align*}
and for $m = 1,\dots, p$ it follows that 
\begin{align}
    G_{m+1} = \begin{pmatrix}
 G_{m+1}(1,1)  & G_{m+1}(1,2) \\
G_{m+1}(2,1) & G_{m+1}(2,2)
\end{pmatrix}  \label{genG2}
\end{align}
where the elements are given by
\begin{align*}
 G_{m+1}(1,1) &= \sum_{k = 1}^{\infty} k^{-1} \sum_{s = 1}^{\infty} \left(  s^{-1} b_{\varphi_m (s+k)}(\varphi_0) + (s+k)^{-1} b_{\varphi_m s}(\varphi_0) \right), \\
 G_{m+1}(1,2) &= -\sum_{k = 1}^{\infty} b_{\varphi' k}(\varphi_0)  \sum_{s = 1}^{\infty} \left(  s^{-1} b_{\varphi_m (s+k)}(\varphi_0) + (s+k)^{-1} b_{\varphi_m s}(\varphi_0) \right), \\
 G_{m+1}(2,1) &=  -\sum_{k = 1}^{\infty} k^{-1} \sum_{s = 1}^{\infty} \left(  b_{\varphi_m s}(\varphi_0) b_{\varphi (s+k)}(\varphi_0) + b_{\varphi_m (s+k)}(\varphi_0) b_{\varphi s}(\varphi_0) \right), \\
 G_{m+1}(2,2) &= \sum_{k = 1}^{\infty}  \sum_{s = 1}^{\infty} \left(  b_{\varphi_m s}(\varphi_0) b_{\varphi (s+k)}(\varphi_0) + b_{\varphi_m (s+k)}(\varphi_0) b_{\varphi s}(\varphi_0) \right)  b_{\varphi' k}(\varphi_0),
\end{align*}

Then
\begin{align*}
    T E(\hat{\vartheta}-\vartheta_0) &= -A^{-1} E(\sigma^{-2}_0 D_{\vartheta} L^*(\vartheta_0) ) + A^{-1}   \begin{bmatrix}
\iota' \left( A^{-1} \odot G_1 \right)\iota \\
\vdots    \\
\iota' \left( A^{-1} \odot G_{p+1} \right)\iota 
\end{bmatrix}
+  A^{-1}   \begin{bmatrix}
\iota' \left( A^{-1} \odot F_1 \right)\iota \\
\vdots    \\
\iota' \left( A^{-1} \odot F_{p+1} \right)\iota 
\end{bmatrix} \\
&\ \ \ - \frac{1}{2} A^{-1} \begin{bmatrix}
\iota' \left(\left(A^{-1} C_{0,1} A^{-1} \right) \odot A \right) \iota \\
\vdots  \\
\iota' \left(\left(A^{-1} C_{0,p+1} A^{-1} \right) \odot A \right) \iota,
\end{bmatrix} +  o(1)
\end{align*}

Hence we can write 
\begin{align}
    E\left( \hat{\vartheta} - \vartheta_0 \right) = S(d_0,\varphi_0) + B(\varphi_0) + o(T^{-1}), \label{trian11}
\end{align}
where 
\begin{align*}
    T S(d_0,\varphi_0) &= - A^{-1} \left[ \sigma^{-2}_0 E \left( \DLvt^*(\vartheta_0) \right)  \right] \\
    &= A^{-1}  \frac{\sum_{t = 1}^T c_{t}(\vartheta_0) c_{\vartheta t}(\vartheta_0)}{\sum_{t = 1}^T c^2_{t}(\vartheta_0)},
\end{align*}
from Lemma \ref{genlemmaexpectations} and 
\begin{align}
    T B(\varphi_0) = A^{-1}   \begin{bmatrix}
\iota' \left( A^{-1} \odot \left( G_{1} + F_{1} \right)\right)\iota \\
\vdots    \\
\iota' \left( A^{-1} \odot\left( G_{p+1} + F_{p+1} \right) \right)\iota 
\end{bmatrix}  - \frac{1}{2} A^{-1} \begin{bmatrix}
\iota' \left(\left(A^{-1} C_{0,1} A^{-1} \right) \odot A_T \right) \iota \\
\vdots  \\
\iota' \left(\left(A^{-1} C_{0,p+1} A^{-1} \right) \odot A_T \right) \iota
\end{bmatrix}. \label{BtINT}
\end{align}

For $d_0 > 1/2$ it follows from Lemma \ref{genlemmaaaa2n} that
\begin{align*}
    TS(d_0,\varphi_0) =  A^{-1} \frac{\sum_{t = 1}^{\infty} c_{t}(\vartheta) c_{\vartheta t}(\vartheta)}{ \sum_{t = 1}^{\infty} c^2_{t}(\vartheta) },
\end{align*}
and for $d_0 < 1/2$ it follows from Lemma \ref{genlemmaaaa2s} that
\begin{align*}
    TS(d_0,\varphi_0) = A^{-1}  \begin{bmatrix}
  -\log(T)+\left(\Psi(1-d_0) + (1-2d_0)^{-1}\right) \\
\frac{D_{\varphi_1 } \phi(1;\varphi)}{ \phi(1;\varphi) } \\
\vdots    \\
\frac{D_{\varphi_p } \phi(1;\varphi)}{ \phi(1;\varphi) }
\end{bmatrix},
\end{align*}
completing the proof for the bias of $\hat{\vartheta}$. It follows from Lemmata \ref{asyappgennon2}, \ref{asyappgennon3}, \ref{asyappgenstat2} and \ref{asyappgenstat3} that analogues of \eqref{trian11} also hold for $\hat{\vartheta}_{\mu_0}$ and $\hat{\vartheta}_m$. Replacing $\DLvt^*(\vartheta_0)$ by $\DLvt_{\mu_0}^*(\vartheta_0)$, it is clear that the expected score term $S(d_0,\varphi_0)$ gets eliminated. Similarly,  $E (\DLvt_{m}^*(\vartheta_0)) = 0$ by construction, and the proof is completed.  

Observe that we additionally write $S_T(d_0,\varphi_0)$ and $B_T(\varphi_0)$, where we use the exact expectations of the expressions in \eqref{exactbias1} —referred to as $F_{T,i}$, $G_{T,i}$, $A_T$ and $C_{T,0i}$—instead of the asymptotic expectations of $F_{i}$, $G_{i}$, $A$ and $C_{0i}$. Then, the ``exact'' bias is given by  
\begin{align*}
    E\left( \hat{\vartheta} - \vartheta_0 \right) = S_T(d_0,\varphi_0) + B_T(\varphi_0) + o(T^{-1}), 
\end{align*}
where 
\begin{align*}
    T S_T(d_0,\varphi_0) &= - A^{-1} \left[ \sigma^{-2}_0 E \left( \DLvt^*(\vartheta_0) \right)  \right]  \\
    &= A^{-1}  \frac{\sum_{t = 1}^T c_{t}(\vartheta_0) c_{\vartheta t}(\vartheta_0)}{\sum_{t = 1}^T c^2_{t}(\vartheta_0)},
\end{align*}
and 
\begin{align*}
    T B_T(\varphi_0) = A^{-1}   \begin{bmatrix}
\iota' \left( A^{-1} \odot \left( G_{T,1} + F_{T,1} \right)\right)\iota \\
\vdots    \\
\iota' \left( A^{-1} \odot\left( G_{T,p+1} + F_{T,p+1} \right) \right)\iota 
\end{bmatrix}  - \frac{1}{2} A^{-1} \begin{bmatrix}
\iota' \left(\left(A^{-1} C_{T,0,1} A^{-1} \right) \odot A_T \right) \iota \\
\vdots  \\
\iota' \left(\left(A^{-1} C_{T,0,p+1} A^{-1} \right) \odot A_T \right) \iota
\end{bmatrix} .
\end{align*}
With $F_{T,k} = E\left(M_{0,\vartheta_k\vartheta T}^{+} \left(M^+_{0,\vartheta T}\right)'  \right)$ such that  
\begin{align*}
    F_{T,1} = \begin{pmatrix}
-T^{-1} \sum_{t = 1}^T \sum_{i = 0}^{t-1}  D_{dd} \pi_i(0)  D_{d} \pi_i(0) &  T^{-1} \sum_{t = 1}^T \sum_{i = 0}^{t-1} D_{dd} \pi_i(0) b_{\varphi'i}(\varphi_0)  \\
 T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1} i^{-1} h_{d \varphi i}(\varphi_0)  & - T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}    h_{d \varphi i}(\varphi_0) b_{\varphi' i}(\varphi_0) 
\end{pmatrix}, 
\end{align*}
and for $m = 1,\dots, p$ we have
\begin{align*}
    F_{T,m+1} = \begin{pmatrix}
  T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}i^{-1} h_{d \varphi_m i}(\varphi_0)    & - T^{-1}\sum_{t = 1}^T \sum_{i = 2}^{t-1}    h_{d \varphi_{m} i}(\varphi_0) b_{\varphi' i}(\varphi_0)  \\
 -T^{-1} \sum_{t = 1}^T \sum_{i = 1}^{t-1} i^{-1} b_{\varphi \varphi_m i}(\varphi_0)  & T^{-1} \sum_{t = 1}^T \sum_{i = 1}^{t-1}  b_{\varphi \varphi_m i}(\varphi_0) b_{\varphi' i}(\varphi_0) 
\end{pmatrix},  
\end{align*}



With $G_{T,k}  = E\left(M_{\vartheta_k,\vartheta T}^{+} \left(M^+_{0,\vartheta T}\right)'  \right)$ such that 
\begin{align*}
   G_{T,1} = \begin{pmatrix}
G_{T,1}(1,1) &  G_{T,1}(1,2) \\
G_{T,1}(2,1) & G_{T,1}(2,2) 
\end{pmatrix}, 
\end{align*}
where the elements are given by
\begin{align*}
G_{T,1}(1,1) &= -2T^{-1} \sum_{t = 1}^T \sum_{k = 1}^{t-1} \sum_{s = t+1}^T k^{-1}(s-t)^{-1}(s-t+k)^{-1}, \\
G_{T,1}(1,2) &=  2T^{-1} \sum_{t = 1}^T \sum_{k = 1}^{t-1} \sum_{s = t+1}^T b_{\varphi' k}(\varphi_0) (s-t)^{-1} (s-t+k)^{-1}, \\
G_{T,1}(2,1) &=  T^{-1} \sum_{t = 1}^T \sum_{k = 1}^{t-1} \sum_{s = t+1}^T k^{-1}\left(  (s-t)^{-1} b_{\varphi (s-t+k)}(\varphi_0) + (s-t+k)^{-1} b_{\varphi (s-t)}(\varphi_0) \right), \\
G_{T,1}(2,2) &= - T^{-1} \sum_{t = 1}^T \sum_{k = 1}^{t-1} \sum_{s = t+1}^T \left(  (s-t)^{-1} b_{\varphi (s-t+k)}(\varphi_0) + (s-t+k)^{-1} b_{\varphi (s-t)}(\varphi_0) \right)  b_{\varphi' k}(\varphi_0),
\end{align*}
and for $m = 1,\dots, p$ we have
\begin{align*}
    G_{m+1,T} = \begin{pmatrix}
 G_{T,m+1}(1,1)  & G_{T,m+1}(1,2) \\
G_{T,m+1,T}(2,1) & G_{T,m+1}(2,2)
\end{pmatrix},  
\end{align*}
where the elements are given by
\begin{align*}
 G_{T,m+1}(1,1) &=T^{-1} \sum_{t = 1}^T \sum_{k = 1}^{t-1} k^{-1}\sum_{s = t+1}^T \left(  (s-t)^{-1} b_{\varphi_m (s-t+k)}(\varphi_0) + (s-t+k)^{-1} b_{\varphi_m (s-t)}(\varphi_0) \right), \\
 G_{T,m+1}(1,2) &= - T^{-1} \sum_{t = 1}^T \sum_{k = 1}^{t-1}b_{\varphi' k}(\varphi_0) \sum_{s = t+1}^T \left(  (s-t)^{-1} b_{\varphi_m (s-t+k)}(\varphi_0) + (s-t+k)^{-1} b_{\varphi_m (s-t)}(\varphi_0) \right), \\
 G_{T,m+1}(2,1) &=  - T^{-1} \sum_{t = 1}^T \sum_{k = 1}^{t-1}k^{-1} \sum_{s = t+1}^T \left(  b_{\varphi_m (s-t)}(\varphi_0) b_{\varphi (s-t+k)}(\varphi_0) + b_{\varphi_m (s-t+k)}(\varphi_0) b_{\varphi (s-t)}(\varphi_0) \right), \\
 G_{T,m+1}(2,2) &= T^{-1} \sum_{t = 1}^T \sum_{k = 1}^{t-1} \sum_{s = t+1}^T  \left(  b_{\varphi_m (s-t)}(\varphi_0) b_{\varphi (s-t+k)}(\varphi_0) + b_{\varphi_m (s-t+k)}(\varphi_0) b_{\varphi (s-t)}(\varphi_0) \right)  b_{\varphi' k}(\varphi_0),
\end{align*}

With $ A_T = E\left(M^+_{0,\vartheta T} (M^+_{0,\vartheta T})' \right)$  such that 
\begin{align*}
    A_T = \begin{pmatrix}
 T^{-1}  \sum_{t = 1}^T \sum_{j = 1}^{t-1} \frac{1}{j^2}  & - T^{-1}  \sum_{t = 1}^T \sum_{j = 1}^{t-1} b_{\varphi' j}(\varphi_0)/j \\
- T^{-1}  \sum_{t = 1}^T \sum_{j = 1}^{t-1} b_{\varphi j}(\varphi_0)/j  & T^{-1}  \sum_{t = 1}^T \sum_{j = 1}^{t-1} b_{\varphi j}(\varphi_0) b_{\varphi' j}(\varphi_0).
\end{pmatrix} 
\end{align*}

Lastly, $C_{T,0i}$ is defined as follows 
\begin{align*}
C_{T,01} = \begin{pmatrix}
 C_{T,01}(1,1)  & C_{T,01}(1,2) \\
C_{T,01}(2,1) & C_{T,01}(2,2)
\end{pmatrix}, 
\end{align*}
where the elements are given by
\begin{align*}
     C_{T,01}(1,1) &= -3 T^{-1} \sum_{t = 1}^T \sum_{i = 0}^{t-1}  D_{dd} \pi_i(0)  D_{d} \pi_i(0), \\
     C_{T,01}(1,2) &= 2 T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}  i^{-1} h_{d \varphi' i}(\varphi_0) + T^{-1} \sum_{t = 1}^T \sum_{i = 0}^{t-1}  D_{dd} \pi_i(0) b_{\varphi'i}(\varphi_0),\\
     C_{T,01}(2,1) &= 2 T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}  i^{-1} h_{d \varphi i}(\varphi_0) + T^{-1} \sum_{t = 1}^T \sum_{i = 0}^{t-1}  D_{dd} \pi_i(0) b_{\varphi i}(\varphi_0),\\
     C_{T,01}(2,2) &=  -T^{-1} \sum_{t = 1}^T \sum_{i = 1}^{t-1}  i^{-1} b_{\varphi \varphi'i}(\varphi_0) -T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}    b_{\vartheta i}(\varphi_0) h_{d \vartheta' i}(\varphi_0) \\
     &\ \ \ - \left(T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}    b_{\vartheta i}(\varphi_0) h_{d \vartheta' i}(\varphi_0)\right)',
\end{align*}
and for $k = 1,\dots, p$ we have that
\begin{align*}
C_{T,0(k+1)} = \begin{pmatrix}
 C_{T,0(k+1)}(1,1)  & C_{T,0(k+1)}(1,2) \\
C_{T,0(k+1)}(2,1) & C_{T,0(k+1)}(2,2),
\end{pmatrix}  
\end{align*}
where the elements are given by
\begin{align*}
     C_{T,0(k+1)}(1,1) &= 2 T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}  i^{-1} h_{d \varphi_k i}(\varphi_0)  + T^{-1} \sum_{t = 1}^T \sum_{i = 0}^{t-1}  D_{dd} \pi_i(0) b_{\varphi_k i}(\varphi_0),   \\
     C_{T,0(k+1)}(1,2) &= -T^{-1} \sum_{t = 1}^T \sum_{i = 1}^{t-1}  i^{-1} b_{\varphi' \varphi_k i}(\varphi_0) -    T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}    b_{\varphi_k i}(\varphi_0) h_{d \varphi'  i}(\varphi_0) \\ 
     &\ \ \ -  T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}   b_{\varphi'  i}(\varphi_0) h_{d \varphi_k  i}(\varphi_0),   \\
     C_{T,0(k+1)}(2,1) &= -T^{-1} \sum_{t = 1}^T \sum_{i = 1}^{t-1}  i^{-1} b_{\varphi \varphi_k i}(\varphi_0) -    T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}    b_{\varphi_k i}(\varphi_0) h_{d \varphi  i}(\varphi_0) \\
     & \ \ \ -  T^{-1} \sum_{t = 1}^T \sum_{i = 2}^{t-1}    b_{\varphi  i}(\varphi_0) h_{d \varphi_k  i}(\varphi_0),  \\
    C_{T,0(k+1)}(2,2) &= \left( T^{-1} \sum_{t = 1}^T \sum_{i = 1}^{t-1}   b_{\varphi i}(\varphi_0) b_{\varphi' \varphi_k i}(\varphi_0) \right)' + T^{-1} \sum_{t = 1}^T \sum_{i = 1}^{t-1}   b_{\varphi  i}(\varphi_0) b_{\varphi' \varphi_k i}(\varphi_0)  \\
    &\ \ \ + T^{-1} \sum_{t = 1}^T \sum_{i = 1}^{t-1}   b_{\varphi_k i}(\varphi_0) b_{\varphi \varphi' i}(\varphi_0).
\end{align*}


\subsubsection{Proof of Corollary \ref{t54}} \label{proofthm54}


The lag polynomial for AR(1) specification is given by
\begin{align*}
\omega(L;\varphi) = (1-\varphi L )^{-1} = \sum_{j = 0}^{\infty} \omega_j(\varphi) L^j,    
\end{align*}
where $\omega_j(\varphi) = \varphi^j$, $j\geq 0$. The inverse lag polynomial is given by 
\begin{align*}
    \phi(L;\varphi) = \omega^{-1}(L;\varphi) = (1-\varphi L ) = \sum_{j = 0}^{\infty} \phi_j(\varphi) L^j,
\end{align*}
where $\phi_0(\varphi) = 1$ and $\phi_1(\varphi) = -\varphi$, $\phi_s(\varphi) = 0$, $s \geq 2$. Taking the derivatives of $\phi_s(\varphi)$ with respect to $\varphi$ yields: $\partial \phi_0(\varphi)/\partial \varphi = 0$, $\partial \phi_1(\varphi)/\partial \varphi = -1$, and $\partial \phi_s(\varphi)/\partial \varphi = 0$, $s \geq 2$. 
Then
\begin{align*}
    b_{\varphi j}(\varphi_0) &= \sum_{k = 0}^{j-1} \omega_k(\varphi_0) \partial \phi_{j-k}(\varphi_0)/\partial \varphi  \\
    &= -\varphi^{j-1},
\end{align*}
for $j \geq 1$. Since $\partial^2 \phi_j(\varphi)/\partial \varphi^2 = 0$  for $j \geq 1$ we get
\begin{align*}
    b_{\varphi \varphi j}(\varphi_0) &= \sum_{k = 0}^{j-1} \omega_k(\varphi_0) \partial^2 \phi_{j-k}(\varphi_0)/\partial \varphi^2 , \\
    &= 0
\end{align*}
To find an expression for the bias we need the following expansion 
\begin{align}
    \sum_{k = 0}^{\infty} x^{k} &= \frac{1}{1-x},  \label{expans1} \\
    \sum_{k = 1}^{\infty} k^{-1} x^k &= - \log(1-x), \label{expans2}\\
    \sum_{k = 1}^{\infty} (k+1)^{-1} x^{k+1}\sum_{n = 1}^k n^{-1} &= \frac{1}{2} \log^2(1-x) , \label{expans3} \\
    \sum_{k = 1}^{\infty} x^k \sum_{s = 1}^{\infty} s^{-1} (s+k)^{-1} &= -Li_{2} (-\frac{x}{1-x}) , \label{expans4}
\end{align}
where $|x|<1$.
The first three expansions are well known and can be found in \textcite{gradshteyn2014table} on pages 7 (0.231-1), 44 (1.513-4), and 45 (1.516-1) respectively. The last expansion makes use of a couple of results. First, we have the expression:
\begin{align*}
    \sum_{s = 1}^{\infty} s^{-1}(s+k)^{-1} &= k^{-1} \left( \Psi(k+1) + \gamma \right), \\
     \Psi(k+1) + \gamma  &= \int_{0}^{1} (1-t)^{-1}(1-t^k) dt, 
\end{align*}
These results can be found in \textcite{abramowitz1964handbook} on page 259, 6.3.16 and 6.3.22 respectively. Using these results and \eqref{expans2}, we get
\begin{align*}
     \sum_{k = 1}^{\infty} x^k \sum_{s = 1}^{\infty} s^{-1} (s+k)^{-1} &=  \sum_{k = 1}^{\infty} x^k   k^{-1}  \int_{0}^{1} (1-t)^{-1}(1-t^k) dt \\
     &= \int_{0}^{1} (1-t)^{-1}\sum_{k = 1}^{\infty} x^k   k^{-1}  (1-t^k) dt \\
     &= \int_{0}^{1} (1-t)^{-1}\log\left( \frac{1-xt}{1-x} \right) dt, 
\end{align*}
Then a change of variable of integration to get 
\begin{align*}
    \int_{0}^{1} (1-t)^{-1}\log\left( \frac{1-xt}{1-x} \right) dt &= - \int_{1}^{(1-x)^{-1}} (1-u)^{-1} \log(u) du  \\
    &= - Li_2(-\frac{x}{1-x}),
\end{align*}
where $Li_{2}(\varphi) = \sum_{i = 1}^{\infty} i^{-2}\varphi^{i}$, or alternatively $Li_2(1-v) = \int_{1}^v (1-t)^{-1}\log(t) dt $, is the dilogarithm function (Spence's integral), see \textcite[page 1004, 27.7.1]{abramowitz1964handbook} for the integral representation where a slight different definition of the dilogarithm function is used, namely $f(x) = Li_{2}(1-x)$.

Next, we find the expression for $A$ in \eqref{genA}, and its inverse  $A^{-1}$ by using \eqref{expans1} and \eqref{expans2}: 
\begin{align*}
    A = \begin{pmatrix}
\pi^2/6 &  - \varphi^{-1} \log(1-\varphi) \\
 - \varphi^{-1} \log(1-\varphi)  &  (1-\varphi^2)^{-1}
\end{pmatrix} ,
\end{align*}
and 
\begin{align*}
    A^{-1} = \frac{\varphi}{\pi^2 \varphi^2 - 6 (1-\varphi^2)
    \log^2(1-\varphi)} \begin{pmatrix}
6 \varphi&  6 \log(1-\varphi) (1-\varphi^2) \\
6 \log(1-\varphi) (1-\varphi^2)   &  \pi^2 \varphi(1-\varphi^2)
\end{pmatrix}.  
\end{align*}

Next, we find the expression for $C_{01}$ and $C_{02}$ in respectively, \eqref{genC1} and \eqref{genCk}. Using \eqref{expans2} and \eqref{expans3} we find 
\begin{align*}
C_{01} = \begin{pmatrix}
  -6 \zeta_3 &  2 \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi}) - \varphi^{-1} \log^2(1-\varphi) \\
 2 \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi}) - \varphi^{-1} \log^2(1-\varphi)  & 2 \frac{\log(1 - \varphi)}{1-\varphi^2}
\end{pmatrix}, 
\end{align*}
and  
\begin{align*}
C_{02} = \begin{pmatrix}
 2 \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi}) - \varphi^{-1} \log^2(1-\varphi)  & 2 \frac{\log(1 - \varphi)}{1-\varphi^2} \\
 2 \frac{\log(1 - \varphi)}{1-\varphi^2} & 0
\end{pmatrix}.
\end{align*}

Next, we find expression for $F_1$ and $F_2$ in respectively, \eqref{genF1} and \eqref{genF2}. Using \eqref{expans2} and \eqref{expans3} we find 
\begin{align*}
    F_{1} = \begin{pmatrix}
-2 \zeta_3 & -\varphi^{-1}\log^2(1-\varphi)   \\
  \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi}) &  \frac{\log(1-\varphi)}{1-\varphi^2}
\end{pmatrix},  
\end{align*}
and 
\begin{align*}
    F_{2} = \begin{pmatrix}
  \varphi^{-1} Li_{2}(-\frac{\varphi}{1-\varphi})   & \frac{\log(1-\varphi)}{1-\varphi^2}\\
 0  & 0
\end{pmatrix}.
\end{align*}

Next, we find expression for $G_1$ and $G_2$ in respectively, \eqref{genG1} and \eqref{genG2}. Using \eqref{expans1},\eqref{expans2} and \eqref{expans4}  we find 
\begin{align*}
   G_{1} = \begin{pmatrix}
-4 \zeta_3 &  2 \varphi^{-1} Li_2(-\frac{\varphi}{1-\varphi}) \\
 - \varphi^{-1} \log^2(1- \varphi) + \varphi^{-1} Li_2(-\frac{\varphi}{1-\varphi}) & \frac{\log(1-\varphi)}{1-\varphi^2} - \varphi^{-2} \left(\frac{\varphi }{1-\varphi } + \log(1-\varphi )\right) 
\end{pmatrix},  
\end{align*}
and
\begin{align*}
    G_{2} = \begin{pmatrix}
 - \varphi^{-1} \log^2(1- \varphi) + \varphi^{-1} Li_2(-\frac{\varphi}{1-\varphi})  & \frac{\log(1-\varphi)}{1-\varphi^2} - \varphi^{-2} \left(\frac{\varphi }{1-\varphi } + \log(1-\varphi )\right) \\
2\log(1-\varphi) \frac{1}{1-\varphi^2}& -2 \frac{\varphi}{(1-\varphi^2)^2}
\end{pmatrix}.
\end{align*}


Next, we find an expression for the score bias term. First we consider the non-stationary region, i.e.\ $d_0 > 1/2$. We have that
\begin{align*}
    c_t(\vartheta) &= \sum_{j = 0}^{t-1} \phi_j(\varphi) \kappa_{0(t-j)}(d) \\
        &=  \kappa_{0t}(d) - \varphi \kappa_{0(t-1)}(d) I(t \geq 2),   
\end{align*}
and, therefore, 
\begin{align}
    \sum_{t = 1}^{\infty} c^2_t(d,\varphi)
        &=  (1+\varphi^2)\sum_{t = 1}^{\infty} \kappa^2_{0t}(d) - 2\varphi \sum_{t = 1}^{\infty} \kappa_{0t}(d) \kappa_{0(t+1)}(d) \nonumber  \\
        &=  (1+\varphi^2)\sum_{t = 1}^{\infty} \kappa^2_{0t}(d) + 2\varphi \sum_{t = 1}^{\infty} \left(\kappa_{0(t+1)}(d)- \kappa_{0t}(d) \right) \kappa_{0(t+1)}(d) - 2  \varphi  \sum_{t = 1}^{\infty} \kappa^2_{0(t+1)}(d) \nonumber \\
        &=  (1+\varphi^2- 2\varphi)\sum_{t = 1}^{\infty} \kappa^2_{0t}(d) + 2\varphi \sum_{t = 1}^{\infty} \kappa_{0t}(d)\kappa_{0t}(1+d), \label{ctarfi}
\end{align}
where we used the properties $\pi_{0}(u) = 1$ and  $\pi_{t}(u) - \pi_{t-1}(u) = \pi_t(u-1)$ for any $u$, see \textcite[Lemma A.4]{johansen2016role}, and 
approximation by Stirling's Formula impling that $\kappa_{0t}(d) = O(t^{-d})$. The first summand in this expression is given in \textcite[Lemma B.1]{johansen2016role}, i.e.\ , 
\begin{align*}
    \sum_{t = 1}^{\infty} \kappa^2_{0t}(d) =  \binom{2d-2}{d-1},
\end{align*}
where $d > 1/2$. The second term can be derived using a similar proof strategy.
We have that 
\begin{align*}
   \sum_{t = 1}^{\infty} \kappa_{0t}(d)\kappa_{0t}(1+d) &= \frac{1}{\Gamma(1-d) \Gamma(-d)} \sum_{t = 0}^{\infty} \frac{\Gamma(1-d+t)\Gamma(-d+t)}{\Gamma(t) t!}, \\
   &= 0.5  \binom{2d}{d},
\end{align*}
where the last equality follows from \textcite[p. 556, eqn. 15.1.20)]{abramowitz1964handbook}.
We conclude that 
\begin{align*}
    \sum_{t = 1}^{\infty} c^2_t = (1-\varphi)^2 \binom{2d-2}{d-1} +  \varphi  \binom{2d}{d}. 
\end{align*}

Next, we find an expression for $\sum_{t = 1}^{\infty} c_t(d,\varphi) D_{d} c_t(d,\varphi)$. Taking the derivative of the left and right-hand side of \eqref{ctarfi} with respect to $d$ gives 
\begin{align*}
     2 \sum_{t = 1}^{\infty} c_t(d,\varphi) D_{d} c_t(d,\varphi) =  2(1-\varphi)^2   \sum_{t = 1}^{\infty} \kappa_{1t}(d)  \kappa_{0t}(d)   +2 \varphi  \sum_{t = 1}^{\infty} \left( \kappa_{1t}(d) \kappa_{0t}(1+d) + \kappa_{0t}(d) \kappa_{1t}(1+d) \right) . 
\end{align*}
The first summand in this expression is given in \textcite[Lemma B.1]{johansen2016role} (there is a small typo; the minus sign should be a plus sign), i.e.\ , 
\begin{align*}
      \sum_{t = 1}^{\infty} \kappa_{0t}(d)\kappa_{1t}(d) = \binom{2d-2}{d-1} \left( \Psi(2d-1)-\Psi(d) \right),
\end{align*}
where $d > 1/2$. The second term can be obtained using a similar approach as in \textcite[Lemma B.1]{johansen2016role}, and is given by 
\begin{align*}
    D_d \sum_{t = 1}^{\infty} \kappa_{0t}(d)\kappa_{0t}(1+d)   = \binom{2d}{d} \left( \Psi(2d+1)-\Psi(d+1) \right).
\end{align*}
We conclude that 
\begin{align*}
     \sum_{t = 1}^{\infty} c_t(d,\varphi) D_{d} c_t(d,\varphi) &= (1-\varphi)^2  \binom{2d-2}{d-1} \left( \Psi(2d-1)-\Psi(d) \right)  \\
     &\ \ \ + \varphi  \binom{2d}{d} \left( \Psi(2d+1)-\Psi(d+1) \right).
\end{align*}


Next, we find an expression for $\sum_{t = 1}^{\infty} c_t(d,\varphi) D_{\varphi} c_t(d,\varphi)$. Taking the derivative of the left-hand side and right-hand side of \eqref{ctarfi} with respect to $\varphi$ gives 
\begin{align*}
     2 \sum_{t = 1}^{\infty} c_t(d,\varphi) D_{\varphi} c_t(d,\varphi) = (2\varphi- 2)\sum_{t = 1}^{\infty} \kappa^2_{0t}(d) + 2\sum_{t = 1}^{\infty} \kappa_{0t}(d)\kappa_{0t}(1+d).
\end{align*}
Note that the expressions for the two summands are given above. We conclude that 
\begin{align*}
    \sum_{t = 1}^{\infty} c_t(d,\varphi) D_{\varphi} c_t(d,\varphi) = (\varphi- 1)   \binom{2d-2}{d-1}  + 0.5 \binom{2d}{d}.
\end{align*}
Therefore the score bias $ S(d_0,\varphi_0)$ for $d_0 > 1/2$ is given by
\begin{align*}
     T S(d,\varphi)  &= A^{-1} \left[ (1-\varphi)^2 \binom{2d-2}{d-1} +  \varphi  \binom{2d}{d} \right]^{-1} \times
     \\ 
     &\begin{bmatrix} (1-\varphi)^2  \binom{2d-2}{d-1} 
     \left( \Psi(2d-1)-\Psi(d) \right)  + \varphi  \binom{2d}{d} \left( \Psi(2d+1)-\Psi(d+1) \right)
  \\
(\varphi- 1)   \binom{2d-2}{d-1}  +  0.5\binom{2d}{d}
\end{bmatrix},
\end{align*}

The score bias $S(d_0,\varphi_0)$ for $d_0 < 1/2$ is given by
\begin{align*}
     T S(d,\varphi)  = A^{-1} 
     &\begin{bmatrix} -\log(T)+\Psi(1-d_0) + (1-2d_0)^{-1}
  \\
-\frac{1}{1-\varphi}
\end{bmatrix}, 
\end{align*}
because $\phi(1;\varphi) = 1-\varphi$ and the proof is complete. 



\end{document}
