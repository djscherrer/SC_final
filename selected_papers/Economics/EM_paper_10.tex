\begin{document}
\affiliation{$$_affiliation_$$}
\title{A Locally Robust Semiparametric Approach to Examiner IV Designs}
\maketitle
\vspace{-0.2cm}

\begin{abstract}
I propose a locally robust semiparametric framework for estimating causal effects using the popular examiner IV design, in the presence of many examiners and possibly many covariates relative to the sample size. The key ingredient of this approach is an orthogonal moment function that is robust to biases and local misspecification from the first step estimation of the examiner IV. I derive the orthogonal moment function and show that it delivers multiple robustness where the outcome model or at least one of the first step components is misspecified but the estimating equation remains valid. The proposed framework not only allows for estimation of the examiner IV in the presence of many examiners and many covariates relative to sample size, using a wide range of nonparametric and machine learning techniques including LASSO, Dantzig, neural networks and random forests, but also delivers root-n consistent estimation of the parameter of interest under mild assumptions. 
\vspace{0.1in}

\noindent \textit{JEL Numbers:} B23, C01, C14, C26, C31, C45

\noindent \textit{Keywords:} examiner IV, semiparametric estimation, orthogonal moment function, machine learning. 
\end{abstract}
\newpage 

\section{Introduction \label{sec:Introduction}}

Examiner instrumental variable (IV) designs are increasingly popular in empirical economics. An early example of these examiner IV designs in applied economics is found in \citet{10.1257/aer.96.3.863}, who leverages  random assignment of judges to estimate the effects of the duration of incarceration spells on two labor market outcomes: employment and earnings prospects. \citet{10.1257/aer.96.3.863} uses the average incarceration length meted out by one’s randomly assigned judge across all cases adjudicated by that judge as an IV for the potentially endogenous duration of incarceration spell. Since \citet{10.1257/aer.96.3.863}’s seminal study, there has been a burst of new literature exploiting idiosyncratic allocation of examiners to cases in pursuit of answers to a wide variety of interesting economic questions in settings beyond the criminal justice system.

The key idea in these designs is to exploit exogenous variation in the treatment propensity of these randomly assigned examiners in an IV strategy. These designs differ from the textbook IV designs insofar as the examiner IV, the examiner’s propensity to assign treatment status, is a latent variable that is estimated from data. To estimate the examiner IV, the canonical examiner IV design exploits jackknife methods to deal with “own observation” bias, which arises when one's own treatment status is used in the estimation of the examiner's treatment propensity. 

The standard approach to estimation and inference in examiner IV designs with a single binary endogenous treatment and covariates is the unbiased jackknife estimation method proposed by \citet{kolesarcowles}.  In most settings, examiners have control over which times (days or time of day) they can work, and where or in which office they can work. Random assignment of examiners is, therefore, conditional on this sorting. To the extent that the random assignment is conditional, the treatment propensity is valid as an IV conditional on these covariates (conditionally ignorable). In some settings, the treatment propensity also varies across case characteristics while in others random assignment happens within batches of cases rather than at the individual level. Different applications, therefore, admit different configurations of covariates to cleanse the examiner IV of the residual confounding variation. \citet{kolesarcowles}'s jackknife method allows for such covariates to avoid the well-known biases from canonical jackknife procedures in the presence of covariates. 

However, \citet{kolesarcowles}'s approach excludes a range of applications. \citet{kolesarcowles}'s method hinges on the assumption that the approximation of the conditional expectation function for the treatment propensity is exactly linear in covariates. By construction,  this linear approximation is exact in saturated specifications. In these specifications, empirical researchers necessarily include all the discrete covariates (e.g. strata indicators) and their interactions. With a few number of covariates, this approach works well but with even a moderate number of covariates, the dimension of the covariate vector scales up quite quickly relative to the sample size. On the other hand, in non-saturated designs, including when continuous covariates are included to predict the treatment propensity of examiners, the assumption that the linear approximation is exact is restrictive and generally unwarranted. Instead, researchers may want to estimate the treatment propensity nonparametrically.  

In general, existing methods are not suitable for applications where the number of covariates is quite large relative to the number of observations, including in the saturated specifications with high dimensional fixed effects. In such settings, it seems eminently reasonable to use machine learning (ML) methods to predict the treatment propensity. Nevertheless, a growing recent literature suggests caution in use of ML in two-step estimation problems. This literature has established that plug-in estimators based on machine learning first steps introduce large regularization and model selection biases which contaminate estimation and inference in the second step (see, for example, \citet{chernozhukov2018double}). In some settings, including applications involving naive LASSO plug-in estimators in two-estimation, the two-step estimators are not even root-n consistent (\citet{chernozhukov2022locally}). I focus on these settings where empirical researchers estimate the conditionally ignorable treatment propensity nonparametrically with many examiners and possibly many covariates relative to the sample size.

I set out to answer the question: how can we perform reliable semiparametric estimation and inference in examiner IV designs where the examiner IV, the treatment propensity of each quasi-randomly examiner, is estimated nonparametrically with possibly many covariates relative to sample size? In an attempt to answer this question, I propose a locally robust semiparametric approach to estimation and inference in examiner IV designs. This method-of-moments approach entails constructing debiased sample moments based on an orthogonal/locally robust score which consists of an identifying moment function and a first step influence function. As I will argue, this approach is well-suited to settings where the examiner IV is estimated nonparametrically with possibly many covariates because it mitigates the machine learning (regularization and model selection) biases and the first step estimation error compared to simple plug-in approaches. This approach also takes care of the own observation bias that arises in this literature (\citet{chernozhukov2022locally}). Furthermore, the framework that I propose delivers root-n consistent estimation using examiner IV in settings where the examiner IV is estimated in the first step using one of a wide range of nonparametric or ML methods. 

This paper contributes to a growing theoretical literature on examiner IV designs. In important work, \citet{frandsen2023judging} revisit these examiner IV designs and propose a new approach to surmount violations of some of the assumptions underpinning standard approaches. Unlike \citet{frandsen2023judging}, I do not interrogate the identification issues in this literature. Instead, I focus exclusively on estimation and inference in the presence of covariates. I build on the excellent work of \citet{kolesarcowles} who proposes an unbiased jackknife estimation method that accounts for covariates in the estimation of the treatment propensity. While \citet{kolesarcowles} contends that the treatment propensity can also be estimated nonparametrically using his method, his method does not account for the biases from the first step if non-parametric methods or machine learning is used in the estimation of the treatment propensity. Another closely related piece of work is \citet{mueller2015criminal} which, to the best of my knowledge, is the earliest application of machine learning to estimate the treatment propensity in examiner IV designs. Unlike \citet{mueller2015criminal}'s approach, which uses LASSO under strong sparsity assumptions without adjusting for regularization and model selections biases, the approach I propose allows for a wide range of ML first steps including LASSO, corrects for first estimation biases and delivers root-n consistent estimation that is not guaranteed for estimators based on naive plug-in LASSO and other ML first steps. My approach does not require strong sparsity assumptions and is robust to misspecification of some of the components estimated in the first step or the outcome model. 

While I focus explicitly on examiner IV designs, this work also contributes to the broader IV literature. Particularly, it contributes to the optimal instruments literature with machine learning or nonlinear first steps, which spans the work of \citet{belloni2012sparse}, \citet{HANSEN2014290}, \citet{chernuzhukov2015}, \citet{syrgkanis2019machine}, and \citet{bai2010instrumental}. \citet{belloni2012sparse} and \citet{chernuzhukov2015} use LASSO under approximate sparsity assumptions, while \citet{HANSEN2014290} and \citet{bai2010instrumental} use ridge regression and gradient boosting respectively. \citet{syrgkanis2019machine} provides a more general framework for using ML to estimate conditional average treatment effects (CATE).  In this paper, I leverage locally robust semiparametric theory to provide a unified framework for semiparametric estimation of the IV model with treatment effect heterogeneity in \citet{kolesarcowles} for a wide range of ML first steps in the presence of many exogenous covariates. This framework admits a completely nonparametric first step estimation of the ``optimal" instrument in presence of many exogenous covariates under mild conditions.

More recent work closely related to this work is \citet{wiemann2023optimal} who propose using a version of K-means clustering to estimate the latent optimal instrument from a large set of candidate categorical instruments. While this approach can be applied to the examiner IV setting insofar as it accounts for many candidate categorical instruments, it does not deal with the aspect of many covariates that is relevant when the latent optimal instrument is conditionally ignorable and which motivates this work. \citet{jochmans2023many} also proposes a group fixed effects characterization of the examiner fixed effects, inspired by the panel data literature. Like \citet{wiemann2023optimal}, \citet{jochmans2023many} does not deal with issues arising from having many covariates. However, an important insight that I exploit from \citet{jochmans2023many} is that cross-fitting reduces the first stage estimation error in the construction of the latent instruments. Thus, besides dealing with ``own observation bias" and lack of Donsker conditions which are not known to hold in high dimensional settings, the cross-fitting as applied in my framework also mitigates the first stage estimation error. \citet{mikusheva2021many} also observes that cross-fitting reduces dependence between machine learning first steps and inference in the second step, which further bolsters the case for cross-fitting and alleviates concerns regarding possible distortions induced by machine learning in empirical practice that have been recently documented by \citet{angrist2022machine}.

This paper draws mainly from a more recent but rapidly expanding literature on automatic debiased machine learning and locally robust semiparametric estimation (see \citet{chernozhukov2018double}, \citet{chernozhukov2021automatic}, \citet{chernozhukov2022automatic}, \citet{chernozhukov2022debiased}) \citet{chernozhukov2022locally} and \citet{ichimura2022influence}). \citet{chernozhukov2018double} introduce various \textit{double debiased} machine learning approaches to discipline use of machine learning in statistics and econometrics, with emphasis on reducing regularization and model selection biases from first step estimation. In this paper, I exploit the locally robust semiparametric approach that debiases the identifying moment function, which depends on a plug-in estimate of a first step nuisance function, by adding to it an influence function adjustment. In developing this approach, I primarily leverage theoretical insights from \citet{chernozhukov2022locally} and \citet{ichimura2022influence}. Using this approach, I provide an estimation method that allows for use of most of the off-the-shelf machine learning algorithms to estimate the examiner IV under mild regularity conditions in a setting where there are many examiners and possibly many covariates. The method also mitigates bias when other nonparametric methods are used, and allows for misspecification in the first step estimation, by virtue of a multiple robustness property of the orthogonal moment function. 

The remainder of the paper is organized as follows: section 2 outlines and further motivates the estimation issues that arise in the examiner leniency design; section 3 presents the main results; and section 4 concludes and provides directions for future work.

\section{The Examiner Leniency Design: Estimation Issues in Empirical Practice}
To fix concepts, I provide a quick overview of the examiner leniency design using an example from a canonical empirical setting: the US criminal justice system. 

Suppose we are interested in the causal effect of pretrial detention on conviction (see \citet{frandsen2023judging}): 
$$Y_{i} = \delta T_{i}+\mathbf{X}_{i}^{\prime} \beta+\epsilon_{i}$$

where $Y_i$ is the outcome; $T_i$ is the bail decision (it is $1$ when a defendant is detained and $0$ otherwise); $X_i$ is a vector of defendant and case characteristics; and $\varepsilon$ includes idiosyncratic factors that also influence the outcome. The treatment status, $T$, is endogenous if there are unobservables that are correlated with both treatment and the outcome. For example, well-to-do defendants can afford better lawyers and are therefore less likely to be detained and less likely to be convicted. Those who get treated (detained) are, therefore, potentially systematically different from those who do not get treated.  This selection biases the OLS estimates of $\delta$.

The idea behind the examiner IV design is to instrument the endogenous treatment status, $T_i$, with one's assigned judge's propensity to detain or release, $T_i$, with $E\left[T_i \mid Z_i\right]$ where $Z_i$ is a $J \times 1$ vector of examiner indicators. Since the examiners (judges, in this case) are quasi-randomly assigned to cases, this propensity to assign treatment status is independent of the unobservables. This treatment propensity, also known as the judge stringency or leniency measure (depending on how $T$ is defined), is a valid instrument under the plain vanilla IV assumptions, namely: the exclusion restriction, which requires that judges affect outcomes only through the bail decision; monotonicity, which requires that a defendant released by a more strict judge is also released by the more lenient judge; and relevance, which effectively requires that the examiners' treatment propensity varies non-trivially. 

This principle of examiner (or judge) IV designs applies more generally in settings where there is an idiosyncratic assignment of individuals to a set of decision-makers, whose propensity to assign treatment status varies non-trivially across those decision-makers. It therefore comes as no surprise that examiner IV designs are being increasingly used for causal inference in economics and adjacent disciplines, given the wide range of settings where such idiosyncratic assignment of decision-makers takes place. These designs have been exploited to examine causal effects of foster care on various socioeconomic outcomes  (e.g. \citet{gross2022temporary} and \citet{bald2022economics}); causal effects of bankruptcy on earnings, adverse financial events and foreclosure rates (e.g. \cite{dobbie2015debt}); as well as causal effects of disability benefits on labor supply, household consumption and mortality (e.g. \citet{black2018effect}), to pick but a few from the expanding list of applications.

Since \citet{10.1257/aer.96.3.863}'s seminal study, several studies have used a leave-one-out sample analogue of $E\left[T_i \mid Z_i\right]$ as an examiner IV:  $$\frac{1}{\left|i^{\prime} \neq i, Z_{i^{\prime}}=Z_i\right|} \sum_{i^{\prime} \neq i, Z_{i^{\prime}}=Z_i} \Tilde{T_i}$$

The leave-one-out approach is used to avoid ``own observation bias" whereby one's own endogenous treatment status used in the estimation of the examiner IV sullies the IV with the same endogeneity we are grappling with. Moreover, in many applications, the assignment of examiners is not completely random. In the working example of the bail system in the US, judges have control over which courtrooms, days and shifts they work so there is clearly intentional sorting of judges into locations and times, which would otherwise invalidate the examiner IV. To cleanse the judge stringency measure of this confounding variation, the location-by-time fixed effects (and possibly other covariates) are partialled out from the treatment variable via an application of the Frisch-Waugh-Lovell theorem. Therefore, researchers use the residualized treatment, $\Tilde{T_i}$, instead of $T_i$. With this estimate, they then apply the two stage least squares (TSLS). This leave-one-out approach effectively comes down to the jackknife IV estimation proposed by \citet{angrist1999jackknife}. 

An alternative approach that is currently recommended is \citet{kolesarcowles}'s unbiased jackknife IV (UJIVE). While the jackknife IV (JIVE) based on \citet{angrist1999jackknife} yields the sample average treatment for an examiner leaving out  $i$'s own treatment status, the UJIVE residualizes the treatment status using a jackknife regression before it is used in an IV estimator. In the UJIVE approach of \citet{kolesarcowles}, the covariates are partialled out linearly via jackknife regressions under the assumption that the first stage regression is saturated so that a linear approximation of the conditional expectation function is exact. 

The UJIVE formulation of \citet{kolesarcowles} is based on the following decomposition of the conditionally ignorable treatment propensity with the covariate vector partialled out:

\begin{align}
\gamma_i &= \mathbb{E}(T_i \mid Z_i, X_i) - \mathbb{E}(T_i \mid X_i) \\
&= Z_i^{\prime}\beta_1 + X_i^{\prime}\beta_2 - X_i^{\prime}\alpha \label{eq:diff}
\end{align}

where the second equality is the linear approximation of the conditionally ignorable treatment propensity. 
\citet{kolesarcowles}'s first step estimator of $\gamma_i$, $\hat{\gamma_i}$, is the sample analogue:
\[\hat{\gamma}_i=Z_i^{\prime}\hat{\beta}_{1/i}+X_i^{\prime}\hat{\beta}_{2/i}-X_i \hat{\alpha}
\]
where $\hat{\beta_{1/i}}$, $\hat{\beta_{2/i}}$ and $\hat{\alpha}$ are jackknife estimators (least squares estimators with observation $i$ left out). A vector $\hat{\gamma}$ of the $\hat{\gamma_i}$'s is then plugged as an instrument into an IV estimator in the second step (see section 6.2 of \citet{kolesarcowles} for more details).

First, note that since $\hat{\gamma}$, is an \textit{estimate} of the true treatment propensity, the estimation error from the construction of $\gamma$, represented as $\hat{\gamma}-\gamma$, potentially contaminates inference in the second step (cf: \citet{hahnridder2013}).  Secondly, the construction above hinges crucially on the assumption that the linear approximation above is exactly linear in the examiner indicators, $Z_i$, and the covariate vector, $X_i$. With discrete controls, the first stage is saturated when all the controls and their interactions are included in the covariate vector, $X$. This often leads to high dimensional fixed effects, which effectively \enquote{chop up} the data into cells and the examiner IVs are estimated in those cells. This potentially raises concerns of precision and many-weak IV (see \citet{bhuller} for an example of this in practice). 

In many applications, researchers resort to throwing away ``small' cells, but what are considered ``small'' cells often varies across settings. \citet{frandsen2023judging}, for example, restrict their sample to judges who had seen at least 50 arraignments. Other researchers choose higher or lower thresholds. In the worst case, this ad hoc solution might induce a severe missing data problem. A less pernicious but still troubling concern is that if those discarded ``small'' cells  belong to the tails of the distribution of examiners' treatment propensity, then the estimand the researchers effectively estimate might be different from the estimand they think they are estimating. To see this, consider a setting where senior judges are more stringent and senior judges end up with few cases. Figure 1 below shows the truncation of the examiner treatment propensity (defined here as ``stringency'') distribution in this hypothetical setting:

\vspace{1.5em}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) -- (6,0);

  \draw[red, very thick, above] (4.5,0) -- (6,0);
  
  \filldraw (2,0) circle (2pt);
  \node[above] at (2,0.2) {$p$};
  
  \filldraw (4.5,0) circle (2pt);
  \node[black, above left] at (4.5,0.2) {$p'$};
\end{tikzpicture}
\end{center}
\vspace{1.5em}

The red segment denotes the treatment propensities thrown away along with the most stringent judges and cannot be estimated. Since the estimand from \citet{kolesarcowles} yields a weighted average of pairwise LATEs between each pair of judges, for a fixed value $p$ we estimate the estimand as a weighted average of pairwise LATEs between $p$ and $p'$. In practice, therefore, all pairwise LATEs in the red segment are not part of the estimand that is estimated. The challenge is that we do not observe $p$ and $p'$, so we do not know empirically the estimand that we are recovering in this setting. This setting, while presented here as a hypothetical scenario, is not far-fetched. \citet{bhuller}, for example, observes that in Norway, senior judges and junior judges are assigned different caseloads. If, in such a setting, senior judges tend to see few arraignments and also tend to be more stringent, this scenario (or variations thereof) would not be inconceivable. 
Moreover, since the threshold for what is considered a ``small" cell is at the discretion of the researcher and there is no theory guiding researchers on the appropriate threshold in a given application, different researchers using the same data would come to different conclusions in their empirical analyses if the estimates are sensitive to the choice of the threshold. 

In non-saturated specifications or when continuous covariates are included, the assumption of a perfectly linear conditional expectation function is dubious. As recently observed by \citet{blandhol2022tsls}, when the first stage is not saturated and unless the first stage is estimated nonparametrically, the TSLS estimand does not yield a LATE within a convex hull of pairwise treatment effects. To use \citet{blandhol2022tsls}'s terminology, \citet{kolesarcowles}'s estimand has a \textit{weakly causal} interpretation when the first stage is perfectly linear or estimated nonparametrically. In non-saturated specifications or when continuous controls are included, \citet{blandhol2022tsls}'s insight suggests estimating the treatment propensity using some nonparametric methods such as sieves or polynomial splines. These methods, however, tend to produce biases which get transmitted to the second step where the parameter of interest is estimated. This motivates an approach that takes into account these biases in the second step. 

More generally, specifications where the number of examiners and number of covariates are very large relative to sample size tip researchers over into the realm of high dimensional estimation and inference, where machine learning methods are routinely used. This paper provides a more general framework with mild theoretical guarantees which permit researchers to use an array of machine learning techniques to estimate treatment propensity in settings with large numbers of examiners and covariates that are not vanishingly small  relative to the sample size. The framework also allows for first step nonparametric methods such as sieves or polynomial splines in settings with low to moderate numbers of examiners and controls. 

In the framework that I provide in the next section, I allow $X$ and $Z$ to be very large relative to the sample size, $N$. In such a high-dimensional setting, we can think of the dimensions of $X$ and $Z$ as \textit{increasing with the sample size} (see \citet{chernozhukov2018double}). In the standard examiner IV designs, $Z$ consists of examiner indicators. In my proposed approach, I allow for $Z$ to be more general and the examiner fixed effects characterization arises as a special case. For example,  $Z$ can be examiner types, and therefore be identified with vectors of examiner's discrete and continuous characteristics such as age, race, experience and so on.  Henceforth, in lieu of the linear approximation in equation (\ref{eq:diff}) and motivated by the issues raised in this section, I use a nonparametric specification where the difference in conditional expectations is a difference in a square-integrable function of both $X$ and $Z$ and a square-integrable function of $X$ only.


\section{Results}

In this section, I present the main theoretical results of this paper. I first construct the orthogonal moment function by adding an influence function to the identifying moment function. As the name suggests, this moment function is \textit{orthogonal} or insensitive to the biases induced in the first step non-parametric estimation of the treatment propensity, and is used to construct \textit{debiased} sample moments. I show that the orthogonal moment function is valid, namely; the function satisfies the key orthogonality properties that are requisite for the locally robust semiparametric framework. I argue that the orthogonal moment function does not only reduce the effect of the first step estimation error and machine learning biases, but it is also multiply robust in the sense that misspecification of some of the first step components or the outcome model leaves the estimating equation valid. I then provide the statistical guarantees for this approach, which mainly consist of mean square convergence rates and a few other primitive conditions on the components estimated in the first step. I show that the examiner IV estimator is asymptotically normal and provide an expression for the variance as well as its consistent estimator.

\subsection{Construction of the Orthogonal Moment Function}

The construction of the orthogonal moment function or locally robust score below follows \citet{chernozhukov2022locally} and \citet{ichimura2022influence}. The orthogonal score consists of two ingredients, namely: the identifying moment function (score) and the influence function adjustment (or correction term). The identifying moment function derives from the estimand of interest while the influence function adjustment follows from an orthogonality condition in a sense that I make precise later in this section.

I use the following IV estimand in \citet{kolesarcowles}: 

\[
\theta:=\frac{\operatorname{Cov}(Y, \gamma)}{\operatorname{Cov}(T, \gamma)}=\frac{\mathbb{E}[Y \gamma]}{\mathbb{E}[T \gamma]}
\]
where $\gamma:=\mathbb{E}[T \mid X, Z]-\mathbb{E}[T \mid X]$ is the conditionally ignorable treatment propensity given a vector of covariates, $X$; $T$ is a binary treatment status; $Z$ is a vector of examiner indicators or examiner types; and $Y$ is a scalar outcome variable. Informally, you may think of $\gamma$ as the treatment propensity after partialling out the covariates, $X$. More formally, treating each examiner as an instrument, we may interpret $\gamma$ as the strength of an instrument assigned to individual $i$ relative to other potential instruments conditional on covariates (see Section 3.2 in \citet{kolesarcowles})
 
\citet{kolesarcowles} shows that, for heterogeneous treatment effects, this estimand yields a convex combination of local average treatment effects between pairs of judges under pairwise monotonicity, exclusion restriction and exogeneity. In this paper, I restrict my attention to this estimand on account of its growing popularity and acceptance in the examiner IV literature. \citet{frandsen2023judging} provide alternative estimators when the stronger versions of monotonicity and exclusion restriction do not hold, while \citet{Frandsen2023} provide another estimator that takes into account the fact that in some settings such as bail hearings in the US, batches of cases rather than individual cases are randomly assigned. My framework can accommodate the latter estimator under suitable conditions by simply including batch indicators in the covariate vector, $X$. On the other hand, unfortunately, it does not immediately extend to the estimator in \citet{frandsen2023judging}. I leave this extension to future work. 

An oracle estimator based on the parameter above would simply be:

\[\hat{\theta}=\frac{\mathbb{E}_n[Y \gamma]}{\mathbb{E}_n[T \gamma]}\]
where $\mathbb{E}_n$ is the empirical analogue of expectation and the oracle tells us the value of $\gamma$. This estimator is infeasible because, of course, there is no oracle to tell us the value of $\gamma$. For the feasible estimator, a researcher replaces the conditionally ignorable treatment propensity,$\gamma$, with an estimate. Then the estimate of the treatment propensity is plugged into the second step to obtain a two-step estimator for the parameter of interest. \citet{kolesarcowles} estimates the treatment propensity using an unbiased jacknife estimation approach described in Section 2 to avoid own observation bias. As will be discussed in the section on estimation, besides eliminating the need for Donsker conditions, cross-fitting also eliminates this own observation bias. However, as pointed out earlier, \citet{kolesarcowles}'s approach, among other limitations, excludes a growing range of applications where the number of covariates and examiner fixed effects is large relative to the sample size, a setting that is suitable for use of ML to estimate the treatment propensity. 

The identifying moment function is the function, $g (W, \gamma, \theta)$ such that $\mathbb{E}[g (W, \gamma, \theta)]=0$ whenever $\theta=\theta_0$. Here $W$ represents a concatenated data observation: $W=(Y,T,X)$. It follows from rearranging \citet{kolesarcowles}'s IV estimand that: $$g (W, \gamma, \theta)=[Y-\theta T]\gamma$$

To see this, notice that $\mathbb{E}[(Y-\theta T)\gamma]=\mathbb{E}[Y \gamma]-\theta \mathbb{E}[T \gamma]=0$ whenever $\theta=\theta_0$. The validity of this moment condition also follows from instrument exogeneity: if $\gamma$ is a valid instrument, then the idiosyncratic error term in the outcome model should be orthogonal to the instrument, implying $\mathbb{E}[(Y-\theta T) \gamma]=0$.

The next important ingredient for the orthogonal moment condition is the influence function adjustment. Use of influence functions to correct for biases in first step estimation has a long history in econometrics (see \citet{newey1994asymptotic}, \citet{hahnridder2013}, \citet{hahn1998role}, and \citet{ai2003efficient}. The construction of the influence function adjustment in this section is based on the work of \citet{chernozhukov2022locally} and \citet{ichimura2022influence}, who provide a general form of the influence function, provided a certain ``exogenous" orthogonality condition is satisfied. \citet{chernozhukov2022locally} and \citet{ichimura2022influence} show that if the following ``exogenous" orthogonality condition is satisfied:
\begin{equation} \label{orthog}
E_F[\delta(X) \lambda(W, \gamma(F)(X))]=0 \quad \text{for all} \quad \delta \in \Gamma
\end{equation}
then under some regularity conditions, the influence function adjustment takes the following general form:
\begin{equation} \label{eq2}
\phi(w, \gamma, \alpha, \theta)=\alpha(x, \theta) \lambda(w, \gamma(x))  
\end{equation}
where $\lambda(w, \gamma(x))$ is a nonparametric residual function and $\alpha(x, \theta)$ is a special function called a Riesz representer. 

In our setting, observe that the conditionally ignorable treatment propensity, $\gamma$ , is just a difference of two conditional expectations, which are just regressions of T on X and Z, and T on X. Let $\gamma_1=\mathbb{E}[T \mid X,Z]$ and $\gamma_2=\mathbb{E}[T \mid X]$. By definition of conditional expectations, $\gamma_1$ is the projection of the endogenous treatment variable, $T$, on the space of square integrable functions of $X$ and $Z$, $\Gamma_1=L^2(X,Z)$; while, on the other hand, $\gamma_2$ is the projection of $T$ on the space of square integrable functions of $X$, $\Gamma_2=L^2(X)$. 

The nice geometry of $L^2$ yields the orthogonality condition in equation (\ref{orthog}). It readily follows from $\gamma_1$ being a projection of $T$ on $L^2(X,Z)$ that $\mathbb{E}(\delta(X,Z)(T - \gamma_1(F)(X,Z)) ) = 0$ for all $\delta \in L^2(X,Z)$. Analogously for $\gamma_2$, $\mathbb{E}(\delta(X)(T - \gamma_2(F)(X)) ) = 0$ for all $\delta \in L^2(X)$ where, only for the time being, I index $\gamma_j$ for each $j=1,2$ by the underlying true distribution of the data, $F$, to emphasize this holds under the true underlying distribution. Henceforth, I suppress this index to avoid notational clutter. This characterization shows that $\gamma_1$ and $\gamma_2$ satisfy the \enquote{exogenous} orthogonality condition of \citet{ichimura2022influence}, which is equation $2.10$ in \citet{chernozhukov2022locally}. Therefore, consistent with the notation of equation 2.10 in \citet{chernozhukov2022locally}, the orthogonality condition can be written as: 
\begin{align*}
    &\mathbb{E}[\delta(X) \lambda(W, \gamma_1(X,Z))]=0 \quad \text { for all } \delta \in \Gamma_1, \text{where } \lambda(W, \gamma_1(X,Z))=T - \gamma_1(X,Z). 
\end{align*}    

Analogously:
\begin{align*}
&\mathbb{E}[\delta(X) \lambda(W, \gamma_2(X,Z))]=0 \quad \text { for all } \delta \in \Gamma_2, \text{where } \lambda(W, \gamma_2(X))=T - \gamma_2 (X).
\end{align*}

Hence the whole theory for a generalized regression, $\gamma$ in \citet{chernozhukov2022locally} goes through with modifications depending on the identifying moment function. Since $\gamma$ consists of two first step functions, $\gamma_1$ and $\gamma_2$, the influence function adjustment consists of two correction terms, one for each first step function (see \citet{ichimura2022influence}, \citet{chernozhukov2022locally} and \citet{newey1994asymptotic}). The terms of the influence function adjustment for the conditional expectations, $\gamma_1$ and $\gamma_2$, take the following more explicit form of equation (\ref{eq2}):

\begin{equation} \label{eq3}
\phi(W, \gamma, \alpha, \theta)=\alpha_1(X,Z)(T-\gamma_1(X,Z))+\alpha_2(X)(T-\gamma_2(X))    
\end{equation}

where $\alpha=(\alpha_1,\alpha_2)$ for Riesz Representers, $\alpha_1$ and $\alpha_2$; $\gamma=h(\gamma_1,\gamma_2$) is a function of $\gamma_1$ and $\gamma_2$; $\lambda(W,\gamma_1(X,Z))=T-\gamma_1(X,Z)$ and $\lambda(W,\gamma_2(X))=T-\gamma_2(X)$ are nonparametric residuals after projecting the endogeneous treatment indicator, $T$ on the respective spaces of square integrable functions of $X$ and $Z$, and square integrable functions of $X$. Since $T$ is binary, the predictions of $\gamma_1(X,Z)$ and $\gamma_2(X)$ need to be bounded between $0$ and $1$, but there is no guarantee these would fall between $0$ and $1$. It thus seems eminently reasonable to use an influence function adjustment of the following form:

\[\phi(W, \gamma, \alpha, \theta)=\alpha_1(X,Z)(T-G(\gamma_1(X,Z))+\alpha_2(X)(T-G(\gamma_2(X))\]
where $G$ is a logit function, in which case the true $\gamma_1$ and $\gamma_2$ functions are logit projections of $T$ on $\Gamma_1$ and $\Gamma_2$ respectively. While this is plausible, I do not explore this suggestion further in this paper, for clarity of exposition. In practice, ML algorithms can be constrained to ``spit out" values between 0 and 1 for $\gamma_1$ and $\gamma_2$, so I use the original influence function adjustment in (\ref{eq3}) without loss of generality. 

While explicit expressions of Riesz representers may be available in closed form or can be calculated, they sometimes take complex analytical shapes and often involve components whose estimation can lead to complications. For conditional expectations, these Riesz Representers typically take the form of projections (see \citet{newey1994asymptotic}). In other settings such as the average treatment effect (ATE), they involve inverse propensity scores which, if the scores are near zero or one for some individuals, can lead to numerical instability. Recent literature provides a valid way of learning these Riesz Representers directly from data using only the orthogonal moment function and without knowing the closed form of the Riesz representers. I use this ``automatic" approach in this paper and therefore I defer further discussion of the estimation of Riesz Representers to the relevant section later. 

Now that we have the identifying moment function and the influence function adjustment, we can write down the orthogonal score:
\[\psi(W, \gamma, \theta, \alpha)=[Y-\theta T]\gamma+\alpha_1(X,Z)(T-\gamma_1(X,Z))+\alpha_2(X)(T-\gamma_2(X))\]

Notice that this orthogonal score is a valid moment condition since it is zero in expectation. To see why, note that from our orthogonality argument above, $\mathbb{E}(\delta(X,Z)(T - \gamma_1(F)(X,Z)) ) = 0$ for all $\delta \in L^2(X,Z)$, and $\alpha_1$ is an element in $L^2(X,Z)$ (more on this in the next section). It immediately follows that $\mathbb{E}(\alpha_1(X,Z)(T-\gamma_1(X,Z)))=0$. Analogously, $\mathbb{E}(\alpha_2(X)(T-\gamma_2(X,Z)))=0$ by the fact that $\alpha_2$ is an element in $L^2(X)$. The identifying moment function is zero in expectation, as shown earlier. Furthermore, as shown in \citet{ichimura2022influence} and \citet{chernozhukov2022locally}, the orthogonal moment function (or score) is also an influence function (see Appendix B or \citet{ichimura2022influence} and \citet{chernozhukov2022locally}). To distinguish between this influence function characterization of the orthogonal moment function and the influence function adjustment above, \citet{ichimura2022influence} and \citet{chernozhukov2022locally} refer to the latter as the \enquote{first step influence function} to emphasize its role as a bias correction term for the first step estimation.

\subsection{Neyman Orthogonality and Multiple Robustness}

The orthogonal moment function we have constructed ought to satisfy two orthogonality properties. The first property, known as Neyman orthogonality, requires that varying the first step function away from its true underlying value should have no effect locally on the average orthogonal moment function. More formally (see equation 2.4 in \citet{chernozhukov2022locally}):

\begin{equation}
\frac{d}{d t} \mathbb{E}\left[\psi\left(W, \gamma_0+t \delta, \alpha_0, \theta\right)\right]=0 \quad \text { for all } \delta \in \Gamma \text { and } \theta \in \Theta
\end{equation}
where $\gamma_0$ is the value of $\gamma$ under the true underlying distribution of the data; $\delta$ represents a deviation of $\gamma$ away from $\gamma_0$; the scalar, $t$, is the size of the deviation; and the derivative above is evaluated at $t=0$. In a setting with multiple steps like ours, this derivative is taken with respect to $t$ for each first step function, holding the other first step functions fixed. 

Let us hold $\gamma_2$ fixed. Since the moment identifying function, $g(W,\gamma_1, \theta)$ is a continuous functional linear in the first step function $\gamma_1$ which lives on the Hilbert space, $\Gamma_1=L^2(X,Z)$, then by the Riesz Representation Theorem, there exist unique random variables, $\alpha_{01} \in \Gamma_1$ such that $g(W,\gamma_1, \theta)=\mathbb{E}(\alpha_{01} \eta(X,Z))$ for all $\eta(X,Z) \in \Gamma_1$. Since $\gamma_1 \in \Gamma_1$, it follows that $\alpha_{01}=\mathbb{E}[Y-\theta T|X,Z]=\mathbb{E}[Y-\theta T|\Gamma_1]$. Let $\gamma_{01}$ be the probability limit of $\hat{\gamma_1}$. It follows that:
\begin{equation}
 \begin{split}
 &\frac{d}{d t} \mathbb{E}\left[\psi\left(W, \gamma_{01}+t \delta, \alpha_0, \theta\right)\right]=\frac{d}{d t}\mathbb{E}[(Y-\theta T)\left(\gamma_{01}+t \delta\right)-(Y-\theta T) \gamma_2 \\
 &+ \left.\alpha_{01}\left(T-\gamma_{01}-t \delta\right)+\alpha_2\left(T-\gamma_2\right)\right] \\
  &=\mathbb{E}\left[(Y-\theta T) \delta-\alpha_{01} \delta\right]= \mathbb{E}[((Y-\theta T)-\alpha_{01})\delta]=0,
\text{  since } \alpha_{01}= \mathbb{E}[(Y-\theta T) \mid X, Z]
\end{split}
\end{equation}

If $\gamma_1$ is held fixed and we allow the identifying moment function to vary in $\gamma_2$, we find a similar result using the above argument. I omit the details for brevity. 

Another property which must be satisfied is the following: 

\[
\mathbb{E}\left[\phi\left(W, \gamma_0, \alpha, \theta\right)\right]=0 \text { for all } \theta \in \Theta \text { and } \alpha \in \mathcal{A}
\]

This follows from the arguments towards the end of the previous section.

Theorem 4 in \citet{chernozhukov2022locally} allows us to verify whether our orthogonal moment function is \enquote{multiply} robust. Multiple robustness, a generalization of double robustness from the case of one first step function to multiple first step functions, is a stronger requirement than Neyman orthogonality. Neyman orthogonality is a \enquote{local} property: varying the first step functions should not have an effect locally on the average moment function. Multiple (or double) robustness imposes a more stringent requirement: the outcome model or one or more of the first step functions may be misspecified, and the orthogonal score will still remain valid as an estimating equation. Theorem 4 of \citet{chernozhukov2022locally} provides a condition under which Neyman orthogonality and multiple (or double) robustness coincide, namely: the orthogonal moment function should be affine in the first step functions. Typically, verifying this condition entails inspecting the identifying moment function and the first step influence function adjustment. If both are affine in the first step function, then the orthogonal moment function is affine in the first step function and the equivalence between Neyman orthogonality and multiple robustness follows immediately by Theorem 4. 

We have already shown Neyman orthogonality and we can easily verify, by sheer inspection,  that both the identifying moment function and the influence function adjustment in our setting are affine in one first step function (say, $\gamma_1$), holding the other (say, $\gamma_2$) fixed. The multiple robustness of our orthogonal moment function, therefore, follows immediately by invoking Theorem 4 of \citet{chernozhukov2022locally}. The implication of this result is that the orthogonal moment function proposed here is not only robust to biases from the first step estimation, but it is also robust to misspecification of either some of the components in the first step estimation or the outcome model.

\subsection{Estimation}

\subsubsection{The Method of Moments Estimator via Cross-Fitting} \label{mom_est}
The method of moments estimator we present in this section combines the multiply robust orthogonal score constructed above with cross-fitting. The estimator is a root to the following debiased sample moments, which are the empirical analogue of population moment conditions based on the orthogonal score: 
\begin{align} \label{debiasedm}
\hat{\psi}(\theta)=\frac{1}{n} \sum_{l=1}^L \sum_{i \in I_{l}}(Y_i-\theta T_i)\left(\hat{\gamma}_{1 l^{-}}\right. \left.\hat{\gamma}_{2 l}\right)+\frac{1}{n} \sum_{l=1}^L \sum_{i \in I_{l}} \hat{\alpha}_{1 l}(T_i- \left.\hat{\gamma}_{1 l}\right)+\frac{1}{n} \sum_{l=1}^L \sum_{i \in I_{l}} \hat{\alpha}_{2 l}\left(T_i-\hat{\gamma}_{2 l}\right)=0
\end{align}

The double summation operator in the above display denotes a cross-fitting procedure, which is a generalization of sample-splitting. In this procedure, we first randomly partition the sample into $L$ folds: $I_l$ for $l=(1,2 \ldots L)$. For each fold, $I_l$ , we use the observations in the complement of this fold, ${I_l}^c$,  to estimate the Riesz representers, $\alpha_{1 l}$ and $\alpha_{2 l}$, and the first step functions, $\gamma_{1 l}$ and $\gamma_{2 l}$. Then given this set of estimates of $\alpha_{1 l}$,$\alpha_{2 l}$,$\gamma_{1 l}$ and $\gamma_{2 l}$, the sample moment function, $\hat{\psi}$, averages over the observations that are in the folds as in equation (\ref{debiasedm}). The debiased examiner IV estimator, $\hat{\theta}$, is a solution to these debiased sample moments.  

Cross-fitting is associated with several nice theoretical properties. The main reason for cross-fitting is that it obviates the need for Donsker conditions which are critical in classical semiparametric theory but are not known to hold in high-dimensional settings with machine learning first steps (see the discussion in Appendix B or the original references: \citet{chernozhukov2018double}, \citet{chernozhukov2022debiased} and \citet{chernozhukov2022locally}).  Like sample splitting (see \citet{angrist1999jackknife}), cross-fitting also eliminates ``own observation bias" \citep{chernozhukov2022locally}, which is a prominent issue in the examiner IV designs (see discussion in the Introduction). In our setting, cross-fitting further reduces bias from estimation of treatment propensities of examiners (see \citet{jochmans2023many}). In part addressing concerns regarding use of machine learning in IV estimation raised by \citet{angrist2022machine}, \citet{mikusheva2021many} observed that cross-fitting also reduces the dependence between flexible first stage estimation of optimal instrument and second step parametric estimation of an IV model. Arguably, this dependence does not really arise in our setting, since our estimation of the conditionally ignorable treatment propensity is based on exogenous covariates. 

For the estimation of $\gamma_1$ and $\gamma_2$, any regression learner such as LASSO, boosting, neural nets, random forests and other ML tools may be used, provided the learner converges fast enough (this is formalized in Assumption 3.2(2) in Section 3). These convergence rates have been ascertained for a wide range of machine learning first steps (see Section 3.2.3 in \citet{chernozhukov2024automatic} for references). On the other hand, the estimation of the Riesz representers is a bit more complicated and I devote the next subsection to its exposition.

\subsubsection{Automatic Estimation of the Riesz Representers}
In this section, I discuss estimation of the Riesz representers, which is arguably one of the most formidable challenges when working with locally robust semiparametric approaches. I describe an ``automatic" approach which I use in this paper to eschew some of the difficulties associated with estimating these functions in practice. 

In general, while estimators of Riesz representers can be obtained by estimating their components where the closed forms of these Riesz representers are available, directly estimating Riesz representers this way is not always a good idea. Directly estimating Riesz representers for conditional expectations, for example, entails working with some form of projection, while Riesz representers for an average treatment effect usually involve estimating propensity scores. Even average policy effects involve estimating a density ratio. As discussed in section 3.1, this direct estimation of Riesz representers is often hard or is marred by some complications including numerical instability of the estimates. In this work, I avoid these challenges by adopting a more recent and increasingly popular approach which estimates these Riesz representers in an \enquote{automatic} fashion. This approach is \enquote{automatic} in the sense that it only requires knowledge of the orthogonal score and data for construction.

In moderate dimensional settings where the number of parameters increases slowly with the sample size, the Riesz representers can be approximated by ``parametric sieves" and researchers may use estimation methods from the well-established literature on sieve estimation (see, for example, \citet{CHEN20075549} for a handbook-chapter treatment of  this broad literature on sieve estimation) but, for the purpose of the exposition in this paper, I work with regularized Riesz representers in a high dimensional setting. Particularly, I propose using LASSO to estimate the Riesz representers, focusing on estimation of $\alpha(X,Z)$ (analogous arguments apply to $\alpha(X)$). To this end, we start with the sample counterpart of the Gateaux derivative that characterizes Neyman orthogonality.  Holding $\gamma_2$ fixed, we obtain:

\[
\begin{aligned}
& \frac{d}{d t} \frac{1}{n-n_l} \sum_{l^{\prime} \neq l} \sum_{i \in l}(Y_i- \tilde{\theta}_{ll^{\prime}} T_i) \left(\tilde{\gamma}_{1ll^{\prime}} + t \delta - \gamma_2\right) + \alpha_1\left(T_i - \tilde{\gamma}_{1ll^{\prime}} - t \delta\right) - \alpha_2\left(T_i -\gamma_2 \right) \\
&=\frac{1}{n-n_l} \sum_{l^{\prime} \neq l} \sum_{i \in l}\left(Y_i-\tilde {\theta}_{ll^{\prime}} T\right) \delta-\alpha_1 \delta=\frac{1}{n-n_l} \sum_{l^{\prime} \neq l} \sum_{i \in l}\left((Y_i-\tilde {\theta}_{ll^{\prime}} T_i)-\alpha_1\right) \delta
\end{aligned}
\]
where $n_l$ is the total number of observations in $L_l$ (so the average is over observations not in $L_l$: $n-n_l$), whereas the index, $ll^{\prime}$, denotes observations not in $I_l$ and $I_{l^{\prime}}$. 

Notice that the last expression corresponds to the population moment condition, $\mathbb{E}[(Y-\theta )-\alpha_{01}) \delta]=0$. This orthogonality condition corresponds to $\alpha_{01}= \mathbb{E}[(Y-\theta T) \mid X, Z]$, which is consistent with what we obtained earlier. In the high dimensional setting that we consider here, I follow \citet{chernozhukov2022locally} and represent $\alpha_1(X,Z)$ as a linear combination of a dictionary of functions, $\rho^{\prime}b(X,Z)$, where $b(X,Z)=(b_1(X,Z) \ldots b_p(X,Z))^{\prime}$ \enquote{spans} the Hilbert space, $\Gamma$, and choose $\delta(X,Z)=b_j(X, Z)$  for some $j^{th}$ element of the dictionary. 

Our sample moment then becomes:
\[
\hat{\varphi}_\gamma(b_j ; \rho^{\prime}b(X,Z))=\frac{1}{n-n_l} \sum_{l^{\prime} \neq l} \sum_{i \in l}((Y_i-\tilde {\theta}_{ll^{\prime}} T_i)-\rho^{\prime} b(X_i,Z_i)) b_j(X_i, Z_i)
\]
Adding an $L1$ penalty or regularization, we obtain the LASSO estimator:
\[\hat{\alpha}_l(x,z)=\hat{\rho}^{\prime} b(x,z)
\]
where, for a suitably chosen penalty level, $r$:
\begin{equation} \label{lasso}
    \hat{\rho}=\operatorname*{argmin}_\rho \frac{1}{n-n_l} \sum_{l^{\prime} \neq l} \sum_{i \in l}((Y_i-\tilde {\theta}_{ll^{\prime}} T_i)-\rho^{\prime} b(X_i,Z_i)) b_j(X_i, Z_i)+
2 r \sum_{j=1}^p|\rho_j|
\end{equation}
for some initial estimate of $\theta$: $\tilde{\theta}_{l l^{\prime}}$.

To obtain this initial estimate, $\tilde{\theta}_{l l^{\prime}}$, we start by designating a number of folds, $L$, as in the cross-fitting procedure described earlier. Let $I_l$ and $I_{l^{\prime}}$ be two hold-out folds from the $L$ folds. Then we use observations that are not in $I_l$ and $I_{l^{\prime}}$ to estimate $\tilde{\theta}_{l l^{\prime}}$ using only the original identifying moment condition (see section 2.2 in \citet{chernozhukov2022locally} for an example of this construction). We then use this initial estimate in the LASSO estimator above, which averages over all observations not in $I_l$ as required for the debiased sample moments in equation (\ref{orthog}), to estimate the Riesz representers. As argued by \citet{chernozhukov2022locally}, this initial estimate does not affect the asymptotic distribution of the debiased examiner IV estimator because of Neyman orthogonality.

For the LASSO estimator in equation (\ref{lasso}), researchers have to \enquote{choose} the penalty term, $r$, and the dictionary of functions, $b(X,Z)$. The penalty term, $r$, can be computed via a cross-validation procedure described in Appendix A.1.1 of \citet{chernozhukov2022automatic}, but other choices are also possible (see some examples in \citet{chernozhukov2022locally}). In practice, $b(X,Z)$ could be a fully-interacted specification with all covariates and examiner fixed effects or types, polynomials of continuous covariates, interactions of all covariates and examiner types, and interactions among covariates, as in \citet{chernozhukov2022debiased}.

Instead of the LASSO estimator, we could also use a Generalized Dantzig Selector (DGS) suggested in \citet{chernozhukov2022debiased}. Other approaches for learning the Riesz representers from data involve minimizing a stochastic loss over more general function spaces in which the true Riesz representer is hypothesized to live. Leveraging critical radius theory, \citet{chernozhukov2020adversarial} provide an adversarial estimator of the Riesz representer over more general function spaces such as neural networks, random forests and reproducing kernel Hilbert spaces (in particular, the neural tangent kernel space). While this is more general than the LASSO approach adopted in this paper, the adversarial loss formulation is computationally harder. We stick to the LASSO approach and leave this and other more general approaches to future work.

\subsection{Asymptotic Theory}
The large sample theory for the method-of-methods estimator proposed in subsection \ref{mom_est} requires mild theoretical guarantees. First, we need the debiased sample moment function to converge at the rate of $\sqrt{n}$ to a sample average of the population orthogonal moment function. To show this, I prove mean square consistency conditions for $\hat{\gamma}$ and $(\hat{\alpha}_l, \Tilde{\theta}_l)$ and a rate condition on an interaction term. Second, I show the asymptotic distribution of the estimator. This section does not provide the statistical guarantees for the \enquote{automatic} estimators of Riesz representers from the previous section, as the generic mean-square convergence rates of such estimators have been sufficiently documented in \citet{chernozhukov2022automatic}. 


\begin{assumption} [Mean Square Consistency] \label{ass1}
 $(i)$ For each $i \in \{1,2\}$, $\hat{\gamma}_{i l}$ is a consistent estimator of ${\gamma_{0 i}}$ in the mean square sense such that $ \|\hat{\gamma}_{il}-\gamma_{0 i} \| \overset{p}{\to} 0$; $(ii)$ For each $i \in \{1,2\}$, $\hat{\alpha}_{i l}$ is a consistent estimator of ${\alpha_{0 i}}$ in the mean square sense such that $\|\hat{\alpha}_{i l}-\alpha_{0 i} \| \overset{p}{\to} 0$, where, in both $(1)$ and $(2)$, $\|a\|=\sqrt{\mathbb{E}[a(W)]}$ is the mean-square norm.
\end{assumption}

\begin{assumption} [Uniform convergence and convergence rates for first steps] \label{ass2}
$(i)$ $\theta_o \in \Theta^\mathrm{o}$, where $\Theta^\mathrm{o}$ is the interior of the parameter space, $\Theta$; and for each $i \in \{1,2\}$, $\hat{\alpha}_{il}$ converges uniformly over $\Theta^\mathrm{o}$; $(ii)$ For each $i \in \{1,2\}$, either $\hat{\alpha}_{il}$ or $\hat{\gamma}_{il}$ converge faster than $\sqrt{n}$ such that:
 \[\left\|\hat{\alpha}_l\left(\tilde{\theta}_l\right)-\alpha_0\left(\theta_0\right)\right\|\left\|\hat{\gamma}_l-\gamma_0\right\|  =O_p\left(\frac{1}{\sqrt{n}}\right)\]
\end{assumption}

\begin{theorem}[$\sqrt{n}$ convergence of the debiased sample moments] \label{thm1}
 Under assumptions \ref{ass1}  and \ref{ass2} and the mean square consistency conditions on $\hat{\gamma}$ and $(\hat{\alpha}_l, \Tilde{\theta}_l)$ in \citet{chernozhukov2022locally}, $\sqrt{n} \hat{\psi}\left(\theta_0\right)=\frac{1}{\sqrt{n}} \sum_{i=1}^n \psi\left(W_i, \theta_0, \gamma_0, \alpha_0\right)+o_p(1)$ 
\end{theorem}

Theorem \ref{thm1} is an asymptotic orthogonality result. In essence, it asserts that the estimation of $\hat{\gamma}$ does not have an asymptotic effect on the orthogonal moment function. The theorem also reaffirms the validity of the orthogonal moment function as an influence function, and clarifies the conditions under which this holds. This follows from the fact that any asymptotically linear and locally regular estimator can be represented as an average of an influence function (see Appendix B or \citet{kosorok2008introduction}). For simplicity, I do not make a distinction between the initial estimate of $\theta$ obtained via the nested cross-fitting procedure in equation (\ref{lasso}) and the initial estimate, $\Tilde{\theta}_l$, in this theorem because asymptotically they are equivalent.

$\Gamma_1$ and $\Gamma_2$ encode all restrictions on the distributions of the data, and since $\Gamma_1=L^2(X,Z)$ and $\Gamma_2=L^2(X)$, then the distributions in this paper are general and unrestricted. From classical semiparametric theory, for general unrestricted distributions, the tangent space is the entire Hilbert space (see Theorem 4.4 in \citet{tsiatis2006semiparametric}) and therefore the influence function in Theorem \ref{thm1} coincides with the efficient influence function. 

To prove Theorem \ref{thm1}, I invoke Lemma 8 in \citet{chernozhukov2022locally} which holds under some mean-square consistency conditions. My proof strategy, therefore, consists mainly of showing that these mean-square consistency conditions hold for the non-parametric first steps in this paper. The proof is rather tedious and is relegated to Appendix A.

Theorem \ref{thm1} yields an important corollary: the asymptotic normality of the estimator. This corollary forms the basis for inference. We, however, need two additional assumptions. 

Let $\sigma_{01}^2(X,Z)=\mathbb{E}\left[\left(T-\gamma_{01}(X,Z)\right)^2 \mid X, Z \right]$ and $\sigma_{02}^2(X)=\mathbb{E}\left[\left(T-\gamma_{02}(X)\right)^2 \mid X \right]$. 
\begin{assumption} [Boundedness] \label{ass3}
$(i)$ For $i \in \{1,2\}$, $\alpha_{0 i}$ and $\alpha_{i l}$ are bounded; $(ii)$ $\sigma_{01}^2(X,Z)$ and $\sigma_{02}^2(X)$ are bounded, and $\mathbb{E}[(g(W, \alpha,\gamma_0, \theta_0)^2] < \infty$
\end{assumption}  

We need another assumption that ensures that $\Bar{Q}_n=\partial \hat{\psi}(\bar{\theta}) / \partial \theta \rightarrow_p Q=\partial \hat{\psi}(\bar{\theta}) / \partial \theta_0$ for any $\bar{\theta} \rightarrow_p \theta_0$. Treating $\gamma$ as one function rather than a difference of two functions, we impose this assumption, which is an adaptation of Assumption 5 in \citet{chernozhukov2022locally}:
\begin{assumption}
   $Q$ exists and there is a neighborhood $\mathcal{N}$ of $\theta_0$ such that $\left.i\right)\left\|\hat{\gamma}-\gamma_0\right\| \rightarrow_p 0$; ii) for all $\left\|\gamma-\gamma_0\right\|$ and $\left\|\alpha-\alpha_0\right\|$ small enough $\psi\left(W, \gamma, \alpha, \theta\right)$ is differentiable in $\theta$ on $\mathcal{N}$ with probability approaching one; and there is $C>0$ and $d\left(W, \gamma, \alpha\right)$ such that $\mathbb{E}\left[d\left(W, \gamma, \alpha\right)\right] \leq C$ and such that for $\theta \in \mathcal{N}$ and $\left\|\gamma-\gamma_0\right\|$ and $\left\|\alpha-\alpha_0\right\|$ small enough
$$
\left|\frac{\partial \psi\left(W, \gamma, \alpha, \theta\right)}{\partial \theta}-\frac{\partial \psi\left(W, \gamma, \alpha, \theta_0\right)}{\partial \theta}\right| \leq d\left(W, \gamma, \alpha\right)\left|\theta-\theta_0\right|^{1 / C}$$
(iii) For each $\ell=1, \ldots, L, \int\left|\partial \psi\left(w, \hat{\gamma}_{\ell}, \theta_0\right) / \partial \theta-\partial \psi\left(w, \gamma_0, \theta_0\right) / \partial \theta\right| F_0(d w) \xrightarrow{p}$ 0.
\end{assumption}

\begin{corollary}[Asymptotic Normality of the Method of Moments Estimator] \label{coro}
Under assumptions 3.1 to 3.4, the MoM estimator is asymptotically normal: 
$$
\sqrt{n}\left(\hat{\theta}-\theta_0\right) \xrightarrow{d} N(0, V) \quad \text { and } \quad \hat{V} \xrightarrow{p} V
$$
where $V=Q^{-2}\Var[\psi^2]=Q^{-2}\mathbb{E}[\psi^2]$ and $\hat{V}$ is a consistent estimator of $V$.
\end{corollary}

The proof of this corollary is also relegated to Appendix A for the interested reader. Heuristically, the expression of the variance comes from the fact that the orthogonal moment function is the efficient influence function, combined with standard arguments from semiparametric theory. In particular, the influence function for the examiner IV estimator is proportional to the influence function for the average moment function (cf: section 21.1.4 in \citet{kosorok2008introduction}). Therefore, the variance of the examiner IV estimator is also proportional to the variance of the average moment function which, in turn, is the second moment of its influence function. With this asymptotic normality result, construction of confidence intervals and test statistics follows in the standard manner.

Corollary \ref{coro} presumes $\sqrt{n}$ consistency of the estimator, $\hat{\theta}$.  Primitive conditions for this consistency are mild and are provided in Appendix F of the Supplemental Material in \citet{chernozhukov2022locally}. In a nutshell, consistency of the estimator requires, \textit{inter alia}, the mean square consistency conditions of Assumption 1 in \citet{chernozhukov2022locally}, which I have proved for Theorem 3.1 in our setting; the rate condition in Assumption 2 in \citet{chernozhukov2022locally}, which I have also proved for Theorem 3.1; and standard consistency assumptions, such as compactness of the parameter space, from \citet{NEWEY19942111}. 

\section{Conclusion}
This paper has provided a new framework for estimation and inference in examiner IV designs when there are many examiners and the examiner IV is valid conditional on possibly a large number of covariates relative to the sample size. The estimation method permits applied researchers to use a wide range of machine learning tools such as LASSO, random forests, neural networks and Dantzig in the first step estimation of the examiner IV. Based on the locally robust semiparametric theory of \citet{chernozhukov2022locally} and \citet{ichimura2022influence}, I provide mild theoretical guarantees for such machine learning first steps and the two-step examiner IV estimator, including root-n consistent estimation of the examiner IV estimator with mean-square consistency conditions on the first steps. 

As an avenue of future research, it would also be interesting to explore locally robust semiparametric approaches for settings where examiners administer more than one treatment, such as in \citet{kamat2023identification}. Furthermore, the method developed in this paper presumes that the canonical identification assumptions of these examiner IV designs obtain, but it might be instructive to extend this framework to settings where some of these assumptions do not hold, a la \citet{frandsen2023judging}. 

\appendix
\section{Appendix A: Proofs of the Asymptotic Theory}
\subsection{Proof of Theorem 3.1}
I set off by proving the mean square consistency conditions of Assumption 1 and the rate condition in Assumption 2 for Lemma 8 in Section 6 of \citet{chernozhukov2022locally}. In all the proofs below, the constant, $C$, may be different in each case. 

Assumption 1(i) of Lemma 8 is:
\[
\int \left\|g\left(w, \hat{\gamma}_{\ell}, \theta_0\right)-g\left(w, \gamma_0, \theta_0\right)\right\|^2 F_0(d w) \xrightarrow{p} 0
\]
Treating $\hat{\gamma}_l$ as one function instead of a difference of two functions (proof goes through either way, but this is more analytically convenient):
\[
\begin{aligned} 
&g\left(W, \hat{\gamma_1}, \theta_0\right)-g\left(w, \gamma_0, \theta_0\right) =\left(Y-\theta_0 T\right) (\hat{\gamma}_l-\gamma_0) 
\end{aligned}
\]

Observe that since the identifying moment function is a continuous linear functional (and, therefore, mean-square continuous), it follows that:
\[
\|(Y-\theta_0 T) (\hat{\gamma}_l-\gamma_0)\|^2 \leq C\left\|\hat{\gamma}_l-\gamma_0\right\|^2
\]
here $C$ is a constant, and the norm on the right-hand side is the mean square norm.

Taking the average of the left-hand side under the distribution, $F_0$, does not change the inequality. Therefore:
\[
\int \|(Y-\theta_0 T) (\hat{\gamma}_l-\gamma_0)\|^2 F_0(d w) \leq C\left\|\hat{\gamma}_l-\gamma_0\right\|^2
\]
By assumption \ref{ass1}, the right-hand side is zero, and the result follows.

Assumption (ii) of Lemma 8 in \citet{chernozhukov2022locally} imposes the following restriction: 
\[
\int\left\|\phi\left(w, \hat{\gamma}_{\ell}, \alpha_0, \theta_0\right)-\phi\left(w, \gamma_0, \alpha_0, \theta_0\right)\right\|^2 F_0(d w) \xrightarrow{p} 0
\]

In our setting, 
\[
\phi\left(w, \alpha_{01}, \alpha_{02}, \hat{\gamma}_{1 l}, \hat{\gamma}_{2 l}, \theta_0\right)=\alpha_{01}\left(T-\hat{\gamma}_{1 l}\right)-\alpha_{02}\left(T-\hat{\gamma}_{2 l}\right)
\]
and 
\[
\phi\left(w, \alpha_{01}, \alpha_{02}, \gamma_{01}, \gamma_{02}, \theta_0\right)=\alpha_{01}\left(T-\gamma_{01}\right)-\alpha_{02}\left(T-\gamma_{02}\right)
\]

Then
\begin{align*}
& \| \phi\left(w, \alpha_{01}, \alpha_{02}, \hat{\gamma}_{1 l}, \hat{\gamma}_{2 l},  \theta_0\right)- \phi\left(w, \alpha_{01}, \alpha_{02}, \gamma_{01}, \gamma_{02}, \theta_0\right) \|^2\\
&= \| \alpha_{01}\left(T-\hat{\gamma}_{1 l}\right)-\alpha_{02}\left(T-\hat{\gamma}_{2 l}\right)-\alpha_{01}\left(T-\gamma_{01}\right)+\alpha_{02}\left(T-\gamma_{02}\right) \|^2 \\
& =\left\|\alpha_{01}\left(\gamma_{01}-\hat{\gamma}_{1 l}\right)+\alpha_{02}\left(\hat{\gamma}_{2 l}-\gamma_{02}\right)\right\|^2 \\
& \leq\left\|\alpha_{01}\left(\gamma_{01}-\hat{\gamma}_{1 l}\right)\right\|^2+\| \alpha_{02}\left(\hat{\gamma}_{22}-\gamma_{02} \|^2\right. \\
&\leq C_1^2\left\|\gamma_{01}-\hat{\gamma}_{1 l}\right\|^2+C_2^2\left\|\hat{\gamma}_{2 l}-\gamma_{02}\right\|^2 \\
\end{align*}
 Where the penultimate inequality is an application of the triangle inequality and the last inequality follows from mean-square continuity.
 
Then it follows that:
\begin{align*}
&\int \| \left(\phi\left(w, \hat{\gamma}_l, \alpha_{0,}, \alpha_0\right)-\phi\left(w, \gamma_{0,} \alpha_{0,}, \theta_0\right) \|^2\right. \\
& \leq  C_1^2\left\|\gamma_{01}-\hat{\gamma}_{1 l}\right\|^2+C_2^2\left\|\hat{\gamma}_{2 l}-\gamma_{02}\right\|^2  
\end{align*}

The result follows by assumption 4.1.

Assumption 1(iii) in \citet{chernozhukov2022locally} requires:
\[
\int\left\|\phi\left(w, \gamma_0, \hat{\alpha}_{l}, \tilde{\theta}_{l}\right)-\phi\left(w, \gamma_0, \alpha_0, \theta_0\right)\right\|^2 F_0(dw) \xrightarrow{p} 0
\]
Suppressing the dependence of the Riesz representers and their estimators on $X$ and $Z$ for notational convenience and letting them depend on only $\theta_0$ and $\tilde{\theta_l}$ respectively:

\begin{align*}
&\int\left\| \phi(w, \gamma_0, \hat{\alpha}_{l}, \tilde{\theta}_{l})-\phi(w, \gamma_0, \alpha_0, \theta_0)\right\|^2 F_0(dw)\\
&= \int \left\| \hat{\alpha}_{1l}(\tilde{\theta}_l)(T-\gamma_{01})+\hat{\alpha}_{2l}(\tilde{\theta}_l)(T-\gamma_{02})-\alpha_{01}(\theta_0)(T-\gamma_{01})-\alpha_{02}(\theta_0)(T-\gamma_{02}) \right\|^2 F_0(dw) \\
& \leq \int\left\|(\hat{\alpha}_{1l}(\tilde{\theta}_l)-\alpha_{01}(\theta_0))(T-\gamma_{01})\right\|^2F_0(dw)+\int \left\| \alpha_{2l}(\hat{\theta}_l) -\alpha_{02}(\theta_0) (T-\gamma_{02}) \right\|^2 F_0(dw) \\
&= \int(T-\gamma_{01})^2\left\|\hat{\alpha}_{1l} (\tilde{\theta}_l)-\alpha_{01}(\theta_0 ) \right\|^2 F_0(dw) + \int(T-\gamma_{02})^2\left\|\hat{\alpha}_{2l} (\tilde{\theta}_l)-\alpha_{02}(\theta_0 ) \right\|^2 F_0(dw)\\
&\leq C_1 \left\|\hat{\alpha}_{1l} (\tilde{\theta}_l)-\alpha_{01}(\theta_0 ) \right\|^2 + C_2 \left\|\hat{\alpha}_{2l} (\tilde{\theta}_l)-\alpha_{02}(\theta_0 ) \right\|^2
\end{align*}

Take $C_1 \left\|\hat{\alpha}_{1l} (\tilde{\theta}_l)-\alpha_{01}(\theta_0 ) \right\|^2$ and observe that the problem of $C_2 \left\|\hat{\alpha}_{2l} (\tilde{\theta}_l)-\alpha_{02}(\theta_0 ) \right\|^2$ is completely symmetric:
\begin{align*}
&\left\|\hat{\alpha}_{1l} (\tilde{\theta}_l)-\alpha_{01}(\theta_0 ) \right\|^2=\left\|\hat{\alpha}_{1l} (\tilde{\theta}_l)-\hat{\alpha}_{1l}(\theta_0)+\hat{\alpha}_{1l}(\theta_0)-\alpha_{01}(\theta_0 ) \right\|^2 \\
& \leq \left\| \hat{\alpha}_{1l}(\tilde{\theta_l})-\hat{\alpha}_{1l}(\theta_0) \right \|^2 + \left\| \hat{\alpha}_{1l}(\tilde{\theta_0})-\alpha_{10}(\theta_0) \right \|^2
\end{align*}

where the last inequality follows from the triangle inequality. By assumption 3.2, $\tilde{\theta}_l$ converges in probability to $\theta_0$, and by the continuous mapping theorem, $\hat{\alpha_{1l}}(\tilde{\theta_l})$ also converges in probability to $\hat{\alpha_{1l}}(\theta_0)$. It follows that $ \left\| \hat{\alpha}_{1l}(\tilde{\theta_l})-\hat{\alpha}_{1l}(\tilde{\theta_0}) \right \|^2$ goes to zero. By the uniform convergence property in assumption 3.2, we know for any $\theta_0 \in \Theta^\mathrm{o}$, $\hat{\alpha}_{1l}(\theta_0)$ converges to $\alpha_{10}(\theta_0)$. Therefore, $C_1 \left\|\hat{\alpha}_{1l} (\tilde{\theta}_l)-\alpha_{01}(\theta_0 ) \right\|^2$ converges to zero and, by symmetry, $C_2 \left\|\hat{\alpha}_{2l} (\tilde{\theta}_l)-\alpha_{02}(\theta_0 ) \right\|^2$ also converges to zero. It follows, therefore, that as desired:
\[
\int\left\|\phi\left(w, \gamma_0, \hat{\alpha}_{l}, \tilde{\theta}_{l}\right)-\phi\left(w, \gamma_0, \alpha_0, \theta_0\right)\right\|^2 F_0(dw) \xrightarrow{p} 0
\] 


Let  $\hat{\Delta}_l(w)=\phi\left(w, \hat{\gamma}_l, \hat{\alpha}, \tilde{\theta}_l\right)-\phi\left(w, \gamma_0, \hat{\alpha}_l, \tilde{\theta}_l\right)-\phi\left(w, \hat{\gamma}_l, \alpha_0, \theta_0\right)+ \phi\left(w, \gamma_0, \alpha_0, \theta_0\right)$. This is what \citet{chernozhukov2022locally} call ``an interaction term". \citet{chernozhukov2022locally} impose a rate condition on this interaction term. I prove that the rate condition in Assumption 2(i) in \citet{chernozhukov2022locally} holds in this setting, namely:
\[
\sqrt{n} \int \hat{\Delta}_{l}(w) F_0(d w) \xrightarrow{p} 0, \quad \int\left\|\hat{\Delta}_{\ell}(w)\right\|^2 F_0(d w) \xrightarrow{p} 0
\]

Treating $\gamma$ as one first step function rather than a difference of two functions to avoid notational clutter:

\begin{align*}
&\int \hat{\Delta}_{\ell}(w) F_0(dw) \\
&= \int(\hat{\alpha}_l(\tilde{\theta}_l)(T-\hat{\gamma}_l)-\hat{\alpha}_l(\tilde{\theta}_l)(T-\gamma_0))-\alpha_0(\theta_0)(T-\hat{\gamma}_l)+\alpha_0(\theta_0)(T-\gamma_0) F_0(dw) \\
&= \int (\hat{\alpha}_l(\tilde{\theta}_l)-\alpha_0(\theta_0))(\gamma_0-\hat{\gamma}_l) F_0(dw) \leq \left\|\hat{\alpha}_l(\tilde{\theta}_l)-\alpha_0(\theta_0) \right \| \|\hat{\gamma}_l-\gamma_0 \| \\
&= O_p\left(\frac{1}{\sqrt{n}}\right)
\end{align*}

Where the inequality follows from applying the Cauchy-Schwarz inequality and the last equality follows from assumption 3.2(ii). This proves $\sqrt{n} \int \hat{\Delta}_{\ell}(w) F_0(d w) \xrightarrow{p} 0$.

We now need to show  $\int\left\|\hat{\Delta}_{\ell}(w)\right\|^2 F_0(d w) \xrightarrow{p} 0$. But this follows easily from  above as follows: 

\[
\int \|\hat{\Delta}_l(w)\|^2 F_0(d w)=\int\left\|\left(\alpha_0\left(\theta_0\right)-\hat{\alpha}_l\left(\tilde{\theta}_l\right)\right)\left(\gamma_l-\gamma_0\right)\right\|^2 F_0 (dw) \leq C\left\|\gamma_l-\gamma_0\right\|^2 \xrightarrow{p} 0
\]

Where the last inequality from the mean-square continuity of the influence function adjustment.

Assumption 3(i) and (ii) of Lemma 8 in \citet{chernozhukov2022locally} are satisfied in our setting. Assumption 3(i) readily follows from the assumption that $\tilde{\theta}_l$ converges to $\theta_0$ and the argument that $\hat{\alpha}_l (\theta_l)$ converges to $\alpha_0(\theta_0)$ which we have established when verifying Assumption 1(iii) above combined with the fact that at the true values of the first step functions, the influence function is zero in expectation. Assumption 3(ii), which requires that the orthogonal moment function is affine in the first step function, is already satisfied in this setting. As I have argued earlier, the orthogonal moment function is affine in the first steps.

Since we have verified all the three assumptions for Lemma 8 in \citet{chernozhukov2022locally}, the conclusion of Lemma 8 immediately follows, namely:

\[\sqrt{n} \hat{\psi}\left(\theta_0\right)=\frac{1}{\sqrt{n}} \sum_{i=1}^n \psi\left(W_i, \theta_0, \gamma_0, \alpha_0\right)+o_p(1) \]

\subsection{Proof of Corollary 3.1}
The proof proceeds by combining standard mean value expansion arguments in classical semiparametric theory and Theorem 3.1. 

By mean value expansion of the debiased sample moments around $\theta_0$ and multiplying both sides by $\sqrt{n}$:

\begin{align*}
&0=\hat{\psi}\left(\theta_0\right)+\frac{\partial}{\partial{\theta}}{\psi(\bar{\theta})(\hat{\theta}-\theta_0)} \\
&=\sqrt{n}\hat{\psi}\left(\theta_0\right)+\frac{\partial}{\partial{\theta}}{\hat{\psi}(\bar{\theta})\sqrt{n}(\hat{\theta}-\theta_0)}
\end{align*}


 It follows that:
 \[
\sqrt{n}(\hat{\theta}-\theta_0)=-(\frac{\partial}{\partial{\theta}}\hat{\psi})^{-1}(\bar{\theta})\sqrt{n}\hat{\psi}(\theta_0) \]

Let $\Bar{Q}_n=\frac{\partial}{\partial \theta}\hat{\psi}(\Bar{\theta})$ and $Q=\frac{\partial}{\partial \theta}\hat{\psi}(\theta_0)$. Then, with the aid of Assumptions 3.3 and 3.4:
\begin{align*}
    &\sqrt{n}(\hat{\theta}-\theta_0)=-(\bar{Q}^{-1}_n-Q^{-1}+Q^{-1})\sqrt{n}\hat{\psi}(\theta_0)\\
    &= -Q^{-1}\sqrt{n}\hat{\psi}(\theta_0)-(\bar{Q}^{-1}_n-Q^{-1})\sqrt{n}\hat{\psi}(\theta_0)\\
    &= -Q^{-1}\sqrt{n}\hat{\psi}(\theta_0)-o_p(1)O_p(1) \to \mathcal{N}(0,V)
\end{align*}
where $V=Q^{-2} \Var(\psi)=Q^{-2}\mathbb{E}[\psi^2]$ since $\mathbb{E}[\psi]=0$.

Notice that the conclusion follows from applying the central limit theorem, Slutsky's lemma and the fact that $\sqrt{n} \hat{\psi}\left(\theta_0\right)=\frac{1}{\sqrt{n}} \sum_{i=1}^n \psi\left(W_i, \theta_0, \gamma_0, \alpha_0\right)+o_p(1)$ from Theorem 3.1.

The estimator for $V$ is:
\[\hat{V}=\widehat{Q}^{-2} \sum_{l=1}^L \sum_{i \in I_{l}} \hat{\psi}(\hat{\theta})\]
where $\widehat{Q}=\sum_{l=1}^L \sum_{i \in I_{l}} \frac{\partial}{\partial{\theta}}\hat{\psi}(\theta)=\sum_{l=1}^L \sum_{i \in I_{l}}T_i \hat{\gamma}_{i l}$.

The consistency of $\hat{V}$ as an estimator of $V$ follows from the proof of Theorem 3 in \citet{chernozhukov2022automatic}.

\section{Appendix B: A Glimpse of Locally Robust Semiparametric Theory}

For a curious reader not familiar with the theory used in this paper, I give an informal sketch of the generic locally robust semiparametric theory (see \citet{chernozhukov2022locally} and \citet{ichimura2022influence} for further exposition), juxtaposing the theory against classical semiparametric theory. Building on the work of \citet{newey1994asymptotic}, the locally robust semiparametric approach adapts classical semiparametric theory to modern practice of statistics and econometrics, so it is instructive to have a basic grasp of the key elements of classical semiparametric theory in order to appreciate the locally robust approach. 

Semiparametric theory is about statistical models that are indexed by a Euclidean parameter and an infinite-dimensional nuisance parameter. Semiparametric theory generally applies to parameters i) that can asymptotically be represented as a sample average of some function called an influence function and a vanishingly small term (asymptotically linear); and (ii) whose limiting distribution is invariant under local pertubations to a sequence of distributions (regular).  Many estimators in econometrics and statistics are asymptotically linear and regular. In classical semiparametric theory, the key ingredients are parametric submodels, influence functions and tangent spaces (see \citet{van_der_Vaart_1998} for a book-chapter treatment or \citet{bickel1993efficient}, \citet{kosorok2008introduction} and \citet{tsiatis2006semiparametric} for a monograph treatment of semiparametric theory).

The first ingredient in semiparametric theory are parametric submodels, which may be construed as a conceptual device for approximating the infinite-dimensional nuisance parameter (see \citet{tsiatis2006semiparametric}). A parametric submodel (also known as a ``path") is the distribution of the data when the true underlying distribution is ``perturbed"  locally in the direction of another distribution. These submodels ought to be ``smooth enough" in the sense of being differentiable in quadratic mean (see \citet{van_der_Vaart_1998} for definition). If the parameter of interest is regular, then indexing it by this parametric submodel does not change its limiting distribution. 

The second ingredient are tangent spaces, which are constructed from parametric submodels. In semiparametric models, tangent spaces are the linear span (or the closure of that span) of scores of these parametric submodels where, analogous to the parametric case, the score is a derivative of the log of the parametric submodel in the direction of the alternative distribution.  Construction of a tangent space, therefore, requires characterizing the corresponding parametric submodels. This tangent space is a subspace of a Hilbert space, the $L^2$ space.

The last ingredient is an influence function. An influence function is derived either via a stochastic expansion of a given estimator or by taking a Gateuaux derivative of the parameter indexed by a parametric submodel where the true underlying distribution is ``perturbed'' locally in the direction of a point mass and the derivative is taken in the direction of this point mass. In the stochastic expansion case, an asymptotically linear estimator will, by definition, be a sample average of this influence function and some negligible term.

When the estimator is a function of a plug-in estimate of an infinite dimensional object, further work is required to obtain an estimator that is a sample average of an influence function and a negligible term. The negligible term is often obtained under additional assumptions, including stochastic equicontinuity (see \citet{tsiatis2006semiparametric}). This stochastic equicontinuity is satisfied when the function class for the parameter (as a function of the first step nuisance object) is Donsker (roughly, not very \enquote{complex}) and an asymptotic orthogonality condition holds (see page 405 in \citet{kosorok2008introduction}). In most settings, it is natural to impose these Donsker conditions on the nuisance functions themselves. For machine learning first steps, however, these conditions are not known to hold (see, inter alia, \citet{chernozhukov2018double}).

 The semiparametric efficiency bound, defined as the lowest possible variance in a given semiparametric model, is the second moment of a unique efficient influence function, where the efficient influence function is a projection of any influence function onto the tangent space. When there are restrictions on the semiparametric model, the tangent space is a proper subspace of the Hilbert space. Absent any such restrictions, the tangent space is the entire Hilbert space of mean-zero functions. In this case, the influence function is unique and is also the efficient influence function. 

The locally robust semiparametric framework proposed by \citet{chernozhukov2022locally} improves upon the classical formulation in a number of respects. In particular, the framework does not require Donsker conditions on the function class for nuisance functions. Cross-fitting, which is a generalization of sample splitting, eliminates the need for such Donsker conditions. This is important when machine learning first steps are used, insofar as it is not known whether machine learning estimators satisfy such conditions.  
The key ingredient of this approach is what is known as an orthogonal moment function. The orthogonal moment function consists of the identifying moment function and an influence function adjustment. It turns out that this orthogonal moment function is an influence function of the parameter of interest. As shown in \citet{ichimura2022influence} and \citet{chernozhukov2022locally}, for a given parametric submodel, $F_\tau=(1-\tau) F_0+\tau H$, where $H$ is a general unrestricted distribution:

$$
\frac{\partial}{\partial \tau} \theta\left(F_\tau\right)=\int\left[g (W, \gamma, \theta)\right] H(d w)+\int \phi\left(w, \gamma_0, \alpha_0\right) H(d w)$$

If $H$ is a point mass, this is an influence function for the estimand. $g (W, \gamma, \theta)$ is an identifying function, the function such that $\theta_0$ solves $\mathbb{E}(g (W, \gamma, \theta))=0$. Since there are no restrictions on the semiparametric model, the  tangent space is the entire Hilbert space, thereby obviating construction of the tangent space, and the influence function coincides with the efficient influence function. Since we do not need to characterize parametric submodels in order to construct a tangent space, this nonparametric approach allows for model-free inference, which avoids possible model misspecification due to construction of a wrong tangent space. 

The orthogonal moment function needs to satisfy two key properties, namely: $\frac{d}{d t} E\left[\psi\left(W, \gamma_0+t \delta, \alpha_0, \theta\right)\right]=0 \quad$ for all $\delta \in \Gamma$ and $\theta \in \Theta$ where $\delta$ is a possible direction of deviation of $\gamma(F)$ away from its true value, $\gamma_0$ with $t$ representing the size of the deviation; and $E\left[\phi\left(W, \gamma_0, \alpha, \theta\right)\right]=0$ for all $\theta \in \Theta$ and $\alpha \in \mathcal{A}$. 

The term, $\phi\left(w, \gamma_0, \alpha_0\right)$, above is an influence function adjustment. This term corrects or ``adjusts" for the local effect of the first step nuisance parameter when the underlying distribution is perturbed locally. It accounts for the presence of the estimate of the first step nuisance parameter in the moment functions and ensures the first step estimation has no first order effect on the moments (see, for example, \citet{ichimura2022influence}, \citet{chernozhukov2021automatic} \citet{chernozhukov2022debiased} and \citet{chernozhukov2022locally} for a thorough and more technical exposition).

Under some assumptions, the influence function above can be represented as a product of a Riesz representer and a nonparametric residual function. The Riesz representer nomenclature follows from the Riesz representation theorem: any continuous linear functional, a real-valued mapping on a Hilbert space, can be represented as an inner product of a unique element in the Hilbert space and any element of that Hilbert space. The unique element is known as the Riesz representer. \citet{newey1994asymptotic} pioneered this Riesz representer framework and derived results for cases where the first step is a conditional expectation or density, and this framework has been generalized to modern high dimensional settings by \citet{chernozhukov2022locally}, \citet{ichimura2022influence}, \citet{chernozhukov2024automatic}, among other recent works. The key requirement for this framework is the following orthogonality condition:
\begin{equation} \label{eq1}
E_F[\delta(X) \lambda(W, \gamma(F)(X))]=0 \quad \text{for all} \quad \delta \in \Gamma
\end{equation}
 
where $\lambda(W, \gamma(F)(X))$ is a scalar nonparametric residual function. When equation (\ref{eq1}), dubbed the "exogenous" orthogonality condition in \citet{ichimura2022influence} (see also equation 2.10 in \citet{chernozhukov2022locally}) is satisfied, then the influence function adjustment can be represented thus:
\begin{equation}
\phi(w, \gamma, \alpha, \theta)=\alpha(x, \theta) \lambda(w, \gamma(x))   
\end{equation}
where $\alpha(x, \theta)$ is a Riesz representer and $\lambda(w, \gamma(x))$ is a nonparametric residual function.

This construction of the influence function is central to the approach developed in the main text of this paper.

\printbibliography
\end{document}



