\begin{document}
\affiliation{$$_affiliation_$$}
\title{GMP-ATL: Gender-augmented Multi-scale Pseudo-label Enhanced Adaptive Transfer Learning for Speech Emotion Recognition via HuBERT}
\maketitle

\begin{abstract}
The continuous evolution of pre-trained speech models has greatly advanced Speech Emotion Recognition (SER). 
However, there is still potential for enhancement in the performance of these methods. 
In this paper, we present GMP-ATL (Gender-augmented Multi-scale Pseudo-label Adaptive Transfer Learning), a novel HuBERT-based adaptive transfer learning framework for SER. 
Specifically, GMP-ATL initially employs the pre-trained HuBERT, implementing multi-task learning and multi-scale k-means clustering to acquire frame-level gender-augmented multi-scale pseudo-labels. 
Then, to fully leverage both obtained frame-level and utterance-level emotion labels, we incorporate model retraining and fine-tuning methods to further optimize GMP-ATL. 
Experiments on IEMOCAP show that our GMP-ATL achieves superior recognition performance, with a WAR of 80.0\\end{abstract}



\section{Introduction}

\noindent
As one of the crucial integral elements in realizing human-computer interaction, speech emotion recognition (SER) aims to comprehend and categorize emotions conveyed through human speech, which has been extensively applied in diverse practical domains \cite{hci,business,healthcare}. 

In recent years, the remarkable advancements in machine learning and deep learning technologies have significantly accelerated the progress of SER.
By constructing robust deep neural networks such as convolutional neural networks and recurrent neural networks \cite{li2019improved,ghriss2022sentiment,TIMNet} based models, these methods can achieve improved recognition performance compared to traditional SER approaches \cite{6023178,1326051,5674019}. 
However, due to many factors such as the inherent variability in emotional expression of human speech \cite{kim2017towards,pan2024msac} and the scarcity of available labeled speech emotion data resulting from difficulties in data collection and annotation \cite{Chen,Gat}, achieving accurate and reliable SER continues to pose challenges.


{
\let\thefootnote\relax
\footnote{$*$ denotes equal contributing.}
\footnote{$\dagger$ denotes the corresponding author.}
\footnote{Work done at Ximalaya Inc.}
}

To tackle this issue, numerous researchers \cite{Morais,yi2023exploring,pan2024gemo,hu2023joint} have attempted to leverage transfer learning by incorporating large-scale pre-trained speech models such as HuBERT \cite{hubert}, Wav2vec 2.0 \cite{wav2vec2}, and WavLM \cite{wavlm} into speech emotion modeling.
For instance, 
Morais \emph{et al}. \cite{Morais} presented a modular end-to-end Upstream + Downstream SER architecture, which facilitates the integration of pretrained, fine-tuned, and averaged Upstream models. 
Pan \emph{et al}. \cite{pan2024gemo} proposed GEmo-CLAP, a novel contrastive cross-modality pretraining framework which leverages audio and text pre-trained models with the guidance of natural language supervision, aiming to enhance emotion recognition performance.
Hu \emph{et al}. \cite{hu2023joint} introduced a joint network combining the pre-trained Wav2Vec 2.0 model and a separate spectrum-based model to achieve SER. 

Nevertheless, despite achieving commendable results, these methods typically rely solely on utterance-level emotion labels as training objectives. However, they normally overlook the local information within each utterance that could provide valuable complementary details for modeling speech emotion.
Additionally, although some approaches \cite{fayek2017evaluating,xia2021temporal,mao2020eigenemo,feng2022semi} attempted to incorporate local information by constructing emotional pseudo-labels, there is still room for improvement in both the quality of these pseudo-labels and the employed modeling methods. 
Therefore, due to the aforementioned issues, the development of accurate, robust, and reliable SER methods remains an open and challenging task.

Hence in this study, we introduce a novel SER framework, GMP-ATL (HuBERT-based Gender-augmented Multi-scale Pseudo-label Adaptive Transfer Learning), as illustrated in Fig. 1. 
Overall, our primary emphasis is on two critical dimensions: the meticulous acquisition of high-quality emotional pseudo-labels and the comprehensive utilization of both frame-level pseudo-labels and utterance-level emotion labels. To achieve this goal, we design three stages with continuous relationships in the proposed GMP-ATL workflow.
Specifically, the first phase of GMP-ATL aims to acquire reasonable Gender-augmented Multi-scale Pseudo-labels (GMPs) through multi-task learning, involving both emotion and gender classification. In contrast to methods directly utilizing final layer features of pre-trained speech models, we choose to conduct multi-scale unsupervised k-means clustering on the third-to-last layer features of the HuBERT model. This decision is motivated by our experimental discovery that the emotion information appears to be most enriched in the third-to-last feature layer within our framework.
Having captured the frame-level GMPs, the objective of the second phase is to guide the HuBERT-based SER model to learn these beneficial information for emotion identification via a retraining process. Finally, we fine-tune the HuBERT-based SER model using the AM-Softmax (AMS) loss \cite{wang2018additive}, with the guidance of utterance-level emotion labels. 
By fully leveraging both utterance-level and frame-level emotion-associated information, our proposed GMP-ATL framework excels in capturing fine-grained contextualized representations of speech emotion, leading to a substantial improvement in the model's recognition performance.


\begin{figure*}[htbp]
\centering
	\includegraphics[height=8.5cm,width=!]{overall3.png}
	\caption{Overview of the proposed GMP-ATL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average pooling, LinearP represents linear projection module, Emo. and Gend. are the abbreviations of emotion and gender.}
\end{figure*}




\section{Related Work}
\label{sec:Related Work}
In this section, we provide a brief overview of relevant works, including the HuBERT-based SER methods, attribute-based multi-task learning for SER, and pseudo-label based SER.


\subsection{HuBERT-based SER Models}
HuBERT stands out as a distinguished pre-trained speech model, celebrated for its prowess in mastering the intricacies of speech representation learning. Adopting a self-supervised learning paradigm, HuBERT undergoes pre-training on extensive speech datasets and iteratively applies unsupervised clustering to generate pseudo-labels for each training phase, thereby fostering the model to acquire the meaningful, efficient, and robust universal feature representations.
Building upon the exceptional capabilities of HuBERT in speech representation learning, a multitude of HuBERT-based SER methods have recently been proposed. For example, \cite{fang23b_interspeech} presented a block architecture search strategy to explore downstream transfer of HuBERT-based features for emotion recognition. \cite{zhang23y_interspeech} introduced an optimal transport approach for cross-domain SER using the pre-trained HuBERT model.


\subsection{Attribute-based Multi-task Learning for SER}
Due to the multitude of attributes present in speech signals, numerous works \cite{li2019improved,ghriss2022sentiment,pan2024gemo,pan2024msac,zhang2022selective} aim to leverage these attributes to enhance SER. Among them, attribute-based multi-task learning emerges as an effective approach. It enhances the recognition performance of SER models by integrating auxiliary tasks, including gender classification, automatic speech recognition, and so forth. 
For instance, Li \emph{et al}. \cite{li2019improved} introduced a multitask learning (emotion and gender classification) based SER method and obtained great recognition results. 
Ghriss \emph{et al}. \cite{ghriss2022sentiment} advocated an end-to-end multi-task approach which employed a sentiment-aware automatic speech recognition pre-training for emotion recognition.
Zhang \emph{et al}. \cite{zhang2022selective} investigated an effective combination approaches on the basis of multi-task learning, with a focus on the style attribute.

\subsection{Pseudo-label based SER}
Constrained by the availability of labeled datasets, the majority of current methods rely on utterance-level emotion labels to achieve SER. 
Nonetheless, according to existing research \cite{fayek2017evaluating,xia2021temporal,mao2020eigenemo,feng2022semi}, utterance-level emotion labels in SER may not be as accurate, and they suggest that introducing frame-level pseudo-labels can be beneficial in modeling speech emotion.
In \cite{xia2021temporal}, the authors investigated the significance of temporal context for SER and advocated a segment-based learning objective to leverage the local features.
In \cite{fayek2017evaluating}, a dynamic intra-utterance or frame based formulation was presented to achieve emotion recognition.
In \cite{feng2022semi}, the researcheres designed Semi-FedSER, a semi-supervised federated learning appraoch for SER using multiview pseudo-labeling. 



\section{METHODOLOGY}
\label{sec:METHODOLOGY}
This section introduces the overall architecture of our proposed GMP-ATL workflow, which is depicted in Fig. 1.
To be precise, we initially give a detailed exposition of the frame-level GMPs extraction phase. Subsequently, the principles underlying the frame-level GMPs based retraining is provided. Third, we elucidate the the specific implementation of the downstream SER fine-tuning stage. 


\subsection{Frame-level GMPs Extraction}
Intuitively, human speech is not likely to be consistently characterized by one single emotion, especially in a long utterance. 
Therefore, to bridge this gap, we believe that it is necessary to extract and incorporate high-quality frame-level pseudo-labels to train the SER model, whose overall schematic diagram is outlined in Fig. 1 (a).


Concretely, we initiate the process by employing the pre-trained HuBERT model as a feature extractor. The derived features are subsequently fed into a bi-directional LSTM network, and the impact of the temporal dimension is alleviated through an average pooling operation. Following this, the acquired features undergo a linear projection module that consists of two linear layers and one ReLU activation layer. 
Furthermore, drawing inspiration from relevant literature \cite{li2019improved,pan2024gemo,nediyanchath2020multi}, we incorporate a multi-task learning strategy into the training process to introduce gender information within speech signals. In this way, the overall model is capable of capturing more feasible emotional representations.
Ultimately, the entire framework is trained using the cross-entropy (CE) loss function, guided by utterance-level emotion and gender categorical labels.
Consequently, the final loss $L_{Total}$ of this stage is formulated as:
\begin{equation}
    \begin{split}
        L_{Total} = \alpha_{e} L_{Emo} + (1-\alpha_{e}) L_{Gender}
    \end{split}
\end{equation}
where $L_{Emo}$ is the CE-loss of emotion attribute, $L_{Gender}$ is the CE-loss of gender attribute, $\alpha_e$ is a parameter to adjust $L_{Emo}$ and $L_{Gender}$. In our case, $\alpha_e$ is set to 0.9.

Afterward, in order to generate frame-level GMPs of higher quality, we attempt to use features from different layers of HuBERT and perform multi-scale unsupervised k-means clustering, a departure from the conventional practice of using features from the final layer. In our case, the features from the third-to-last layer gains the best performance. The number of cluster centers is empirically set to 64, 512, and 4096, respectively.



\subsection{Frame-level GMPs based Retraining}
In the second stage, we adopt the model retraining strategy that uses the obtained frame-level GMPs to retrain and optimize the HuBERT-based SER model, the concrete components of which are illustrated in Fig. 1 (b).


As depicted in the figure, we adhere to the conventional masking operation of HuBERT and implement a linear projection module, which comprises two linear layers and one ReLU activation layer following the HuBERT model, to accurately harness and align the GMPs. The entire model is trained using CE loss as well.
In light of this means, the frame-level GMP-based retraining method also aligns well with the original pre-training objectives of the HuBERT model, ensuring an effective utilization of information from both the original pre-trained HuBERT model and the obtained frame-level GMPs of previous phase.


\subsection{Utterance-level Emotion Label based Fine-tuning}
Due to many factors such as the inherent ambiguity in speech emotion, an issue arises where speech emotion categories show similarity between different classes and differences within the same classes. 

To alleviate this problem, in the last fine-tuning stage, we employ the AMS loss to fine-tune the HuBERT-based SER model under the guidance of utterance-level emotion labels. 
By incorporating a constraint vector margin, this approach is able to increase the inter-class distance between different emotion categories and reduce the intra-class distance within the same emotion category, thus enhancing the model's recognition performance.
Moreover, it is worth noting that we utilize the HuBERT model trained in the second stage as the feature extractor for this phase, with its comprehensive system layout presented in Figure 1 (c).

As a result, the final loss function for the fine-tuning stage can be defined as:
\begin{equation}
    \begin{split}
        L = -\frac{1}{N}\sum_{i=1}^N\log
        \frac{e^ {s(\cos(\theta_{y_i, i}) - m)}}
        {e^{s(\cos(\theta_{y_i, i}) - m)} + \sum_{j\neq{y_i}}e^{s(\cos(\theta_{j, i}))}}
    \end{split}
\end{equation}
\begin{equation}
    \begin{split}
        \cos(\theta_{j, i}) = \frac{{x}_{i}^{T}w_j}
        {\left\Vert x_i \right\Vert \left\Vert w_j \right\Vert}
    \end{split}
\end{equation}
\noindent
where $L$ represents the ultimate loss, $x_i$ and $y_i$ represent the feature vector and label of the $i$th sample, $w_j$ corresponds to the feature vector of class $j$, and $\theta_{j, i}$ signifies the angle between $x_i$ and $w_j$. N denotes the batch size, while s is the scaling factor, and m is the additive margin. In our specific scenario, the values for m and s are set at 0.2 and 30.
{
\let\thefootnote\relax
\footnote{$^1$https://huggingface.co/facebook/hubert-base-ls960}
}

\section{EXPERIMENTS}
\label{sec:EXPERIMENTS}

In this section, we first outline the experimental setups of this study, encompassing the experimental database and implementation details. 
Second, a comprehensive comparison and analysis of the proposed approach with existing SOTA unimodal and multimodal SER methods is given.  
Finally, to examine the validity of our proposed GMP-ATL framework, we conduct an ablation study.


\subsection{Experimental Setups}
\subsubsection{Experimental Datasets}

In this work, we conduct extensive experiments using the most challenging IEMOCAP \cite{IEMOCAP} corpus, widely recognized for evaluating SER systems. IEMOCAP consists of scripted and improvised multimodal interactions recorded in five sessions with the participation of 10 subjects (5 male and 5 female actors), and the dataset is labeled by three annotators. 

To facilitate a fair comparison with other SOTA methods, we merge "excited" and "happy" into the "happy" category, resulting in four emotion categories: angry, happy, neutral, and sad. The detailed information is shown in Table \ref{tab:data-details}.

\begin{table}[htbp]
    \centering
    \caption{Details of the experimental dataset IEMOCAP.}
    \label{tab:data-details}
    \renewcommand{\arraystretch}{0.8} 
    \begin{tabular}{cccc}
        \toprule
        Categories        &  Number   &   Duration (min)  \\
        \midrule
        Angry             &  1103   &  83      \\
        Happy             &  1636   &  126     \\
        Neutral           &  1708   &  111     \\
        Sad               &  1084   &  99      \\
        \midrule
        Total             &  5531   &  419      \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Implementation Details}

In all experiments, we adopt Adam to optimize the proposed HuBERT-based GMT-ATL framework with an initial learning rate of 1e-4 and a batch size of 64. The pre-trained HuBERT model is accessible on an open-source website$^1$.

As for evaluating metrics, we use weighted average recall (WAR) and unweighted average recall (UAR) in the context of the speaker-independent setting.
Additionally, we perform a standard 5-fold cross-validation, aligning with current SOTA SER methods, to evaluate our GMT-ATL. Within each fold, one session is reserved as the test set, and we calculate the average of the acquired UAR and WAR across all folds.


\subsection{Main Results}

To examine the effectiveness of the proposed HuBERT-based GMP-ATL framework, we first compare its recognition performance with SOTA unimodal and multimodal SER approaches, with the results summarized in Table \ref{tab:comparison-results}.

\begin{table}[htbp]
    \centering
    \caption{Recognition comparison of SOTA SER methods on IEMOCAP. A and T are audio and text modalities, respectively.}
    \label{tab:comparison-results}
    \renewcommand{\arraystretch}{0.9} 
    \begin{tabular}{cccc}
        \toprule
        Model                      &  Modalities  &   UAR   &  WAR   \\
        \midrule
        TCSER \cite{xia2021temporal}  &  A  &   66.9   &  65.4  \\
        Wav2vec2-PT \cite{Pepino}  &  A  &    -   &  67.2  \\
        Joint Network \cite{hu2023joint}  &  A  &  73.3  &  72.5   \\
        TAP  \cite{Gat}            &  A  &  74.2  &  -      \\
        P-TAPT \cite{Chen}         &  A  &  74.3  &  -      \\
        BAS \cite{fang23b_interspeech} &  A  &  75.0  &  74.3  \\
        UDA  \cite{Morais}         &  A  &  77.8  &  77.4  \\
        \midrule
        MF-WavBERT \cite{zhao2022multi}     &  A + T  &  76.3  &  -      \\
        KS-Transformer \cite{KS-Transformer} & A + T  &   75.3   &  74.3  \\
        BAM  \cite{Knowledge}      &  A + T  &  77.0  &  75.5  \\
        Bi-modal \cite{yang2022exploiting}  & A + T  &   78.5   &  77.7  \\
        W2B-CA-Aux \cite{sun2023}  &  A + T  &  79.7  &  78.4  \\
        MMER \cite{ghosh2022mmer}  &  A + T  &    -    &  81.2  \\
        GEmo-CLAP \cite{pan2024gemo}  &  A + T  &   83.2    &  81.4  \\
        \midrule
        GMP-ATL (Ours)   &  A  &  82.0  &  80.0  \\
        \bottomrule
    \end{tabular}
\end{table}


As illustrated in the above table, it is apparent that our GMP-ATL framework achieves superior recognition results.
Specifically, when compared with SOTA unimodal SER methods, the proposed GMP-ATL attains the best UAR and WAR of 82.0\In addition, the proposed GMP-ATL also demonstrates competitive recognition results as opposed to SOTA multimodal SER approaches, showcasing the effectiveness of our proposed method.

\subsection{Ablation Study}

To comprehensively evaluate the validity of our design, ablation studies are performed. 

\subsubsection{Effect of key components in GMT-ATL.}

We first validate the effectiveness of various crucial components of the proposed GMT-ATL workflow in Table \ref{tab:ablation-results1}.

\begin{table}[htbp]
    \centering
    \caption{The effect of GMPs and AMS Loss on the performance of the proposed GMP-ATL framework.}
    \label{tab:ablation-results1}
    \renewcommand{\arraystretch}{0.9} 
    \begin{tabular}{cccccc}
        \toprule
        GMPs &  CE Loss  & AMS Loss &   Layer ID   &   UAR   &  WAR   \\
        \midrule
                    &  \checkmark  &              &  -3   &  78.0    &  75.8  \\
                    &              &  \checkmark  &  -3   &  79.8    &  77.6  \\
        \checkmark  &  \checkmark  &              &  -3   &  80.2    &  78.4  \\
        \checkmark  &              &  \checkmark  &  -3   &  \textbf{82.0}    &  \textbf{80.0}  \\
        \bottomrule
    \end{tabular}
\end{table}


From the table, we can easily observe that without the proposed frame-level GMPs, the performance of our GMP-ATL framework has dropped by at least 2\Without AMS-loss, the performance drops as well, indicating that the AMS-loss is more suitable for the proposed GMP-ATL SER approach.



\subsubsection{Effect of GMPs Using Different Feature Layers.}

Moreover, we study the effect of GMPs based on different feature layers of HuBERT within the proposed GMP-ATL framework. Results are shown in Table \ref{tab:ablation-results2}.


\begin{table}[htbp]
    \centering
    \caption{The effect of GMPs based on different HuBERT's feature layers on the performance of our proposed GMP-ATL workflow.}
    \label{tab:ablation-results2}
    \renewcommand{\arraystretch}{1.0} 
    \begin{tabular}{cccc}
        \toprule
        Method                   &  Layer ID   &   UAR   &  WAR   \\
        \midrule
        \multirow{8}{*}{GMT-ATL} &  -1   &  80.4    &  79.0  \\
                                 &  -2   &  80.2    &  \textbf{\emph{79.7}}  \\
                                 &  -3   &  \textbf{82.0}    &  \textbf{80.0}  \\
                                 &  -4   &  \textbf{\emph{81.9}}    &  \textbf{\emph{79.7}}  \\
                                 &  -5   &  80.6    &  78.6  \\
                                 &  -6   &  80.6    &  78.1  \\
                                 &  -7   &  81.2    &  79.5  \\
                                 &  -8   &  80.6    &  79.3  \\
        \bottomrule
    \end{tabular}
\end{table}

It is evident that under the proposed GMP-ATL workflow, the performance of GMPs based on the final layer features of HuBERT is suboptimal. Conversely, GMPs relying on the features from the third-to-last and fourth-to-last layers demonstrate superior results in sentiment recognition. This phenomenon may be attributed to the final layer features of the Hubert model containing more semantic information rather than emotion-related details.


\section{CONCLUSIONS}
\label{sec:CONCLUSIONS}
In this work, we propose an effective transfer learning-based SER framework GMP-ATL, optimizing the pre-trained HuBERT model to discern emotions by incorporating more reasonable emotional pseudo-labels. 
Extensive experiments on the IEMOCAP corpus indicate that our proposed GMP-ATL framework not only outperforms SOTA unimodal SER methods but achieves competitive performance compared to multimodal SER approaches. We demonstrate that incorporating reasonable frame-level gender-augmented multi-scale pseudo-labels can effectively enhance the recognition performance of the proposed workflow. 
Furthermore, we observe that within our proposed framework, the features extracted from the third-to-last layer of the pre-trained HuBERT model seem to encompass more emotion-related information, a phenomena that may be applicable to other speech attributes as well.



\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
