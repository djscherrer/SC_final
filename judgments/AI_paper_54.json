{"title":"AI_paper_54","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"ETH Zurich","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{ETH Zurich, Switzerland}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9.5"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Lausanne","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Lausanne, Switzerland}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Zurich University of Applied Sciences (ZHAW)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Zurich University of Applied Sciences (ZHAW), Switzerland}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Cambridge","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Cambridge, England}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 9\n\n2. Method 10\n\n3. Credibility 9\n\n4. Understandability 8\n\n5. Relevance 10\n\n6. Quality of Citations 9\n\n7. Linguistic style and soundness of grammar 9\n\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Dundee","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Dundee, England}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.5"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Wolverhampton","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Wolverhampton, England}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 9\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Peking University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Peking University, China}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 8\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Dalian University of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Dalian University of Technology, China}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.5"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"China University of Mining and Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{China University of Mining and Technology, China}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 9\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Indian Institute of Technology Bombay (IITB)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Indian Institute of Technology Bombay (IITB), India}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality: 10\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.33"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Delhi","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Delhi, India}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 9\n2. Method 10\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Indian Institute of Technology Hyderabad (IITH)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Indian Institute of Technology Hyderabad (IITH), India}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Sharif University of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Sharif University of Technology, Iran}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"Failed to generate answer for title: AI_paper_54 and university: Sharif University of Technology"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Shiraz University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Shiraz University, Iran}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality: 10\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 10\n8. Overall score: 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Ferdowsi University of Mashhad","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Ferdowsi University of Mashhad, Iran}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality: 10\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.3"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Massachusetts Institute of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Massachusetts Institute of Technology, USA}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.75"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Houston","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Houston, USA}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10 \n2. Method 10 \n3. Credibility 9 \n4. Understandability 9 \n5. Relevance 10 \n6. Quality of Citations 9 \n7. Linguistic style and soundness of grammar 9 \n8. Overall score 9.5"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Western Washington University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Western Washington University, USA}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.75"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Sao Paulo","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Sao Paulo, Brazil}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 9\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Universidade de Bras\u00edlia","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Universidade de Bras\u00edlia, Brazil}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS, Brazil}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9.33"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Cairo University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Cairo University, Egypt}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 9\n2. Method 10\n3. Credibility 9\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Alexandria University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Alexandria University, Egypt}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9.25"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Zagazig University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Zagazig University, Egypt}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.88"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"The University of Melbourne","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{The University of Melbourne, Australia}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"The University of Adelaide","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{The University of Adelaide, Australia}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Queensland University of Technology (QUT)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Queensland University of Technology (QUT), Australia}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Technical University of Munich","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Technical University of Munich, Germany}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"### Quality Rating of the Paper\n\n1. Originality: 10\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 10\n7. Linguistic style and soundness of grammar: 10\n8. Overall score: 10"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Potsdam","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Potsdam, Germany}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 9\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"Leibniz University Hannover","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Leibniz University Hannover, Germany}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of Cape Town","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Cape Town, South Africa}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.86"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of South Africa","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of South Africa, South Africa}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 9\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9.25"}
{"title":"AI_paper_54","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","input_length":12512,"university_association":"University of the Free State","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of the Free State, South Africa}\n\\title{Reinforcement Learning-Guided \\\\ Semi-Supervised Learning}\n\\maketitle\n\n\n\\begin{abstract}\n In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. \nHowever, most current SSL methods \nrely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. \nThey are limited to \nexploiting loss functions and regularization methods within the standard norm.\nIn this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, \nthat formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward\nto adaptively guide the learning process of the prediction model. \nRLGSSL incorporates a carefully designed reward function \nthat balances the use of labeled and unlabeled data to enhance generalization performance. \nA semi-supervised teacher-student framework is further deployed to increase the learning stability. \nWe demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets \nand show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods. \n\\end{abstract}\n\n\\section{Introduction}\nSemi-supervised learning (SSL) is a significant research area in the field of machine learning, \naddressing the challenge of effectively utilizing limited labeled data alongside abundant unlabeled data. SSL techniques bridge the gap between supervised and unsupervised learning, offering a practical solution when labeling large amounts of data is prohibitively expensive or time-consuming. The primary goal of SSL is to leverage the structure and patterns present within the unlabeled data to improve the learning process, generalization capabilities, and overall performance of the prediction model. \nOver the past few years, there has been considerable interest in developing various SSL methods, and these approaches have found success in a wide range of applications, from computer vision \\cite{berthelot2019mixmatch}  to natural language processing \\cite{howard2018universal} \nand beyond \\cite{rasmus2015semi, zhu2009introduction}.\n\nWithin the SSL domain, a range of strategies has been devised to effectively utilize the information available in both labeled and unlabeled data. Broadly, SSL approaches can be categorized into three key paradigms: regularization-based, mean-teacher-based, and pseudo-labeling methodologies. \nRegularization-based approaches form a fundamental pillar of SSL~\\cite{miyato2018virtual,laine2017temporal,zhang2020consistency}. \nThese methods revolve around the core idea of promoting model robustness against minor perturbations in the input data. A quintessential example in this category is Virtual Adversarial Training (VAT) \\cite{miyato2018virtual}. VAT capitalizes on the introduction of adversarial perturbations to the input space, thereby ensuring the model's predictions maintain consistency. The second category, Mean-teacher based methods, encapsulates a distinct class of SSL strategies that leverage the concept of temporal ensembling. This technique aids in the stabilization of the learning process by maintaining an exponential moving average of model parameters over training iterations. Mean Teacher \\cite{tarvainen2017mean} notably pioneered this paradigm with their Mean Teacher model, illustrating its efficacy across numerous benchmark tasks. Lastly, the category of Pseudo-labeling approaches has attracted attention due to its simplicity and effectiveness. These methods employ the model's own predictions on unlabeled data as ``pseudo-labels'' to augment the training process. The MixMatch \\cite{berthelot2019mixmatch} framework stands as one of the leading representatives of this category, demonstrating the potential of these methods in the low-data regime.\n\nDespite these advancements, achieving high performance with limited labeled data continues to be a significant challenge in SSL, often requiring intricate design decisions and the careful coordination of multiple loss functions. \nIn this paper,\nwe propose to approach SSL outside the conventional design norms by \ndeveloping a Reinforcement Learning Guided Semi-Supervised Learning (RLGSSL) method.\nRL has emerged as a promising direction for addressing learning problems, with the potential to bring a fresh perspective to SSL. \nIt offers a powerful framework for decision-making and optimization, which can be harnessed to discover novel and effective strategies for utilizing the information present in both labeled and unlabeled data. \nIn RLGSSL, we formulate SSL as a bandit problem, \nwhere the prediction model serves as the policy function, and pseudo-labeling acts as the actions. \nWe define a simple reward function that balances the use of labeled and unlabeled data \nand improves generalization capacity \nby leveraging linear data interpolation,\nwhile the prediction model is trained under the standard RL framework to maximize\nthe empirical expected reward.\nFormulating the SSL problem as such an RL task allows our approach to\ndynamically adapt and respond to the data. \nMoreover, \nwe further deploy a teacher-student learning framework to enhance \nthe stability of learning. \nAdditionally, we integrate a supervised learning loss to improve and accelerate the learning process. \nThis new SSL framework has the potential to pave the way for more robust, flexible, and adaptive SSL methods.\nWe evaluate the proposed method through extensive experiments on benchmark datasets.\nThe contribution of this work can be summarized as follows:\n\\begin{itemize}\n    \\item \nWe propose RLGSSL, a novel Reinforcement Learning-based approach that effectively tackles SSL \nby leveraging RL's power to learn effective strategies for generating pseudo-labels and guiding the learning process.\n\n\n    \\item We design a prediction assessment reward function that encourages the learning of \n\t    accurate and reliable pseudo-labels \n\t    while maintaining a balance between the usage of labeled and unlabeled data, thus promoting better generalization performance.\n    \\item\n\t  We introduce a novel integration framework that combines the power of \n\t\tboth RL loss and standard semi-supervised loss for SSL, \n\t\tproviding a more adaptive and data-driven approach \nthat has the potential to lead to more accurate and robust SSL models.\n    \\item \n\t    Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in SSL.\n\\end{itemize}\n\n\n\n\\section{Related Work}\n\\subsection{Semi-Supervised Learning}\nExisting SSL approaches can be broadly classified into three primary categories: \nregularization-based methods, teacher-student-based methods, and pseudo-labeling techniques.\n\n\n\\textbf{Regularization-Based Methods} \nA prevalent research direction in SSL focuses on regularization-based methods, which introduce additional terms to the loss function to promote specific properties of the model. For instance, the $\\Pi$-model \\cite{laine2017temporal} and Temporal-Ensemble \\cite{laine2017temporal} incorporate consistency regularization into the loss function, with the latter employing the exponential moving average of model predictions. Virtual Adversarial Training (VAT) \\cite{miyato2018virtual} is yet another regularization-based technique that aims to make deep neural networks robust to adversarial perturbations. In a similar vein, Consistency Regularization for Generative Adversarial Networks (CR-GAN) \\cite{zhang2020consistency} integrates a generative adversarial network (GAN) with a consistency regularization term, facilitating the generation of pseudo-labels for unlabeled data.\n\n\\textbf{Teacher-Student-Based Methods} \nTeacher-student-based methods offer an alternative approach in SSL research. These techniques train a student network to align its predictions with those of a teacher network on unlabeled data. \nMean Teacher (MT) \\cite{tarvainen2017mean}, a prominent example in this category, leverages an exponential moving average (EMA) on the teacher model. To enhance performance, MT + Fast SWA \\cite{athiwaratkun2019there} combines Mean Teacher with Fast Stochastic Weight Averaging. Smooth Neighbors on Teacher Graphs (SNTG) \\cite{luo2018smooth} takes a different approach, utilizing a graph for the teacher to regulate the distribution of features in unlabeled samples. Meanwhile, Interpolation Consistency Training (ICT) \\cite{verma2022interpolation} aims to promote consistent predictions across interpolated data points by ensuring that a model's predictions on an interpolated set of unlabeled data points remain consistent with the interpolation of the predictions on those points.\n\n\\textbf{Pseudo-Labeling Methods} \nPseudo-labeling is an effective way to\nextend the labeled set when the number of labels is limited. Pseudo-Label \\cite{lee2013pseudo} produces labels for unlabeled data using model predictions and filters out low-confidence predictions. MixMatch \\cite{berthelot2019mixmatch} employs data augmentation to create multiple input versions, obtaining predictions for each and averaging them to generate pseudo-labels. In contrast, works such as ReMixMatch \\cite{berthelot2020remixmatch}, UDA \\cite{xie2020unsupervised}, and FixMatch \\cite{sohn2020fixmatch} apply confidence thresholds to produce pseudo-labels for weakly augmented samples, which subsequently serve as annotations for strongly augmented samples.\nLabel propagation methods, including TSSDL \\cite{shi2018transductive} and LPD \\cite{iscen2019label}, assign pseudo-labels based on local neighborhood density. DASO \\cite{oh2021distribution} combines confidence-based and density-based pseudo-labels in varying ways for each class. Approaches such as Dash \\cite{xu2021dash} and FlexMatch \\cite{zhang2021flexmatch} dynamically adjust confidence thresholds in a curriculum learning manner to generate pseudo-labels. Meta Pseudo-Labels \\cite{pham2021meta} uses a bi-level optimization strategy, deriving the teacher update rule from student feedback, to learn from limited labeled data.\nCo-Training \\cite{blum1998combining} is an early representative of pseudo-lableing which involves training two classifiers on distinct subsets of unlabeled data and using confident predictions to produce pseudo-labels for one another. Similarly, Tri-Training \\cite{zhou2005tri} trains three classifiers on separate unlabeled data subsets and generates pseudo-labels based on the disagreements between their predictions.\n\n\\subsection{Reinforcement Learning}\n\nReinforcement Learning (RL) is a field of study that focuses on optimizing an agent's decision-making abilities by maximizing the cumulative reward obtained through interactions with its environment~\\citep{sutton2018reinforcement}. \nRL methodology has been widely applied to solve many other learning problems, including  \nsearching for optimized network architectures~\\citep{zoph2016neural},\ntraining sequence models for text generation by receiving reward signals~\\citep{bahdanau2016actor,ranzato2015sequence},\nand solving online planning problems~\\citep{fickinger2021scalable}.\nRecently, RL has been applied to fine-tune complex models that typically fail to align with users' preferences. \nMoreover, based on RL from Human Feedback (RLHF;~\\citep{christiano2017deep, ziegler2019fine, stiennon2020learning}), \nChatGPT achieves great success in dialogue generation by fine-tuning Large Language Models (LLM)\n\\citep{ouyang2022training}. \nIt frames the training of LLM as a bandit problem, \nspecifically a one-armed bandit problem~\\citep{sutton2018reinforcement}, \nwhere the objective is to determine the optimal action (dialogue generation) for a given state (user prompt) \nwithin a single step. \n\n\nThe bandit problem \nwas originally described as a statistical decision model used by agents to optimize their decision-making process~\\citep{robbins1952some}. In this problem, an agent receives a reward upon taking an action and learns to make the best decision by maximizing the given reward. The bandit problem found its application in economics and has been widely used in market learning, specifically in finding the optimal market demands or prices to maximize expected profits~\\citep{rothschild1974two}. Bergemann et al.~\\citep{bergemann2006bandit} and Lattimore et al.~\\citep{lattimore2020bandit} have extensively discussed the literature and modern applications of the bandit problem. Additionally, Mortazavi et al.~\\citep{mortazavi2022theta} introduced a Single-Step Markov Decision Process (SSMDP) to formulate the bandit problem in a manner that aligns with modern RL techniques. This advancement enables the utilization of standard RL methods on conventional bandit problems.\n\n\n\\begin{figure}[t]\n  \\centering\n \\includegraphics[width= 0.78 \\textwidth]{images\/figure.pdf} \n\t \\caption{Overview of the RLGSSL Framework. \n\t The prediction networks ($\\theta_S, \\theta_T$) serve as the policy functions, \n\t and pseudo-labeling ($P_{\\theta_T}(X^u)$) acts as the actions.\n\t The model has three loss terms in total: \n\t RL loss ($\\mathcal{L}_{\\text{rl}}$), supervised loss ($\\mathcal{L}_{\\text{sup}}$), \n\t and consistency loss ($\\mathcal{L}_{\\text{cons}}$). \n\t The teacher policy function is used to execute the actions and compute the consistency loss,\n\t while the student policy function is used for all other aspects. \n}\n   \\label{fig:method}\n   \\vskip -.1in\n\\end{figure}\n\n\\section{The Proposed Method}\n\nWe consider the following semi-supervised learning setting: \nthe training dataset consists of a small number of labeled samples, \n$\\mathcal{D}_l = (X^l, Y^l)=\\{(x_i^l, {\\bf y}_i^l)\\}_{i=1}^{N^l}$, and a large number of unlabeled samples, \n$\\mathcal{D}_u = X^u= \\{x_i^u\\}_{i=1}^{N^u}$, with $N^u \\gg N^l$,\nwhere $x_i^l$ (or $x_i^u$) denotes the input instance\nand ${\\bf y}_i^l$ denotes the one-hot label vector with length $C$. \nThe goal is to train a $C$-class classifier $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ \nthat generalizes well to unseen test data drawn from the same distribution as the training data.\n\n\nIn this section, we present the proposed RLGSSL method, which formulates SSL \nas a one-armed bandit problem with a continuous action space,\nand deploys a standard RL loss to guide the SSL process \nbased on a reward function specifically designed for semi-supervised data. \nMoreover, we further incorporate a semi-supervised teacher-student framework\nto augment the RL loss with a supervised loss and a prediction consistency regularization loss, \naiming to enhance the learning stability and efficacy. \nFigure \\ref{fig:method} illustrates \nthe overall framework of the proposed RLGSSL,\nand the following subsections will elaborate on the approach. \n\n\\subsection{Reinforcement Learning Formulation for SSL}\n\\label{sec:RLGSSL}\n\nWe treat SSL as a special one-armed bandit problem with a continuous action space. \nOne-armed bandit problem can be considered a single-step Markov Decision Process (MDP)~\\citep{mortazavi2022theta}. \nIn this problem, the agent takes a single action and receives a reward based on that action. \nThe state of the environment is not affected by the action. \nThe one-armed bandit problem involves selecting an action to maximize an immediate reward,\nwhich can be regarded as learning a policy function under the RL framework. \nFormulating SSL as a one-armed bandit problem within the RL framework\nand deploying RL techniques to guide SSL\nrequires defining the following key components: state space $\\mathcal{S}$, \naction space $\\mathcal{A}$, \na policy function $\\pi: \\mathcal{S} \\to \\mathcal{A}$, and  \na reward function $\\mathcal{R}: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$.\nThe objective is to learn an optimal policy $\\pi^\\star$ \nthat maximizes the one-time reward $\\mathcal{R}(s, \\pi(\\cdot|s))$ \nin the given environment (state $s$): \n$\\pi^\\star=\\arg\\max_\\pi\\; J_r(\\pi)=\\mathbb{E}_{\\pi}[\\mathcal{R}(s,\\pi(\\cdot|s))]$.\n\n\\paragraph{State}\nThe state encapsulates the provided knowledge about the environment and is used as input for the policy function.\nAs the action does not affect the state of the environment under one-armed bandit problem, \nwe use the observed data from the SSL problem as the state; i.e., $s=(X^l, Y^l, X^u)$. \n\n\\paragraph{Action and Policy Function}\nAs the goal of SSL is to learn an optimal classifier $f_\\theta$ (i.e., prediction network parameterized with $\\theta$), \nwe use the classifier $f_\\theta$, usually denoted by its parameters $\\theta$, as the policy function $\\pi_\\theta$\nto {\\em unify the goals of RL and SSL}. \nIn particular, we consider a probabilistic policy function\/prediction network \n$\\pi_\\theta(\\cdot)=P_\\theta(\\cdot)$. \nSince policy function is used to project a mapping from the state $s$ to the action space, \n$a=\\pi_\\theta(\\cdot|s)$, \nby using a probabilistic prediction network as the policy function, \nit naturally determines a continuous action space $\\mathcal{A}$.\nSpecifically, given the fixed state $s$, taking an action\nis equivalent to making probabilistic predictions on the unlabeled data in $s$:\n$Y^u=P_\\theta(X^u)=\\pi_\\theta(\\cdot|s)$, as the labeled data already has labels.  \nFor each unlabeled instance $x_i^u$, \nthe action is a probability vector produced as ${\\bf y}_i^u=P_\\theta(x_i^u)$,\nwhich can be regarded as soft pseudo-labels in the SSL setting.\nThis links the action of RL to the pseudo-labeling in SSL. \n\\subsubsection{Reward Function}\nThe reward function serves as feedback to evaluate the performance of the action (prediction) provided by the policy. It needs to be thoughtfully crafted to maximize the model's ability to extract useful information from both labeled and unlabeled data, which is central to the SSL paradigm. \nThe underlying motivation is to guide the learning process to induce a more generalizable and robust prediction model. \nTo this end, we adopt a data mixup~\\cite{zhang2018mixup}\n strategy to produce new data points \nfrom the given labeled data $(X^l,Y^l)$ and pseudo-labeled data $(X^u,Y^u)$,\nwhich together form $(s,a)$,\nthrough linear data interpolation,  \nand assess the prediction model's generalization ability on such data points as the reward signal. \nThis decision is inspired by the proven effectiveness of mixup in enhancing model performance in various tasks. \nThe idea of data mix-up is to generate virtual training examples by creating convex combinations of pairs of input data and their corresponding labels. This technique encourages the model to learn more fluid decision boundaries, leading to improved generalization capabilities.\n\n\nSpecifically, we propose to generate new data points by performing {\\em inter-mixup} \nbetween labeled and unlabeled data points,\naiming to maintain a balanced utilization of both labeled and unlabeled data. \nIn order to address the size discrepancy between the labeled dataset $\\mathcal{D}^l$ and \nthe unlabeled dataset $\\mathcal{D}^u$, with $N^u \\gg N^l$, \nwe replicate the labeled dataset $\\mathcal{D}^l$ by a factor of \n$r = \\lceil\\frac{N^u}{N^l}\\rceil$ times, \nresulting in an extended labeled dataset $\\widetilde{\\mathcal{D}}^l$.\nAfter shuffling the data points in each set, \nwe generate a mixup data point by mixing an unlabeled point \n$x_i^u \\in \\mathcal{D}^u$ with a labeled point $x_i^l \\in \\widetilde{\\mathcal{D}}^l$ \nalong with their corresponding pseudo-label ${\\mathbf{y}^u_i} \\in \\mathcal{D}^u$ and \nlabel ${\\mathbf{y}^l_i} \\in \\widetilde{\\mathcal{D}}^l$:\n\\begin{equation}\n\\begin{gathered}\nx_i^{\\text{m}}=\\mu\\,x_i^u+(1-\\mu)\\,x^l_i,\\qquad \n{\\mathbf{y}}_i^{\\text{m}}=\\mu\\,{\\mathbf{y}}_i^u + (1-\\mu)\\,{\\mathbf{y}}_i^l\n\\end{gathered}\n\\label{eq:mix}\n\\end{equation}\nwhere the mixing parameter $\\mu$ is sampled from a Beta distribution.\nWith this procedure, we can generate \n$N^{\\text{m}}=N^u$ mixup samples\nby mixing all the unlabeled data with the extended labeled data.  \n\nWe then define the reward function to measure the negative mean squared error (MSE) \nbetween the model's prediction $P_\\theta(x_i^{\\text{m}})$ and the mixup label $\\mathbf{y}_i^{\\text{m}}$ \nfor each instance in the mixup set. \nThis results in a single, comprehensive metric that quantifies the overall (negative) \ndisagreement between the model's predictions and the mixup labels over a large set of interpolated data points:\n\\begin{equation}\n\\mathcal{R}(s, a;\\text{sg}[\\theta]) =\n\\mathcal{R}(X^l, Y^l, X^u, Y^u;\\text{sg}[\\theta]) \n\t= - \\frac{1 }{C\\cdot N^\\text{m} }\\sum\\nolimits_{i=1}^{N^\\text{m}} ||P_{\\theta}(x_i^{\\text{m}})-\\mathbf{y}_i^{\\text{m}}||^2_2\n\\label{eq:reward}\n\\end{equation}\nwhere $C$ denotes the number of classes and $\\text{sg}[\\cdot]$ is the stop gradient operator \nwhich stops the flow of gradients during the backpropagation process. \nThis ensures that the reward function is solely employed for model assessment, \nrather than being directly utilized for model updating, \nenforcing the working mechanisms of RL. \nMixup labels capture both the supervision information in the labeled data\nand the uncertainty in the pseudo-labels of unlabeled data. \nWith the designed reward function, a good reward value can only be returned \nwhen the prediction model not only exhibits strong alignment with the labeled data \nbut also delivers accurate predictions on the unlabeled data. \nConsequently, through RL, this reward function will \nnot only promote accurate predictions but also enhance the model's robustness and generalizability.\n\n\\subsubsection{Reinforcement Learning Loss}\n\nBy deploying the probabilistic pedictions on the unlabeled data, \n$Y^u= \\pi_\\theta(X^u)=P_\\theta(X^u)$, \nas the action, we indeed adopt a deterministic policy. \nFollowing the principle of one-armed bandit problem on \nmaximizing the expected one-time reward w.r.t. the policy output, \nwe introduce a weighted negative reward based on the deterministic policy output\nas the RL loss for the proposed RLGSSL.\nSpecifically, we treat the output of the policy network, $Y^u$, \nas a uniform distribution over the set of $N^u$ probability vectors, \n$\\{{\\bf y}_1^u,\\cdots, {\\bf y}_{N^u}^u\\}$,\npredicted for the unlabeled instances.\nLet ${\\bf e}={\\bf 1}\/C$ denote a discrete uniform distribution vector with length $C$. \nWe design the following KL-divergence weighted negative reward as the RL loss:\n\\begin{equation}\n\\begin{aligned}\n\\label{eqa:rl_loss-kl}\n\\mathcal{L}_{\\text{rl}} \n&= - \\mathbb{E}_{{\\bf y}_i^u\\sim\\pi_\\theta} \\mbox{KL}({\\bf e}, {\\bf y}_i^u)\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n&= - \\mathbb{E}_{x_i^u\\in \\mathcal{D}_u} \\mbox{KL}({\\bf e}, P_\\theta(x_i^u))\\mathcal{R}(s,a;\\text{sg}[\\theta])\\\\ \n\\end{aligned}\n\\end{equation}\nwhere the KL-divergence term measures the distance of each label prediction probability vector\n${\\bf y}_i^u$ from a uniform distribution vector.\nGiven that a uniform probability distribution signifies the least informative prediction outcome, \nthe expected KL-divergence captures the level of informativeness in the policy output\nand hence serves as a meaningful weight for the reward, \nwhich inherently encourages the predictions to exhibit greater discrimination.\n\n\nThe minimization of this loss function over the prediction network parameterized by $\\theta$ \nis equivalent to learning an optimal policy function $\\pi_\\theta$ by maximizing \nthe KL-divergence weighted reward,\nwhich aims at an optimal policy function (also the probabilistic classifier $P_\\theta$) \nthat not only maximizes the reward signal but is also discriminative.\nFrom the perspective of SSL, the utilization of this novel RL loss introduces a fresh approach \nto designing prediction loss functions. \nInstead of directly optimizing the alignment between predictions and targets, \nit offers a gradual learning process guided by reward signals. \nThis innovative approach presents a more \nadaptive and flexible \nsolution for complex data scenarios,\nwhere the traditional optimization-based methods may fall short.\n\n\\subsection{Teacher-Student Framework for RL-Guided SSL}\n\\label{sec:teacher-student}\n\nTeacher-student models~\\cite{tarvainen2017mean} \nhave been popularly deployed to exploit unlabeled data for SSL,\nimproving the learning stability.\nWe extend this mechanism to provide a teacher-student framework\nfor RL-guided SSL \nby maintaining a dual set of model parameters: \nthe student policy\/model parameters $\\theta_S$, and the teacher policy\/model parameters $\\theta_T$. \nThe student model is directly updated through training, \nwhereas the teacher model is updated via an exponential moving average (EMA) of the student model. \nThe update is conducted as follows:\n\\begin{equation}\n\\theta_T= \\beta\\,\\theta_T + (1-\\beta)\\,\\theta_S\n\\label{eq:ema}\n\\end{equation}\nwhere $\\beta$ denotes a hyperparameter that modulates the EMA's decay rate. \nThe utilization of the EMA update method ensures a stable and smooth transfer of knowledge from the student model \nto the teacher model. \nThis leads to a teacher model with consistent and reliable parameter values \nthat are not susceptible to random or erratic fluctuations during the training process.\nLeveraging this desirable characteristic, \nwe propose to {\\em employ the teacher model for executing actions} within the RL framework \ndescribed in the subsection \\ref{sec:RLGSSL} above; \nthat is, $Y^u=P_{\\theta_T}(X^u)$, \nwhile retaining the student model for other aspects. \nBy doing so, we ensure that stable actions are taken, \nreducing the impact of random noise in the generated pseudo-labels and enhancing the accuracy of reward evaluation.\n\n\nWithin the teacher-student framework, we further propose to augment the RL loss \nwith a supervised loss $\\mathcal{L}^{\\text{sup}}$ on the labeled data \nand a consistency regularization loss $\\mathcal{L}^{\\text{cons}}$ on the unlabeled data. \nWe adopt a standard cross-entropy loss function $\\ell_{CE}$ to compute the supervised loss,\npromoting accurate predictions on $\\mathcal{D}^l$\nwhere the ground-truth labels are available: \n\\begin{equation}\n\\label{eq:cl-loss}\n\\mathcal{L}^{\\text{sup}} = \n\t\\mathbb{E}_{(x^{l}, {\\bf y}^{l})\\in \\mathcal{D}^l}\n\t\\left[\\ell_{CE}\\left(P_{\\theta_{S}}(x^{l}),{\\bf y}^{l}\\right) \\right]\n\\end{equation}\nThis loss can enhance effective exploitation of the ground-truth label information, \nproviding a solid basis for exploring the parameter space via RL. \nThe consistency loss $\\mathcal{L}^{\\text{cons}}$ is deployed to encourage \nthe prediction consistency between the student and teacher models on the unlabeled data $\\mathcal{D}^u$:\n\\begin{equation}\n\\label{eq:cons-loss}\n\\mathcal{L}^{\\text{cons}} = \n\t\\mathbb{E}_{x^u\\in\\mathcal{D}^u} \\left[\\ell_{\\text{KL}}\\left(P_{\\theta_{S}}(x^u), P_{\\theta_T}(x^u)\\right)\\right]\n\\end{equation}\nwhere $\\ell_{\\text{KL}}(\\cdot,\\cdot)$ denotes the Kullback-Leibler divergence \nbetween two probability distributions.\nBy enforcing consistency, this loss encourages the student model to make more confident and reliable predictions, \nreducing the impact of random or misleading information in the training set.\nIt also acts as a form of regularization, discouraging the student model from overfitting to the labeled data.\n\n\\begin{algorithm}[t]\n  \\caption{Pseudo-Label Based Policy Gradient Descent}\n  \\begin{algorithmic}\n\t  \\State{{\\bf Input:} $\\mathcal{D}^l, \\mathcal{D}^u$, and extended  $\\widetilde{\\mathcal{D}}^l$;\n\t  \\quad initialized $\\theta_S, \\theta_T$;\\quad hyperparameters}\n\t  \\vskip .01in\n\\For{$iteration=1$ to maxiters}\n\\For{${x}_i^u \\in \\mathcal{D}^u$}\n\t  \\State Compute soft pseudo-label vector ${\\bf y}_i^u = P_{\\theta_T}({x}_i^u)$ to form \n\t  $({x}_i^u, {\\bf y}_i^u)$\n\\EndFor\n\\State Generate mixup data $\\mathcal{D}^m$ =$(X^{m},Y^{m})$ on $\\mathcal{D}^u$ and $\\widetilde{\\mathcal{D}}^l$ \n\t  using Eq.(\\ref{eq:mix}) with shuffling\n\\For{$step=1$ to maxsteps} \n\t  \\State Draw a batch of data $B=\\{({x}_i^m, {\\bf y}_i^m)\\}$ from $\\mathcal{D}^m$\n\t  \\State Calculate the reward function $\\mathcal{R}(\\cdot;sg[\\theta_S])$ using the batch $B$\n\t  \\State Compute the objective in Eq.(\\ref{eq:total-loss})\n\\State Update the policy parameters $\\theta_S$ via gradient descent \n\t  \\State Update teacher model $\\theta_T$ via EMA in Eq.(\\ref{eq:ema})\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\label{alg:batch-wise2}\n\\end{algorithm}\n\n\n\\subsection{Training Algorithm for RL-Guided SSL}\n\\label{sec:optimization-process}\n\nThe learning objective for the RLGSSL approach is formed by combining the reinforcement learning loss\n$\\mathcal{L}_\\text{rl}$ with the two augmenting loss terms, \nthe supervised loss $\\mathcal{L}_\\text{sup}$ and the consistency loss $\\mathcal{L}_{\\text{cons}}$,\nusing hyperparameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n\\label{eq:total-loss}\n\\mathcal{L}(\\theta_{S}) =\\mathcal{L}_{\\text{rl}} + \\lambda_1 \\mathcal{L}_{\\text{sup}} +\\lambda_2 \\mathcal{L}_{\\text{cons}}\n\\end{equation}\nBy deploying such a joint loss,\nthe RLGSSL framework can benefit from the strengths of \nboth reinforcement exploration and semi-supervised learning. \nThe RL component, in particular, introduces a dynamic aspect to the learning process, enabling the model to improve iteratively based on its own experiences. This innovative combination of losses allows the model to effectively learn from limited labeled data while still exploiting the abundance of unlabeled data.\n\nWe develop a stochastic batch-wise gradient descent algorithm to minimize the joint objective in Eq.(\\ref{eq:total-loss})\nfor RL-guided semi-supervised training. \nThe algorithm is summarized in Algorithm~\\ref{alg:batch-wise2}.\n\n\\section{Experiments}\n\\subsection{Experimetal Setup}\n\n\\paragraph{Datasets}\nWe conducted comprehensive experiments on three \nimage classification benchmarks: CIFAR-10, CIFAR-100 \\cite{krizhevsky2009learning}, and SVHN \\cite{netzer2011reading}. \nWe adhere to the conventional dataset splits used in the literature.\nCIFAR-10 and CIFAR-100 are split into 50,000 training images and 10,000 test images.\nThe SVHN dataset\nprovides 73,257 images for training and 26,032 images for testing.\nConsistent with previous works, we preserved the labels of a randomly selected subset of training samples with an equal number of samples for each class in CIFAR and SVHN, and left the remaining samples unlabeled. \nWe performed experiments on CIFAR-10 with 4000, 2000, and 1000 labeled samples, on CIFAR-100 with 10000 and 4000 labeled samples, and on SVHN with 1000 and 500 labeled samples.\n\n\n\\paragraph{Implementation Details}  \nIn our study, we adopted the data augmentation strategy from previous works \\cite{luo2018smooth,tarvainen2017mean} by applying random $2 \\times 2$ translations to the training set images.  \nWe conducted experiments using two different\nnetwork architectures: a 13-layer Convolutional Neural Network (CNN-13) and a Wide-Residual Network with 28 layers and a widening factor of 2 (WRN-28). For training the CNN-13 architecture, we employed the Stochastic Gradient Descent (SGD) optimizer with a Nesterov momentum of 0.9. We used an L2 regularization coefficient of 1e-4 for the CIFAR-10 and CIFAR-100 datasets, and 5e-5 for the SVHN dataset. The initial learning rate \nwas set to 0.1. To schedule the learning rate effectively, we utilized the cosine learning rate annealing technique as proposed in \\cite{loshchilov2017sgdr,verma2022interpolation}. For the WRN-28 architecture, we followed the suggestion from \\cite{berthelot2019mixmatch} and used an L2 regularization coefficient of 4e-4. \nTo compute the parameters of the teacher model, \nwe employed the EMA method with a decay rate $\\beta$ of 0.999. \nWe selected all hyperparameters and training techniques based on relevant studies to ensure a fair comparison between our approach and the existing methods. \nSpecifically for RLGSSL, we set the batch size to 128, $\\lambda_1$ to 0.1, and $\\lambda_2$ to 0.1. Before beginning with RLGSSL, we pre-train the model for 50 epochs using the Mean-Teacher algorithm. We then proceed to train RLGSSL for 400 epochs. We conducted five runs of the experiments and reported the mean test errors with their standard deviations.\n\n\n\\begin{table}[t]\n\\centering\n\\caption{ Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report\nthe average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:cifar}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\nDataset                   & \\multicolumn{3}{c|}{CIFAR-10}                        & \\multicolumn{2}{c}{CIFAR-100}                                                              \\\\ \nNumber of Labeled Samples & {1000}           & {2000}           & {4000}           & {4000}           & 10000          \\\\\n\\hline\nSupervised & ${39.95}_{(0.75)}$ & ${27.67}_{(0.12)}$ & ${20.42}_{(0.21)}$ & ${58.31}_{(0.89)}$ & ${44.56}_{(0.30)}$ \\\\\nSupervised + MixUp \\cite{zhang2018mixup} & ${31.83}_{(0.65)}$ & ${24.22}_{(0.15)}$ & ${17.37}_{(0.35)}$ & ${54.87}_{(0.07)}$ & ${40.97}_{(0.47)}$ \\\\\n$\\Pi$-model \\cite{laine2017temporal} & ${28.74}_{(0.48)}$ & ${17.57}_{(0.44)}$ & ${12.36}_{(0.17)}$ & ${55.39}_{(0.55)}$ & ${38.06}_{(0.37)}$ \\\\\nTemp-ensemble \\cite{laine2017temporal} & ${25.15}_{(1.46)}$ & ${15.78}_{(0.44)}$ & ${11.90}_{(0.25)}$ & {-} & ${38.65}_{(0.51)}$ \\\\\nMean Teacher\\cite{tarvainen2017mean} & ${21.55}_{(0.53)}$ & ${15.73}_{(0.31)}$ & ${12.31}_{(0.28)}$ & ${45.36}_{(0.49)}$ & ${35.96}_{(0.77)}$ \\\\\nVAT \\cite{miyato2018virtual} & ${18.12}_{(0.82)}$ & ${13.93}_{(0.33)}$ & ${11.10}_{(0.24)}$ & {-} &{-} \\\\\nSNTG \\cite{luo2018smooth} & ${18.41}_{(0.52)}$ & ${13.64}_{(0.32)}$ & ${10.93}_{(0.14)}$ & {-} & ${37.97}_{(0.29)}$ \\\\\nLearning to Reweight \\cite{ren2018learning} & ${11.74}_{(0.12)}$ & -& ${9.44}_{(0.17)}$ & ${46.62}_{(0.29)}$ & ${37.31}_{(0.47)}$ \\\\\nMT + Fast SWA \\cite{athiwaratkun2019there} & ${15.58}$ & ${11.02}$ & ${9.05}$ & {-} & ${33.62}_{(0.54)}$ \\\\\nICT \\cite{verma2022interpolation} & ${12.44}_{(0.57)}$ & ${8.69}_{(0.15)}$ & ${7.18}_{(0.24)}$ & ${40.07}_{(0.38)}$ & ${32.24}_{(0.16)}$ \\\\\nRLGSSL (Ours) & $\\mathbf{9.15}_{(0.57)}$ & $\\mathbf{6.90}_{(0.11)}$ & $\\mathbf{6.11}_{(0.10)}$ &$\\mathbf{36.92}_{(0.45)}$& $\\mathbf{29.12}_{(0.20)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Performance of RLGSSL and state-of-the-art SSL algorithms with the CNN-13 network. We report the average test errors and the standard deviations of 5 trials.}\n\\vskip .05in\n\\label{table:svhn}\n\\setlength{\\tabcolsep}{2pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c}\n\\hline\n & VAT \\cite{miyato2018virtual} & $\\Pi$-model \\cite{laine2017temporal} & Temp-ensemble \\cite{laine2017temporal} & MT \\cite{tarvainen2017mean} & ICT \\cite{verma2022interpolation} & SNTG \\cite{luo2018smooth} & RLGSSL (Ours)\\\\\n\\hline\nSVHN\/500 & {-} & ${6.65}_{(0.53)}$ & ${5.12}_{(0.13)}$ & ${4.18}_{(0.27)}$ & ${4.23}_{(0.15)}$ & ${3.99}_{(0.24)}$ & $\\mathbf{3.12}_{(0.07)}$ \\\\\nSVHN\/1000 & ${5.42}_{(0.00)}$ & ${4.82}_{(0.17)}$ & ${4.42}_{(0.16)}$ & ${3.95}_{(0.19)}$ & ${3.89}_{(0.04)}$ & ${3.86}_{(0.27)}$ & $\\mathbf{3.05}_{(0.04)}$ \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.05in\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption{Performance of RLGSSL with the WRN-28 network. Average test errors and standard deviations of\n5 trials are reported.}\n\\vskip .05in\n\\label{table:wrn}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n{Dataset} & \\multicolumn{3}{|c|}{CIFAR-10}                        & \\multicolumn{1}{c}{CIFAR-100}&\\multicolumn{1}{|c}{SVHN} \\\\\nNumber of Labeled Samples                                         & 1000     & 2000     & 4000     & 10000  &1000  \\\\\n\\hline\nMean Teacher \\cite{tarvainen2017mean}& $17.32_{(4.00)}$ & $12.17_{(0.22)}$ & $10.36_{(0.25)}$ & - &$5.65_{(0.45)}$ \\\\\nVAT \\cite{miyato2018virtual} & $18.68_{(0.40)}$ & $14.40_{(0.15)}$ & $11.05_{(0.31)}$ & - &${5.35_{(0.19)}}$\\\\\nMixMatch\\cite{berthelot2019mixmatch} & $7.75_{(0.32)}$ & $7.03_{(0.15)}$ & $6.24_{(0.06)}$ & $30.84_{(0.29)}$ &$3.27_{(0.31)}$\\\\\nMeta Pseudo-Labels \\cite{pham2021meta} & - & - & $3.89_{(0.07)}$ & -&$1.99_{(0.07)}$\\\\\nRLGSSL (Ours) & $\\mathbf{4.92}_{(0.25)}$ & $\\mathbf{4.24}_{(0.10)}$ & $\\mathbf{3.52}_{(0.06)}$ & $\\mathbf{28.82}_{(0.22)}$&$\\mathbf{1.92}_{(0.05)}$\\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table}[t]\n\\vskip -.05in\n\\centering\n\\caption{Ablation study results. We report the test errors on CIFAR-100 with 10000, and 4000 labels on  CNN-13 backbone 5 trials.}\n\\vskip .05in\n\\label{table:main-ablation}\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\n & RLGSSL & $- \\text{w\/o } \\mathcal{L}_\\text{rl}$ & $- \\text{w\/o } \\mathcal{L}_\\text{sup}$ & $- \\text{w\/o } \\mathcal{L}_\\text{cons}$ & $- \\text{w\/o }$ EMA & $- \\text{w\/o }$ mixup\\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${44.92}_{(0.55)}$ & ${39.52}_{(0.58)}$ & ${38.78}_{(0.48)}$ & ${43.12}_{(0.52)}$ & ${40.12}_{(0.51)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${33.12}_{(0.52)}$ & ${32.67}_{(0.45)}$ & ${31.48}_{(0.32)}$ & ${32.84}_{(0.45)}$ & ${31.48}_{(0.32)}$   \\\\\n\\hline\n\t& RLGSSL & $\\mathcal{R}=1 $ & $\\mathcal{R}:\\mu=0$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$ & $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$  & $\\mathcal{R}:$ w\/o sg[$\\theta$] \\\\\n\\hline\nCIFAR-100\/4000 & $\\mathbf{36.92}_{(0.45)}$ & ${39.52}_{(0.63)}$ & ${39.54}_{(0.33)}$ & ${38.02}_{(0.42)}$ & ${39.52}_{(0.45)}$ & ${40.62}_{(0.55)}$ \\\\\nCIFAR-100\/10000 & $\\mathbf{29.12}_{(0.20)}$ & ${31.25}_{(0.62)}$ & ${32.37}_{(0.57)}$ & ${31.12}_{(0.52)}$ & ${31.39}_{(0.68)}$ & ${32.12}_{(0.62)}$   \\\\\n\\hline\n\\end{tabular}}\n\\vskip -.1in\n\\end{table}\n\n\n\\subsection{Comparison Results}\nWe compare RLGSSL with \na great number of SSL algorithms, \nincluding \nSupervised + MixUp \\cite{zhang2018mixup}, $\\Pi$-model \\cite{laine2017temporal}, Temp-ensemble \\cite{laine2017temporal}, Mean Teacher \\cite{tarvainen2017mean}, VAT \\cite{miyato2018virtual}, SNTG \\cite{luo2018smooth}, Learning to Reweight \\cite{ren2018learning}, MT + Fast SWA \\cite{athiwaratkun2019there}, MixMatch \\cite{berthelot2019mixmatch}, Meta Pseudo-Labels \\cite{pham2021meta},  ICT \\cite{verma2022interpolation}, using CNN-13 or WRN-28 as the backbone network. \n\nTable \\ref{table:cifar} reports the comparison results on CIFAR-10 with 4000, 2000, and 1000 labeled samples and on CIFAR-100 with 10000 and 4000 labeled samples when CNN-13 is used as the backbone network.\nFor CIFAR-10, RLGSSL outperforms the compared methods across all \nsettings with different numbers of labeled samples, i.e., 1000, 2000, and 4000. \nIn terms of CIFAR-10 performance with 1000 labeled samples, RLGSSL surpasses ICT, the second best method, by a significant margin of $3.29\\the second best method, with a margin  of $3.15\\\nTable \\ref{table:svhn} reports the comparison results on the SVHN dataset with CNN-13 as the backbone network. \nOur method, RLGSSL, surpasses all other SSL techniques for both settings. \nSpecifically, for 500 labeled samples, RLGSSL achieves a test error of $3.12\\\nTable \\ref{table:wrn} reports the performance of the proposed RLGSSL method with the WRN-28 network on different datasets.\nOn the CIFAR-10 dataset, with the number of labeled samples increasing from 1000 to 4000, RLGSSL showed consistently better performance compared to other methods. For 1000 labeled samples, our method improved over the best competing method, MixMatch, by a margin of $2.83\\\nThese results show the effectiveness of RLGSSL, demonstrating its consistent superior performance across different numbers of labeled samples. Furthermore, RLGSSL's dominance across both CIFAR and SVHN datasets substantiates its robustness and adaptability to different datasets and scenarios.\n\n\n\\subsection{Ablation Study}\n\n\nIn order to evaluate the significance of various components of our RLGSSL approach, we executed an ablation study on the CIFAR-100 dataset using the CNN-13 network. \nIn particular, we compared the full model RLGSSL with the following variants: \n(1) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{rl}$\u201d, which does not incorporate the Reinforcement Learning (RL) loss term; \n(2) \u201c $\\; - \\text{w\/o } \\mathcal{L}_\\text{sup}$\u201d, which excludes \nthe supervised loss term; \n(3) \u201c$\\; - \\text{w\/o } \\mathcal{L}_\\text{cons}$\u201d, which  does not include the consistency loss term; \n(4) \u201c$\\; - \\text{w\/o }$ EMA\u201d, which drops the teacher model \nby disabling the EMA update; \nand \n(5) \u201c$-\\text{w\/o }$ mixup\u201d, which only uses unlabelled data in the reward function ($\\mu=1$), and the mixup operation is excluded.\nThe ablation results are reported in the top section of Table \\ref{table:main-ablation}.\nThe full model, RLGSSL, achieved the lowest test error, confirming the overall effectiveness of our method. The most significant observation from our study lies in the removal of the RL loss, denoted as $\\mathcal{L}_{\\text{rl}}$. Upon removal of this component, we witness a substantial increase in test errors, which highlights the indispensable role played by the RL component in our model.  The ablation study further illustrates the importance of each component \nby analyzing the performance of the model when the component is removed. \nIn each of these cases, we observe that the removal of any individual component consistently leads to an increase in test errors. This finding underpins the notion that each component of the RLGSSL model plays a significant role in the overall performance of the model.\n\n\nIn addition, we also conducted another set of ablation study centered on the proposed RL loss and the reward function.\nWe compared the full model RLGSSL with the following variants: \n(1) \"$\\mathcal{R}=1$\", which drops RL by setting the reward as a constant 1;   \n(2) $\\mathcal{R}:\\mu=0$, which only uses labeled data to compute the reward by setting $\\mu=0$;   \n(3) $\\mathcal{R}(L_2\\rightarrow\\mbox{KL})$, \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the KL-divergence distance; \n(4) $\\mathcal{R}(L_2\\rightarrow\\mbox{JS})$,    \nwhich replaces the squared L2 norm distance in the reward function in Eq.(\\ref{eq:reward}) \nwith the JS-divergence distance; \nand \n(5) $\\mathcal{R}:$ w\/o sg[$\\theta$], which removes the stop-gradient operator from the reward function\nand makes the reward function differentiable w.r.t $\\theta$.\nThe ablation results are reported in the bottom section of Table \\ref{table:main-ablation}.\nWe can see that all these variants with altered reward functions \nproduced degraded performance comparing to the full model\nwith the proposed reward function. \nIn particular, the performance degradation of \"$\\mathcal{R}=1$\" and \"$\\mathcal{R}:$ w\/o sg[$\\theta$]\" \nthat drop RL in different manners\nfurther validates the contribution of the proposed framework of guiding SSL with RL. \n\n\n\\subsection{Hyperparameter Analysis}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_1.png}\n\\caption{$\\lambda_1$}\n\\end{subfigure}\\qquad\\quad\n\\begin{subfigure}{0.40\\textwidth}\n\\centering\n\\includegraphics[width = .9\\textwidth, height=1.25in]{images\/lambda_2.png}\n\\caption{$\\lambda_2$}\n\\end{subfigure}\n\\caption{Sensitivity analysis for four hyperparameters $\\lambda_1$ and  $\\lambda_2$ CIFAR-100 using 10000 labeled samples (a) $\\lambda_1$, (b) $\\lambda_2$.}\n\\label{fig:hyper_sen}\n \\vskip -.15in\n\\end{figure}\nWe conduct sensitivity analysis over the two hyperparameters of the proposed RLGSSL: \n$\\lambda_1$---the trade-off parameter for the supervised loss, \nand $\\lambda_2$---the trade-off parameter for the consistency loss. \n\nThe results are reported in Figure \\ref{fig:hyper_sen}.\nIn the case of $\\lambda_1$, lower values (e.g., $1e-4$ and $1e-3$) result in less emphasis on the supervised loss term in the objective function. As a result, the model might not learn effectively from the limited available labeled data, \nleading to increased test error rates. \nConversely, higher values of $\\lambda_1$ (e.g., $0.2$ and $0.3$) may overemphasize the supervised loss term, \npotentially causing the model to overfit to the labeled data and ignore useful information from the unlabeled data. \nThe sweet spot lies in the middle (around $0.1$), where the model strikes a balance between learning from labeled data and leveraging information from unlabeled data.\n\nRegarding $\\lambda_2$, very low values (e.g., $1e-4$ and $1e-3$) may not enforce sufficient consistency in the model predictions on unlabeled data, resulting in a model that fails to generalize well. \nHowever, if $\\lambda_2$ is too high (e.g., $0.2$ and $0.3$), the model may overemphasize the consistency constraint, possibly leading to a model that is too rigid to capture the diverse patterns in the data. An optimal value of $\\lambda_2$ (around $0.1$ in our experiments) ensures a good balance between encouraging prediction consistency and allowing the model to adapt to the diverse patterns in the data.\n\nThe optimal value choice for hyperparameters $\\lambda_1$ and $\\lambda_2$ (around 0.1) \nalso validates that the RL loss is the main leading term, while the supervised loss and consistency loss are augmenting terms. \n\n\n\n\\section{Conclusion}\nIn this paper, we presented Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a unique approach that integrates the principles of RL to tackle the challenges inherent in SSL. \nThis initiative was largely driven by the limitations of conventional SSL techniques. \nRLGSSL employs a distinctive strategy where an RL-optimized reward function is utilized. \nThis function adaptively promotes better generalization performance through\nmore effectively leveraging both labeled and unlabeled data.\nWe also further incorporated a student-teacher framework to integrate the strengths of RL and SSL. \nExtensive evaluations were conducted on multiple benchmark datasets,\ncomparing RLGSSL to existing state-of-the-art SSL techniques. \nRLGSSL consistently outperformed these other techniques across all the datasets, which attests to the effectiveness and generalizability of our approach. The results underline the potential of integrating RL principles into SSL, and the RLGSSL method introduced in this paper is a significant stride in this direction. \n\n\n\\bibliographystyle{ieeetr}\n\\bibliography{neurips_2023}\n\n\\end{document}\n\n","rating_answer":"1. Originality: 10\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 10\n8. Overall score: 9\n"}
