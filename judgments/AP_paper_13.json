{"title":"AP_paper_13","field":"Statistics","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"ETH Zurich","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{ETH Zurich, Switzerland}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Lausanne","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Lausanne, Switzerland}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 7\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Zurich University of Applied Sciences (ZHAW)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Zurich University of Applied Sciences (ZHAW), Switzerland}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 10\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.67"}
{"title":"AP_paper_13","field":"Statistics","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Cambridge","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Cambridge, England}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8 \n4. Understandability 7\n5. Relevance 8\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AP_paper_13","field":"Statistics","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Dundee","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Dundee, England}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.3"}
{"title":"AP_paper_13","field":"Statistics","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Wolverhampton","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Wolverhampton, England}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 10\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 10\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 10\n8. Overall score: 9"}
{"title":"AP_paper_13","field":"Statistics","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Peking University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Peking University, China}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 10\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AP_paper_13","field":"Statistics","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Dalian University of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Dalian University of Technology, China}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AP_paper_13","field":"Statistics","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"China University of Mining and Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{China University of Mining and Technology, China}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AP_paper_13","field":"Statistics","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Indian Institute of Technology Bombay (IITB)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Indian Institute of Technology Bombay (IITB), India}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 10\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 10\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.5"}
{"title":"AP_paper_13","field":"Statistics","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Delhi","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Delhi, India}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10 \n2. Method 10 \n3. Credibility 10 \n4. Understandability 9 \n5. Relevance 10 \n6. Quality of Citations 9 \n7. Linguistic style and soundness of grammar 10  \n8. Overall score 9.5  "}
{"title":"AP_paper_13","field":"Statistics","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Indian Institute of Technology Hyderabad (IITH)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Indian Institute of Technology Hyderabad (IITH), India}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 8\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Sharif University of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Sharif University of Technology, Iran}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Shiraz University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Shiraz University, Iran}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Ferdowsi University of Mashhad","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Ferdowsi University of Mashhad, Iran}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 8\n2. Method 9\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.5"}
{"title":"AP_paper_13","field":"Statistics","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Massachusetts Institute of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Massachusetts Institute of Technology, USA}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 10\n3. Credibility 10\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9"}
{"title":"AP_paper_13","field":"Statistics","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Houston","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Houston, USA}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 9\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 10\n8. Overall score: 9"}
{"title":"AP_paper_13","field":"Statistics","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Western Washington University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Western Washington University, USA}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Sao Paulo","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Sao Paulo, Brazil}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 10\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Universidade de Bras\u00edlia","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Universidade de Bras\u00edlia, Brazil}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.75"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS, Brazil}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Cairo University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Cairo University, Egypt}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.5"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Alexandria University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Alexandria University, Egypt}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 10\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9.0"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Zagazig University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Zagazig University, Egypt}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9.17"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"The University of Melbourne","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{The University of Melbourne, Australia}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 7\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"The University of Adelaide","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{The University of Adelaide, Australia}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 9\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.17"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Queensland University of Technology (QUT)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Queensland University of Technology (QUT), Australia}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Technical University of Munich","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Technical University of Munich, Germany}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 10\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Potsdam","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Potsdam, Germany}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 9\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AP_paper_13","field":"Statistics","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"Leibniz University Hannover","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Leibniz University Hannover, Germany}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 10\n4. Understandability 10\n5. Relevance 10\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 10\n8. Overall score 10"}
{"title":"AP_paper_13","field":"Statistics","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of Cape Town","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Cape Town, South Africa}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.5"}
{"title":"AP_paper_13","field":"Statistics","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of South Africa","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of South Africa, South Africa}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AP_paper_13","field":"Statistics","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","input_length":13558,"university_association":"University of the Free State","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of the Free State, South Africa}\n\\title{{\\bf \\Large Overcoming model uncertainty -- how equivalence tests can benefit from model averaging\n}\n\\maketitle\n\n\\begin{abstract}\n    A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. \n    Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.\n    In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.\n\n    In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\nIn numerous research areas, particularly in clinical trials, a common problem is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups \\citep[see, e.g.,][]{Otto2008,Jhee2004}. \nEquivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.\nThe choice of this threshold is crucial as it resembles the maximal amount of deviation for which equivalence can still be concluded. One usually chooses the threshold based on prior knowledge, as a percentile of the range of the outcome variable or resulting from regulatory guidelines.\nEquivalence tests provide a flexible tool for plenty of research questions. For instance, they can be used to test for equivalence across patient groups, e.g. based on gender or age, or between treatments.\nMoreover, they are a key ingredient of bioequivalence studies, investigating whether two formulations of a drug have nearly the same effect and are hence considered to be interchangeable \\citep[e.g.][]{moellenhoff2022, Hauschke2007}. \n\nClassical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest \\citep{Schuirmann1987, Lakens2017}. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, considering the entire covariate range, describing for instance the time window or a dose range, has recently been proposed by testing equivalence of whole regression curves. Such test are typically based on the principle of confidence interval inclusion \\citep{Liu2009, Gsteiger2011, Bretz2018}. However, a more direct approach applying various distance measures has been introduced by \\citet{Dette2018}, which turned out to be particularly more powerful. Based on this, many further developments, e.g. for different outcome distributions, specific model structures and\/or responses of higher dimensions, have been introduced \\citep[see, e.g.,][]{Moellenhoff2020, Moellenhoff2021, Moellenhoff2024, Hagemann2024}.\n\nAll these approaches have one thing in common: they base on the assumption that the true underlying regression model is known. In practice this usually implies that the models need to be chosen manually, either based on prior knowledge or visually. \nHence, these approaches might not be robust with regard to model misspecification and, consequently, suffer from problems like inflated type I errors or reduced power \\citep{Guhl2022, Dennis2019}.\nOne idea to tackle this problem is implementing a testing procedure which explicitly incorporates the model uncertainty. This can be based on a formal model selection procedure, see, e.g., \\citet{Moellenhoff2018}, who propose conducting a classical model choice procedure prior to preforming the equivalence test. \n\nAn alternative to this is the incorporation of a model averaging approach into the test procedure.\nAs outlined by \\cite{Bornkamp2015} model selection has some disadvantages compared to model averaging. Particularly, model selection is not stable in the sense that minor changes in the data can lead to major changes in the results \\citep{Breiman1996}. This also implies that model selection is non-robust with regard to outliers. In addition, the estimation of the distribution of post model selection parameter estimators is usually biased \n\\citep{Leeb2005, Leeb2008}.\nModel averaging is omnipresent whenever model uncertainty is present, which is, besides other applications, often the case in parametric dose response analysis. Besides practical applications, there are also several methodological studies regarding model averaging in dose-response studies \\citep[see, e.g.,][]{Schorning2016, Aoki2017, Buatois2018} and \\citet{Bornkamp2009}, who incorporated model averaging as an alternative to model selection in their widely used dose-finding method MCPMod.\n\nTherefore, in this paper, we propose an approach utilising model averaging rather than model selection. \nThere are frequentist as well as Bayesian model averaging approaches. \nThe former almost always use the smooth weights structure introduced by \\citet{Buckland1997}. These weights depend on the values of an information criterion of the fitted models. Predominantly, the Akaike information criterion \\citep[AIC;][]{AIC} is used but other information criteria can be used as well.\nWhile only few of the Bayesian approaches perform fully Bayesian inference \\citep[see, e.g.,][]{Ley2009}, the majority makes use of the fact that the posterior model probabilities can be approximated by weights based on the Bayesian information criterion \\citep[BIC;][]{BIC} that have the same smooth weights structure as the frequentist weights \\citep{Wasserman2000}. \n\nIn this paper, we propose an equivalence test incorporating model-averaging and hence overcoming the problems caused by model uncertainty. Precisely, we first make use of the duality between confidence intervals and hypothesis testing and propose a test based on the derivation of a confidence interval. By doing so, we both guarantee numerical stability of the procedure and provide confidence intervals for the measure of interest.\nWe demonstrate the usefulness of our method with the example of toxicological gene expression data. \nIn this application, using model averaging enables us to analyse the equivalence of time-response curves between two groups for 1000 genes of interest without the necessity of specifying all 2000 correct models separately, thus avoiding both a time-consuming model selection step and potential model misspecifications.\n\nThe paper is structured as follows: \nIn Section \\ref{sec:MA}, dose-response models and the concept of model averaging are succinctly discussed. \nIn Section \\ref{sec:EquiTest}, the testing approach is introduced, proposing three different variations. Finite sample properties in terms of Type I and II error rates are studied in Section \\ref{sec:simu}.\nSection \\ref{sec:case} illustrates the method using the toxicological gene expression example before Section \\ref{sec:conclusion} closes with a discussion. \n\n\n\\section{Model averaging for dose-response models} \\label{sec:MA}\n\\subsection{Dose-response models} \\label{sec:MA:regression}\nWe consider two different groups, indicated by an index $l = 1, 2$, with corresponding response variables $y_{lij}$ \nwith $\\mathcal{Y} \\subseteq \\RR$ denoting the set of all possible outcomes. There are $i = 1, ..., I_l$ dose levels and $j = 1, ..., n_{li}$ denotes the observation index within each dose level. For each group the total number of observations is $n_l$ and $n$ is the overall number of observations, i.e. $n_l = \\sum_{i=1}^{I_l} n_{li}$ and $n=n_1 + n_2$.\nFor each group we introduce a flexible dose-response model  \n$$y_{lij}=m_l(x_{li}, \\boldsymbol{\\theta}_{l}) + e_{lij}, \\quad j = 1, ..., n_{li}, \\quad i = 1, ..., I_l, \\quad l=1,2,$$\nwhere $x_{li} \\in \\mathcal{X} \\subseteq \\RR$ is the dose level, i.e. the deterministic explanatory variable. We assume the \nresiduals $e_{lij}$ to be independent, have expectation zero and finite variance $\\sigma^2_l$.\nThe function $m_l(\\cdot)$ models the effect of $x_{li}$ on $y_{lij}$ via a regression curve with $\\boldsymbol{\\theta}_{l} \\in \\RR^{\\dim(\\boldsymbol{\\theta}_{l})}$ being its parameter vector. \nWe assume $m_{l}(\\cdot)$ to be twice continuously differentiable.\nIn dose-response studies, as well as in time-response studies, often either a linear model\n\\begin{equation} \\label{eq:linear}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x,\n\\end{equation}\na quadratic model\n\\begin{equation} \\label{eq:quadratic}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} x + \\beta_{l2} x^2,\n\\end{equation}\nan emax model\n\\begin{equation} \\label{eq:emax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x}{\\beta_{l2} + x},\n\\end{equation}\nan exponential (exp) model \n\\begin{equation} \\label{eq:exp}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left( \\exp \\left( \\frac{x}{\\beta_{l2}} \\right) -1 \\right),\n\\end{equation} \na sigmoid emax (sigEmax) model\n\\begin{equation} \\label{eq:sigemax}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\frac{x^{\\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l3}} + x^{\\beta_{l3}}},\n\\end{equation}\nalso known as Hill model or 4pLL-model, or a beta model\n\\begin{equation} \\label{eq:beta}\n    m_l(x, \\theta_l) = \\beta_{l0} + \\beta_{l1} \\left(\\frac{\\left( \\beta_{l2} + \\beta_{l3}\\right)^{\\beta_{l2} + \\beta_{l3}}}{\\left( \\beta_{l2}\\right)^{\\beta_{l2}}+ \\left( \\beta_{l3}\\right)^{\\beta_{l3}}}\\right) \\left(\\frac{x}{s}\\right)^{\\beta_{l2}}  \\left(1 - \\frac{x}{s} \\right) ^{\\beta_{l3}},\n\\end{equation}\nwhere $s$ is a fixed scaling parameter, is deployed \n\\citep[][]{Bretz2005, Pinheiro2006, Pinheiro2014, Duda2022}.\nThese models strongly vary in the assumed underlying dose-response relation, e.g. in terms of monotony, and consequently in the shape of their curves. Therefore, choosing a suitable dose-response model is crucial for all subsequent analyses.  \n\nHowever, in practical applications the true underlying model shape is in general unknown. Thus, it might not always be clear which functional form of \\ref{eq:linear}-\\ref{eq:beta} should be imployed. A possible answer to this is implementing\nmodel averaging which, as outlined in Section \\ref{sec:intro}, has several advantages over the simpler alternative of model selection. \n\n\\subsection{Model averaging} \\label{sec:MA:MA}\nAs outlined before, frequentist as well as Bayesian model averaging approaches usually both use the same smooth weights structure introduced by \\citet{Buckland1997} and \\citet{Wasserman2000}, respectively. \nAccordingly, by leaving out the group index $l=1,2$ for better readability \nthe averaged model is given by \n\\begin{equation} \\label{eq:MA}\n    m(x, \\hat{\\boldsymbol{\\theta}}) := \\sum_{k=1}^K w_{k} \\, m_{k}(x, \\hat{\\boldsymbol{\\theta}}_k),\n\\end{equation}\nwhere the $m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}), k =1,..,K,$ correspond to the  $K$ candidate models,\n\\begin{equation} \\label{eq:weights}\n    w_{k} = \\frac{\\exp(0.5 \\, I(m_k(x, \\hat{\\boldsymbol{\\theta}}_{k}))}{\\sum_{\\Tilde{k}=1}^K \\exp(0.5 \\, I( m_{\\Tilde{k}}(x, \\hat{\\boldsymbol{\\theta}}_{\\Tilde{k}})))}\n\\end{equation}\nare the corresponding weights and $I(\\cdot)$ is an information criterion. Usually the AIC is used for frequentist model averaging, while the BIC is usually deployed for Bayesian model averaging \\citep{Schorning2016}.  \n\nDespite the prevalence of the AIC and BIC, other information criteria are sometimes used as well, e.g. the deviance information criterion \n\\citep[see, e.g.,][]{Price2011}.\nOccasionally, model averaging also bases on cross-validation or machine learning methods. \nFor a more general introduction to model averaging techniques the reader is refereed to e.g. \\citet{Fletcher_2018} or \\citet{Claeskens_2008} and an overview specifically focusing on dose-response models is given by \\citet{Schorning2016}.  \n\\subsection{Inference} \\label{sec:MA:inference}\nAs the parameter estimation is conducted for each of the candidate models separately, it is not influenced by the subsequent model averaging. Therefore, here the index $k$ is left out. \nInference can be based on an ordinary least squares (OLS) estimator, i.e. minimising \n$$\n\\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2.\n$$ \nAlternatively, under the assumption of normality of the residuals, i.e.\n$$e_{lij} \\stackrel{iid}{\\sim} N(0, \\sigma^2_{l}), \\quad l=1,2,$$\na maximum likelihood estimator (MLE) can also be deployed where the log-likelihood is given by\n\\begin{equation} \\label{eq:logLik}\n   \\ell(\\boldsymbol{\\theta}_{l}, \\sigma_l^2    ) = -\\frac{n_l}{2} \\ln(2\\pi\\sigma_l^2) - \\frac{1}{2\\sigma_l^2} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2. \n\\end{equation}\nUnder normality both approaches are identical and, hence, lead to the same parameter estimates $\\hat{\\boldsymbol{\\theta}}_{l}$. From \\eqref{eq:logLik} an MLE for the variance \n\\begin{equation} \\label{eq:var}\n\\hat{\\sigma}_l^2 = \\frac{1}{n_l} \\sum_{i=1}^{I_l} \\sum_{j=1}^{n_{li}} (y_{lij} - m_{l}(x_{li}, \\boldsymbol{\\theta}_{l}))^2, \\quad l=1,2\n\\end{equation}\ncan be derived as well. \nIn \\texttt{R} inference is performed with the function \\texttt{fitMod} from the package \\texttt{Dosefinding} \\citep{Bornkamp2009, Pinheiro2014} which performs OLS estimation. The value of the log-likelihood needed for the AIC or BIC is then calculated by plugging the OLS estimator into the log-likelihood \\eqref{eq:logLik}. \n\n\\section{Model-based equivalence tests under model uncertainty} \\label{sec:EquiTest}\n\n\\subsection{Equivalence testing based on confidence intervals} \\label{sec:EquiTest:CI}\nModel-based equivalence tests have been introduced in terms of the $L^2$-distance, the $L^1$-distance or the maximal absolute deviation (also called $L^\\infty$-distance) of the model curves \\citep{Dette2018, Bastian2024}. \nAlthough all of these approaches have their specific advantages and disadvantages as well as specific applications, subsequent research \\citep[see, e.g.,][]{Moellenhoff2018, Moellenhoff2020, Moellenhoff2021, Hagemann2024} is predominately based on the maximal absolute deviation due to its easy interpretability.\nAccordingly, we state the hypotheses \n\\begin{equation} \\label{eq:hypotheses}\n    H_0: d \\geq \\epsilon \\text{   vs.   } H_1: d < \\epsilon\n\\end{equation}\nof equivalence of regression curves with respect to \nthe maximal absolute deviation,\nthat is\n$$\nd = \\max_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|\n$$\nis the maximal absolute deviation of the curves and $\\epsilon$ is the pre-specified equivalence threshold, i.e. that a difference of $\\epsilon$ is believed\nnot to be clinically relevant.\nThe test statistic is given as the estimated maximal deviation between the curves\n\\begin{equation} \\label{eq:test_stat}\n    \\hat{d} = \\max_{x \\in \\mathcal{X}}|m_1(x, \\hat{\\boldsymbol{\\theta}}_{1}) - m_2(x, \\hat{\\boldsymbol{\\theta}}_{2})|.\n\\end{equation}\n\n As the distribution of $\\hat d$ under the null hypothesis is in general unknown, it is usually either approximated based on a parametric bootstrap procedure or by asymptotic theory.\nIn \\citet{Dette2018} the asymptotic validity of both approaches is proven, but the corresponding simulation study shows that the bootstrap test outperforms the asymptotic test in finite samples. For the bootstrap test several studies \\citep[see, e.g.,][]{Dette2018, Moellenhoff2018, Moellenhoff2020} show reasonable results for finite samples across applications. \n\nHowever, in light of practical application, this approach can have two disadvantages: First, it does not directly provide confidence intervals (CI) which provide useful information about the precision of the test statistic. \nFurther, they would have an important interpretation analogously to their interpretation in classical equivalence testing known as TOST \\citep[two one-sided tests;][]{Schuirmann1987}, where the bounds of the confidence interval are typically compared to the confidence region of $[-\\epsilon,\\epsilon]$. \n\nSecond, it requires the estimation of the models under the constraint of being on the edge of the null hypothesis, i.e. the maximal absolute deviation being equal to $\\epsilon$ \\citep[see Algorithm 1 in][]{Dette2018}.\nTechnically, this is usually conducted using augmented Lagrangian optimisation.\nHowever, with increasing model complexity, this becomes numerically challenging. In the context of model averaging, these numerical issues are particularly relevant since all models would need to be estimated jointly as they need to jointly fulfil the constraint. This leads to a potentially high dimensional optimisation problem with a large number of parameters. In addition, for model averaging the side constraint has a highly complex structure because with every parameter update not only the model curves change but also the model weights do.\n\nAs an alternative to approximating the distribution under the null hypothesis, we propose to test hypotheses \\eqref{eq:hypotheses} based on the well-known duality between confidence intervals and hypothesis testing \\citep{Aitchison1964}. This testing approach is similar to what \\citet[][Algorithm 1]{Bastian2024} introduced for the L$^1$ distance of regression models. \nTherefore, let $(-\\infty, u]$ be a one-sided lower $(1-\\alpha)$-CI for $d$ which we can rewrite as $[0, u]$ due to the non-negativity of $d$, i.e.\n$$\n\\Prob(d \\leq u)=\\Prob(d \\in (-\\infty, u])=\\Prob(d \\in [0, u]) \\geq 1-\\alpha.\n$$\n\nAccording to the duality between CI and hypothesis testing, we reject the null hypothesis and conclude equivalence if\n\\begin{equation} \\label{eq:test_decision}\n    \\epsilon > u.\n\\end{equation}\nThis testing procedure is an $\\alpha$-level test as\n\\begin{align*}\n    & \\, \\Prob_{H_0}(\\epsilon > u)\\\\\n    \\leq & \\, \\Prob(d > u) \\\\\n    =& \\, 1 - \\Prob(d \\leq u) \\\\\n    \\leq & \\, 1-(1-\\alpha) = \\alpha.\n\\end{align*}\n\nHowever, as the distribution of $\\hat d$ is in general unknown, obtaining $u$ is again a challenging problem. It is obvious from \\eqref{eq:test_decision} that the quality of the testing procedure crucially depends on the quality of the estimator for $u$. If the CI is too wide\nthe test procedure is conservative and lacks power. In contrast, a too narrow CI can lead to type I\nerror inflation due to not reaching the desired coverage probability $1- \\alpha$.\nWe propose three different possibilities to calculate the CI, namely\n\\begin{enumerate}\n    \\item CI based on a parametric percentile bootstrap,\n    \\item asymptotic CI based on the asymptotic distribution of $\\hat d$ derived by \\citet{Dette2018}, and\n    \\item a hybrid approach using the asymptotic normality of $\\hat d$ but estimating its standard error based on a parametric bootstrap.\n\\end{enumerate}\n\nOne-sided CIs based on a parametric percentile bootstrap can be constructed in the same way \\citet{Moellenhoff2018} proposed for two-sided CIs. \nIn order to do so, they obtain parameter estimates (either via OLS or maximum likelihood optimisation), generate bootstrap data from these estimates and calculate  the percentiles from the ordered bootstrap sample.\nThe resulting test is similar to what \\citet[][Algorithm 1]{Bastian2024} derived for the L$^1$ distance of regression models. That is\n$$\n\\left[0, \\hat{q}^*(1- \\alpha)\\right],\n$$\nwhere $\\hat{q}^*(1- \\alpha)$ denotes the $(1-\\alpha)$-quantile of the ordered bootstrap sample. \n\nAsymptotic CIs can be derived directly from test (5.4) of \\citet{Dette2018} and are given by \n\\begin{equation} \\label{eq:CIasymp}\n\\left[0, \\hat{d} + \\sqrt{\\frac{\\widehat{\\Var}(d)}{n}} z \\right]\n\\end{equation}\nwhere $z$ is the $(1-\\alpha)$-quantile of the standard normal distribution and $\\widehat{\\Var}(d)$ is the closed-form estimator for the variance of $d$ given by equation (4.7) of \\citet{Dette2018}. \nHowever, the asymptotic validity of this variance estimator is only given under the assumption that within $\\mathcal{X}$ there is only one unique value $x_0$ \nwhere the absolute difference curve attains its maximum, i.e.\n$x_0 = \\argmax_{x \\in \\mathcal{X}}|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})|$ \n and, moreover, that this value $x_0$ is known. \nThis does not hold in general as \\citet{Dette2018} give two explicit counterexamples in terms of two shifted emax or exponential models. In addition, in practical applications $x_0$ is in general not known and needs to be estimated. If the absolute deviation along $x$ is small, the estimation of $x_0$ can become unstable leading to an unstable variance estimator. \nMoreover, as mentioned before, the simulation study of \\citet{Dette2018} shows that for finite samples the bootstrap test is superior to the asymptotic test. \n\nGiven the disadvantages of the asymptotic CI and especially of the underlying variance estimator, we introduce a hybrid approach which is a combination of both approaches. It is based on the asymptotic normality of $\\hat d$ but estimates the standard error of $\\hat{d}$ based on a parametric bootstrap \nleading to \n$$\n\\left[0, \\hat{d} + \\widehat{\\se}(\\hat{d}) z \\right],\n$$\nwhere the estimator $\\widehat{\\se}(\\hat{d})$ of the standard error of $\\hat{d}$ is the empirical standard deviation of the bootstrap sample. \n\nUnder the assumptions introduced by \\citet{Dette2018}, all three approaches are asymptotically valid. For the test based on the asymptotic CI, this follows directly from \\citet{Dette2018}. This also applies to the hybrid CI-based test due to $\\widehat{\\se}(\\hat{d})$ being an asymptotically unbiased estimator for the standard error of $\\hat{d}$ as outlined by \\citet{Efron1994}.\nThe asymptotic validity of the \npercentile approach follows from \\citet[][Appendix: proof of Theorem 4]{Dette2018} analogously to how \\citet[][Appendix A.3]{Bastian2024} derived the validity of their approach from there.\nThe finite sample properties of the three methods are compared in Section \\ref{sec:simu:CI}.\n\n\n\\subsection{Model-based equivalence tests incorporating model averaging} \\label{sec:EquiTest:MA}\nWe now combine the model averaging approach presented in Section \\ref{sec:MA:MA} with the CI-based test introduced in Section \\ref{sec:EquiTest:CI}. For the asymptotic test that is estimating $m_l(x, \\hat{\\boldsymbol{\\theta}}_{l}), \\, l=1,2$ using \\eqref{eq:MA}\nwith model weights \\eqref{eq:weights} and then calculating the test statistic \\eqref{eq:test_stat}. \nSubsequently, the asymptotic CI \\eqref{eq:CIasymp} can be determined using the closed form variance estimator given by \\citet{Dette2018}. Using this CI, the test decision is based on decision rule \\eqref{eq:test_decision}.\n\nThe testing procedure of the percentile and hybrid approach is shown in Algorithm \\ref{alg1}, where the first two steps are essentially the same as for the asymptotic test. The percentile test is conducted by performing Algorithm step 4a, while conducting step 4b instead leads to the hybrid test. In the following we will refer to this as Algorithm \\ref{alg1}a and Algorithm \\ref{alg1}b, respectively.\n\n\\begin{breakablealgorithm} \\caption{} \\label{alg1} \n\n  \\begin{enumerate}\n      \\item Obtain parameter estimates $\\hat{\\boldsymbol{\\theta}}_{lk}, \\, k=1,...,K_l, \\, l = 1,2$ for the candidate models, either via OLS or maximum likelihood optimisation (see Section \\ref{sec:MA:inference}). \n      Determine the averaged models from the candidate models using \\eqref{eq:MA}, i.e. by calculating\n      $$\n      m_l(x, \\hat{\\boldsymbol{\\theta}_l}) = \\sum_{k=1}^{K_l} w_{lk} \\, m_{lk}(x, \\hat{\\boldsymbol{\\theta}}_{lk}), \\quad l=1,2,\n      $$ \n      with weights \\eqref{eq:weights} as well as the variance estimator $\\hat{\\sigma}_l^2, \\, l=1,2$ from \\eqref{eq:var}.\n      \\item  Calculate the test statistic \\eqref{eq:test_stat}.\n\t\\item Execute the following steps:\n\t\t\t\\begin{enumerate}[label=3.\\arabic*]\n\t\t\t\t\\item Obtain bootstrap samples by generating data according to the model parameters $\\hat{\\boldsymbol{\\theta}}_{l}, \\, l = 1,2$. Under the assumption of normality that is\n    $$\n    y^*_{lij} \\sim N(m_l(x_{li}, \\hat{\\boldsymbol{\\theta}}_{l}), \\hat{\\sigma}_l^2), \\quad i=1,...,n_l, \\, l=1,2.\n    $$\n\t\t\t\t\\item From the bootstrap samples, estimate the models  $m_{l}(x_{li}, \\hat{\\boldsymbol{\\theta}}^{*}_{l}), \\, l = 1, 2$ as in step $(1)$ and the test statistic\n\t\t\t\t\\begin{equation}\n\t\t\t\t    \\label{boot}\n        \\hat{d}^* = \\max_{x \\in \\mathcal{X}}|m_{1}(x, \\hat{\\boldsymbol{\\theta}}_{1}^{*}) - m_{2}(x, \\hat{\\boldsymbol{\\theta}}_{2}^{*})|.\n        \\end{equation}\n\t\t\\item Repeat steps (3.1) and (3.2) $n_{boot}$ times to generate replicates $\\hat{d}^*_{1}, \\dots, \\hat{d}^*_{n_{boot}}$ of $\\hat{d}^*$.\n            Let $\\hat{d}^*_{(1)} \\leq \\ldots \\leq \\hat{d}^*_{(n_{boot})}$\n\t\t\tdenote the corresponding order statistic. \n            \\end{enumerate}\t\n        \\item Calculate the CI using one of the following approaches: \n\\begin{enumerate}[label=\\alph*]\n\t\t\t\t\\item \\underline{Percentile CI}: Obtain the estimated right bound of the percentile bootstrap CI as the $(1-\\alpha)$-quantile of the bootstrap sample $$\n\\hat{u} = \\hat{q}^*(1- \\alpha)= \\hat{d}^*_{(\\lfloor n_{boot} (1-\\alpha) \\rfloor)}.\n$$\n    \\item \\underline{Hybrid CI}: Obtain the estimator for the standard error of $\\hat{d}$ as $\\widehat{\\se}(\\hat{d})= \\sqrt{\\widehat{\\Var}(\\hat{d}^*_1,...,\\hat{d}^*_{n_{boot}})}$ and the estimated right bound of the hybrid CI as\n    $$\\hat{u} = \\hat{d} + \\widehat{\\se}(\\hat{d}) z.$$\n            \\end{enumerate}\t\n\n        \\item \n            Reject the null hypothesis in \\eqref{eq:hypotheses} and assess equivalence if $$\\epsilon > \\hat{u}.$$\n\t\\end{enumerate}\t\t\n\n\\end{breakablealgorithm}\n\\section{Finite sample properties} \\label{sec:simu}\nIn the following we investigate the finite sample properties of the proposed tests by a simulation study. In order to ensure comparability, we reanalyse the simulation scenarios given by \\cite{Dette2018}.\nThe dose range is given by $\\mathcal{X}=[0,4]$ and data is observed for dose levels $x=0, 1,2,3$ and 4 with equal number of observations $n_{li} = \\frac{n_l}{5}$ for each dose level.\nAll three simulation scenarios use the same three variance configurations $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ as well as the same four different sample sizes $(n_1, n_2) \\in \\{(10, 10), (10, 20), (20, 20), (50, 50) \\}$ and the same significance level of $\\alpha = 0.05$.\n\nIn the first simulation scenario the equivalence of an emax model and an exponential model is investigated. The other two simulation scenarios consist of testing for equivalence of two shifted models, either both being emax models or both being exponential models. \nIn contrast to the first scenario where the absolute deviation of the models is observed at one unique $x_0$, this is not the case for the latter two scenarios. \nHere, the deviation of both models is constant across the whole dose range $\\mathcal{X}=[0,4]$, i.e. \n$$\n|m_1(x, \\boldsymbol{\\theta}_{1}) - m_2(x, \\boldsymbol{\\theta}_{2})| = d \\, \\, \\forall \\, \\, x \\in \\mathcal{X}\n$$\nas $m_1$, $m_2$ are just shifted. \n\nHence, for these two scenarios the asymptotic test is not applicable as its close form variance estimator bases on the uniqueness of $x_0$. Therefore, only simulation scenario 1 is used to compare the three CI-based tests to each other as well as to the results observed by \\citet{Dette2018}. \nSubsequently, all three scenarios are used to compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \n\n\\subsection{Finite sample properties of confidence interval-based equivalence testing} \\label{sec:simu:CI}\nPrior to the investigation of the effect of model averaging onto the finite sample properties, we first inspect the performance of the CI-based testing approach, i.e. test \\eqref{eq:test_decision}, itself. \nFor the asymptotic test, the CI is defined by \\eqref{eq:CIasymp}. \nFor the percentile as well as the hybrid approach, the tests are conducted as explained in Section \\ref{sec:EquiTest:CI} which is formally defined by setting $K_1 = K_2 = 1$ in Algorithm \\ref{alg1}. \n\nAs outlined before, simulation scenario 1 of \\citet{Dette2018} is given by tesing for the equivalence of an emax model \\eqref{eq:emax}\nwith $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (1, 2, 1)$ and an exponential model \\eqref{eq:exp}\nwith $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (\\beta_{20}, 2.2, 8)$.\nIt consists of 60 sub-scenarios resulting from the three different variance configurations each being combined with the four different sample sizes and\nfive different choices of $\\beta_{20} \\in \\{0.25, 0.5, 0.75, 1, 1.5\\}$, leading to the corresponding deviations of the regression curves being $d \\in \\{1.5, 1.25, 1, 0.75, 0.5\\}$.\nThe test is conducted for $\\epsilon = 1$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter two deviations correspond to the alternative and are used to estimate the power of the tests. \n\nAs the type I error rates are always smaller than the nominal level of $\\alpha = 0.05$ for all three approaches (see table S1 of the supplementary material for exact values), i.e. all testing approaches always hold the nominal level, the following analysis focuses on the power of the tests. \nFigure \\ref{fig:CI_Power} shows the power for all three tests for all sub-scenarios under the alternative as well as the corresponding power of the tests of \\cite{Dette2018}. In each sub-scenario we observe that the hybrid test has superior power compared to the other two CI-based tests. In addition, one can observe that the power achieved by the hybrid test is quite similar to the one \\citet{Dette2018} observed for their bootstrap test and, therefore, is also superior to the power of their asymptotic test. The power of the test based on the percentile CI is considerably smaller which indicates that the test might be overly conservative in finite samples. The test based on the asymptotic CI leads to nearly the same results as the asymptotic test of \\citet{Dette2018} which is not surprising as it is directly derived from it. Consequentially, the lack of power that \\citet{Dette2018} observed for their asymptotic test in comparison to their bootstrap test, is also present for the test based on the asymptotic CI. \n\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{CI_Power_final.pdf} \n        \\caption{Comparison of the power of the CI-based testing approaches to the testing approaches proposed by \\citet{Dette2018} with $\\epsilon = 1$. The results are shown for two distances of the regression curves $d \\in \\{0.5, 0.75\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.5, 0.5), (0.25, 0.5)\\}.$}\n        \\label{fig:CI_Power} \n\\end{figure}\n\nIn conclusion, the hybrid approach which provides numerically advantages compared to the bootstrap test of \\citet{Dette2018} and also leads to additional interpretability due to providing CIs, achieves nearly the same power as the bootstrap test while holding the nominal level. \n\n\\subsection{Finite sample properties under model uncertainty} \\label{sec:simu:MA}\nWe now investigate the finite sample properties under model uncertainty. Due to the clear superiority of the hybrid approach observed in Section \\ref{sec:simu:CI}, only the hybrid test is used for this analysis. \nWe compare the performance of the test using model averaging to the one based on the correct specification of the models as well as under model misspecification. \nWe use frequentist model averaging with AIC-based weights as the inference is frequentist as well. The corresponding equivalence tests are conducted using Algorithm \\ref{alg1}b.\n\n\\subsubsection{Comparison of an emax\nwith an exponential model} \\label{sec:simu:MA:1}\nFirst, we again investigate the first simulation scenario introduced in Section \\ref{sec:simu:CI} but now under model uncertainty where it is unclear if an emax or an exponential model applies for each of the groups implying $K_1 = K_2 = 2$ and leading to one correct specification, as well as three misspecifications. \nFigure \\ref{fig:Ex1_Type1} shows the corresponding type I error rates for all sub-scenarios under the null hypothesis, i.e. for $d \\in \\{1, 1.25, 1.5\\}$ and $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}$ for the correct specification, the three misspecifications as well as under model averaging.  \n\\begin{figure}  \n        \\centering\n        \\includegraphics[scale = 0.795]{Ex1_Power_final.pdf}\n        \\caption{Comparison of the type I error rates of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 1. The results are shown for $\\epsilon = 1$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex1_Type1}\n\\end{figure}\n\nOne can observe that falsely specifying the same model for both responses leads to highly inflated type I error rates which, in addition, even increase for increasing sample size.\nThe highest type I errors are present if an exponential model is specified for both responses leading to type I error rates being as large as 0.410 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$. \nIf an emax model is specified for both responses, the type I error inflation is smaller but still present and reaches up to 0.187 which is observed for $\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 50$.\nThe third misspecification under investigation is that specifying the models the wrong way round, i.e. an exponential model for the first group and an emax model for the second one. In comparison to the other two misspecifications, this leads to less extreme results but type I error inflation is still observable. This results from the fact that using one convex (exponential) and one concave (emax) model usually leads to a larger  maximal absolute deviation than using two convex or two concave models and, therefore, in general to fewer rejections of the null hypothesis.\n\nCompared to these results, the type I errors resulting from model averaging are closer to the nominal level of the test. However, for two out of the 36 investigated sub-scenarios ($\\sigma_1^2  = \\sigma_2^2 = 0.25$ and $n_1 = n_2 = 20$ as well as $n_1 = n_2 = 50$) the type I errors still exceeds the nominal level but to a much lesser extent compared to model misspecification, reaching a maximum of 0.061. As expected, when using the true underlying model, the test holds the nominal level of $\\alpha = 0.05$.\nComparison of the power of the tests is not meaningful as some of them are not holding the nominal level. However, the estimated power is shown in table S2 of the supplementary material.  \n\\subsubsection{Comparison of two shifted emax models} \\label{sec:simu:MA:2}\nWe continue by investigating the fine sample properties for the case of two shifted emax models, i.e. model \\ref{eq:emax} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 5, 1)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 5, 1)$, which implies $d = \\beta_{10}$. The levels of $d$ under investigation are $1, 0.75, 0.5, 0.25, 0.1$ and $0$. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. \nThe latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nWe only observe few type I error rates which are non-zero and these are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.003$ (all values can be found in table S3 of the supplementary material). Hence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex2_Power}.\n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Ex2_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 2. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex2_Power} \n\\end{figure}\nOne can observe falsely specifying one of the models to be an exponential model leads to the power being constantly equal to zero even for sub-scenarios which are quite far under the alternative. For misspecification in terms of using an exponential model for both responses, the power loss is not that extensive but still occurs for smaller sample sizes, which is especially visible for $\\sigma^2_1 = \\sigma^2_2 = 0.25$ due to the estimation uncertainty being  the smallest. \nIn contrast, model averaging results in nearly the same power as using the true model. This also leads to the fact that in some cases the black line is even hardly visual as it is nearly perfectly overlapped by the green one.\n\n\\subsubsection{Comparison of two shifted exponential models} \\label{sec:simu:MA:3}\nThe third simulation scenario is given by two shifted exponential models, i.e. model \\ref{eq:exp} now applies for both groups, where $\\theta_1 = (\\beta_{10}, \\beta_{11}, \\beta_{12}) = (\\beta_{10}, 2.2, 8)$ and $\\theta_2 = (\\beta_{20}, \\beta_{21}, \\beta_{22}) = (0, 2.2, 8)$ which implies $d = \\beta_{10}$, resulting in the same values for $d$ as in Section \\ref{sec:simu:MA:2}. The test is conducted for $\\epsilon = 0.5$ such that the first three deviations are under the null hypothesis and, therefore, used to investigate the type I error rates. The latter three deviations are under the alternative and used to estimate the power of the tests.  \n\nAs previously observed in Section \\ref{sec:simu:MA:2} only few type I error rates are non-zero and these exceptions are still much smaller than the nominal level of $\\alpha = 0.05$, reaching a maximum of only $0.009$ (all values can be found in table S4 of the supplementary material). \nHence, the analysis focuses on the power of the tests which is shown in Figure \\ref{fig:Ex3_Power}. \n\\begin{figure} \n        \\centering\n        \\includegraphics[scale = 0.79]{Ex3_Power_final.pdf}\n        \\caption{Comparison of the power of the test using the true model, the model averaging-based test and the tests under model misspecification in scenario 3. The results are shown for $\\epsilon = 0.5$, three distances of the regression curves $d \\in \\{1, 1.25, 1.5\\}$ and three different combinations of variances $(\\sigma_1^2, \\sigma_2^2) \\in \\{(0.25, 0.25), (0.25, 0.5), (0.5, 0.5)\\}.$}\n        \\label{fig:Ex3_Power} \n\\end{figure}\nThe loss of power resulting from model misspecification is not as large as in Section \\ref{sec:simu:MA:2} but still present. Especially if one of the models is falsely specified to be an emax model but the other one specified correctly, we observe a notable loss of power not only compared to using the true model but also compared to using model averaging.\nMoreover, this effect is increasing with increasing sample size.\nIn contrast to Section \\ref{sec:simu:MA:2}, the power resulting from using model averaging is notably smaller than the one observed when using the true model. However, compared to two out of the three misspecifications, the loss in power is extensively reduced.\nIn conclusion, if the models are misspecified we observe either type I error inflation or a lack of power, both often of substantial extend, in all three scenarios. Using model averaging considerably reduces these problems, often leading to similar results as knowing and using the true underlying model.\n\n\\section{Case study} \\label{sec:case}\nWe illustrate the proposed methodology through a case study analysing the equivalence of time-response curves (also known as exposure duration-response curves) using data which was originally published by \\citet{Ghallab2021}. The study aims to investigate dietary effects onto the gene expression.\nThe dataset consists of two groups of mice which were fed with two different diets and then sacrificed at different time points. The first one is a high-fat or ``Western\" diet (WD) while the other one is a standard diet (SD). As no data has been collected in the first 3 weeks, they are not included into our analysis. Consequentially, the beginning of the study ($t=0$) resembles week 3 of the actual experiment.\nData is then observed at $t=0, 3, 9, 15, 21, 27, 33, 39$ and $45$ for the Western diet and at $t=0, 3, 27, 33, 39$ and $45$ for the standard diet with sample sizes $5, 5, 5, 5, 5, 5, 5, 4, 8$\nand $7, 5, 5, 7, 3, 5$, respectively. \nFor each group the gene expression of 20733 genes is measured in terms of gene counts. For our analysis, we focus on the 1000 genes \\citet{Ghallab2021} classified as especially interesting due to high activity. \nAlthough gene expression is measured as count data, it is treated as continuous due to the very high number of counts. The raw count data is preprocessed in terms of the gene count normalisation conducted by \\citet{Ghallab2021} and subsequent $\\log_2$-transformation of the normalised counts as suggested by \\citet{Duda2022, Duda2023}.\n\nUsing this data, we aim to investigate the equivalence of the time-gene expressions curves between the two diets at a 5 \\\nAs time-response studies are relatively rare, no specific time-response models have been developed. Hence, dose-response models are deployed for time-response relations as well. Methodological review studies \\citep[e.g.][]{Kappenberg2023} do also not distinguish between dose-response and time-response studies. In addition, it seems intuitive that the effects of the high-fat diet accumulate with increasing time of consumption in a similar manner as the effects in dose response-studies accumulate with increasing dose. \n\nAs outlined by \\citet{Ghallab2021}, the dose-response relations vary across genes such that there is no single model which fits to all of them and, hence, model uncertainty is present. In addition, the models cannot be chosen manually due to the high number of genes. \nWe introduce frequentist model averaging using AIC-based weights and the equivalence tests are performed using hybrid CI, i.e. by conducting Algorithm \\ref{alg1}b. \nWe deploy the set of candidate models suggested by \\citet{Duda2022}, i.e. a linear model \\eqref{eq:linear}, a quadratic model \\eqref{eq:quadratic}, an emax model \\eqref{eq:emax}, an exponential model \\eqref{eq:exp}, a sigmoid emax model \\eqref{eq:sigemax} and a beta model \\eqref{eq:beta}. This set of candidate models can capture quite diverse effects, as it includes linear and nonlinear, increasing and decreasing, monotone and non-monotone as well as convex, concave and sigmoid curves. \n\n\nThe ranges of the response variables, i.e. the ranges of $\\log_2$(normalised counts), are not comparable across different genes. Hence, different equivalence thresholds are needed for each of the genes. As such thresholds can not be chosen manually due to the high number of genes, we determine the thresholds as a percentile of the range of the response variable. For a gene $g \\in \\{1,...,1000\\}$ that is \n$$\n\\epsilon_g = \\Tilde{\\epsilon} \\left( \\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli}) \\right),\n$$\nwhere $\\Tilde{\\epsilon} \\in (0,1)$ is the corresponding percentile and $\\Tilde{\\epsilon} = 0.2$ or 0.25 would be typical choices. Alternatively, one can proceed the other way around, calculate \n$$\n\\Tilde{u}_g = \\frac{u_g}{\\max_{l,i} (\\hat{y}_{gli}) - \\min_{l,i} (\\hat{y}_{gli})}\n$$ and directly compare $\\Tilde{u}_g$ to $\\Tilde{\\epsilon}$, i.e. the decision rules $\\epsilon_g > u_g$ and $\\Tilde{\\epsilon} > \\Tilde{u}_g$ are equivalent. \n\nSubfigures (a) and (b) of Figure \\ref{fig:case_weights} show boxplots of the model weights for both diets. It can be observed that less complex models, i.e. the linear and quadratic model, have higher weights for the standard diet compared to the Western diet. In contrast, for the two most complex models, i.e. the beta and sigEmax model, the opposite can be observed: they have higher weights for the Western diet compared to the standard diet.\n\\begin{figure}\n        \\centering\n        \\begin{overpic}[scale = 0.795]{Case_study_2in1_neu.pdf} \n            \\put(0,64){(a)}\n            \\put(69,64){(c)}\n            \\put(33.5,64){(b)}\n            \\put(69,30.5){(d)}\n            \\put(0,30.5){(e)}\n        \\end{overpic}\n        \\caption{Subfigures (a) and (b) show boxplots of the model weights for each of the two diets. Subfigures (c) and (d) show histograms of the highest model weight per gene for both genes. Subfigure (e) shows the number of genes for which equivalence between the time-gene expression curves of the two diets can be concluded in dependence of the equivalence threshold $\\Tilde{\\epsilon}$.}\n        \\label{fig:case_weights} \n\\end{figure}\nIn addition, Subfigures (c) and (d) of Figure \\ref{fig:case_weights} show histograms of the highest model weight per gene. It can be observed that for the Western diet the model weights tend to be larger compared to the standard diet. In addition, for the Western diet there are notably many genes for which the highest model weight is very close to 1, i.e. the averaged model consists nearly fully of only one of the candidate models. \n\n\nSubfigure (e) of Figure \\ref{fig:case_weights} shows the number of genes for which equivalence of the time-gene expression curves of both diets can be concluded, i.e. the number of genes for which H$_0$ can be rejected depending on the choice of $\\Tilde{\\epsilon}$. For very small choices of $\\Tilde{\\epsilon}$ (e.g. 0.05 or 0.075)  equivalence cannot be concluded for any gene and for $\\Tilde{\\epsilon} = 0.1$ only three genes would be assessed as equivalent. \nFor more typical choices of $\\Tilde{\\epsilon}$ being 0.2, 0.25 or 0.3, equivalence could be concluded for 37, 68 and 114 genes, respectively. With further increasing $\\Tilde{\\epsilon}$ the number of rejections also further increases and approaches 1000. However, this is not shown for $\\Tilde{\\epsilon} > 0.3$ as performing an equivalence test with a threshold larger than 30 \\\n\nFigure \\ref{fig:case_curves} shows the results for three exemplary genes. For ENSMUSG00000095335 it can be observed that both time-response curves are extremely close to each other and that the maximum absolute deviation of the curves is quite small. This leads to $\\Tilde{u}=0.098$. Regarding the model weights it can be observed that both time-response curves consist essentially of the same models. \nFor ENSMUSG00000024589 we observe that both time-response curves have a similar shape both being emax-like, although their model weights are not as similar as before. However, their distance is larger than for ENSMUSG00000095335 which leads to $\\Tilde{u} \\approx 0.275$. Hence, the curves are not equivalent for typical choices of $\\Tilde{\\epsilon}$ being e.g. 0.2 or 0.25 but only for a more liberal choice of $\\Tilde{\\epsilon}= 0.3$. For the last example ENSMUSG00000029816 we observe that the two curves are completely different with regard to both, shape and location. For the standard diet an almost constant curve is present while for the Western diet a typical emax shape is observable. This is also reflected by the model weights where models which have high weights for one curve, have small ones for the other one and vise versa, the only exempt to this is the emax model which has a medium large weight for both of the groups. Due to the large maximum absolute deviation between the curves given by $\\hat d=0.847$, similarity cannot be concluded for any reasonable equivalence threshold.  \n\\begin{figure}\n        \\centering\n        \\includegraphics[scale = 0.795]{Case_study_curves.pdf} \n        \\caption{Results for three exemplary genes. The first row of figures shows the data for both diets as well as the fitted models. The second row of figures shows the corresponding model weights.}\n        \\label{fig:case_curves} \n\\end{figure}\n\n\\section{Conclusion} \n\\label{sec:conclusion}\nIn this paper, we introduced a new approach for\nmodel-based equivalence testing which can also be applied in the presence of model uncertainty -- a problem which is usually faced in practical applications. \nOur approach is based on a flexible model averaging method which relies on information criteria and a testing procedure which makes use of the duality of tests and confidence intervals rather than simulating the distribution under the null hypothesis, providing a numerically stable procedure. \nMoreover, our approach leads to additional interpretability due to the provided confidence intervals while retaining the asymptotic validity and a similar performance in finite samples as the bootstrap based test proposed by \\citet{Dette2018}. \nPrecisely, we investigated the finite sample properties of the proposed method by reanalysing the simulation study of \\citet{Dette2018} and observed similar results for the CI-based test compared to their test. \nIn the presence of model uncertainty, model misspecification frequently led to either type I error inflation or a lack of power, both often of substantial extend. In contrast, our approach considerably reduced these problems and in many cases even achieved similar results as knowing and using the true underlying model. \nThe presented case study outlines the practical usefulness of the proposed method based on a large data application where choosing the models manually would be time-consuming and could easily lead to many model misspecifications.  \nHence, introducing model averaging here is essential to test for the equivalence of time-gene expression curves for such large numbers of genes, typically occurring in practice.\n\nFuture possible research includes extending the presented method for other model averaging techniques, e.g. cross validation-based model averaging. In addition, transferring this approach to other model classes (e.g. survival models) as well as to multidimensional responses, i.e. multiple endpoints, merits further exploration.\n\n\\section*{Software and data availability}\nSoftware in the form of R code  available at \\url{https:\/\/github.com\/Niklas191\/equivalence_tests_with_model_averaging.git}.\nThe case study data set is publicly available at the SRA database with reference number \\hyperlink{https:\/\/www.ncbi.nlm.nih.gov\/sra\/PRJNA953810}{PRJNA953810}.\n\n\\section*{Funding}\nThis work has been supported by the Research Training Group \"Biostatistical Methods for High-Dimensional Data in Toxicology`` (RTG 2624, P7) funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, Project Number 427806116).\n\n\\section*{Competing interests}\nThe authors declare no competing interests.\n\n\\input{main.bbl}\n\n\\end{document}\n","rating_answer":"1. Originality: 9\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9"}
