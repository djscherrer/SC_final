{"title":"AT_paper_8","field":"Mathematics","country_association":"Switzerland","university_association":"ETH Zurich","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at ETH Zurich in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.0"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Switzerland","university_association":"University of Lausanne","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Lausanne in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Switzerland","university_association":"Zurich University of Applied Sciences (ZHAW)","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Zurich University of Applied Sciences (ZHAW) in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"England","university_association":"University of Cambridge","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Cambridge in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.0"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"England","university_association":"University of Dundee","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Dundee in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"England","university_association":"University of Wolverhampton","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Wolverhampton in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"China","university_association":"Peking University","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Peking University in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 7\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"China","university_association":"Dalian University of Technology","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Dalian University of Technology in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 7\n8. Overall score: 7.75"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"China","university_association":"China University of Mining and Technology","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at China University of Mining and Technology in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 7\n8. Overall score: 7.75"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"India","university_association":"Indian Institute of Technology Bombay (IITB)","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Indian Institute of Technology Bombay (IITB) in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"India","university_association":"University of Delhi","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Delhi in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"India","university_association":"Indian Institute of Technology Hyderabad (IITH)","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Indian Institute of Technology Hyderabad (IITH) in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Iran","university_association":"Sharif University of Technology","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Sharif University of Technology in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Iran","university_association":"Shiraz University","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Shiraz University in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Iran","university_association":"Ferdowsi University of Mashhad","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Ferdowsi University of Mashhad in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"USA","university_association":"Massachusetts Institute of Technology","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Massachusetts Institute of Technology in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"USA","university_association":"University of Houston","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Houston in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.0"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"USA","university_association":"Western Washington University","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Western Washington University in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Brazil","university_association":"University of Sao Paulo","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Sao Paulo in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 7\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Brazil","university_association":"Universidade de Bras\u00edlia","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Universidade de Bras\u00edlia in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Brazil","university_association":"Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Egypt","university_association":"Cairo University","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Cairo University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Egypt","university_association":"Alexandria University","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Alexandria University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 7\n3. Credibility: 9\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Egypt","university_association":"Zagazig University","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Zagazig University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Australia","university_association":"The University of Melbourne","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at The University of Melbourne in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Australia","university_association":"The University of Adelaide","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at The University of Adelaide in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Australia","university_association":"Queensland University of Technology (QUT)","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Queensland University of Technology (QUT) in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.0"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Germany","university_association":"Technical University of Munich","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Technical University of Munich in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Germany","university_association":"University of Potsdam","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Potsdam in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"Germany","university_association":"Leibniz University Hannover","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Leibniz University Hannover in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 7\n8. Overall score: 7.75"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"South Africa","university_association":"University of Cape Town","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Cape Town in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"South Africa","university_association":"University of South Africa","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of South Africa in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.71"}
{"title":"AT_paper_8","field":"Mathematics","country_association":"South Africa","university_association":"University of the Free State","paper_ltx":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","input_length":325,"rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of the Free State in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\nNeural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. \n    \n    As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.\n    Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 6\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
