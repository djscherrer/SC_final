{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"ETH Zurich","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{ETH Zurich, Switzerland}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Lausanne","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Lausanne, Switzerland}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Zurich University of Applied Sciences (ZHAW)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Zurich University of Applied Sciences (ZHAW), Switzerland}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Cambridge","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Cambridge, England}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 8\n8. Overall score 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Dundee","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Dundee, England}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Wolverhampton","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Wolverhampton, England}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Peking University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Peking University, China}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Dalian University of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Dalian University of Technology, China}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 9\n4. Understandability 8 \n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"China University of Mining and Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{China University of Mining and Technology, China}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Indian Institute of Technology Bombay (IITB)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Indian Institute of Technology Bombay (IITB), India}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 8\n2. Method 9\n3. Credibility 8\n4. Understandability 7\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Delhi","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Delhi, India}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Indian Institute of Technology Hyderabad (IITH)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Indian Institute of Technology Hyderabad (IITH), India}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Sharif University of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Sharif University of Technology, Iran}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Shiraz University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Shiraz University, Iran}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Ferdowsi University of Mashhad","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Ferdowsi University of Mashhad, Iran}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 7\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Massachusetts Institute of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Massachusetts Institute of Technology, USA}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Houston","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Houston, USA}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Western Washington University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Western Washington University, USA}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Sao Paulo","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Sao Paulo, Brazil}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 7\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Universidade de Bras\u00edlia","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Universidade de Bras\u00edlia, Brazil}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS, Brazil}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 7\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 8\n8. Overall score 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Cairo University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Cairo University, Egypt}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 10\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Alexandria University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Alexandria University, Egypt}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Zagazig University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Zagazig University, Egypt}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"The University of Melbourne","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{The University of Melbourne, Australia}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"The University of Adelaide","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{The University of Adelaide, Australia}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Queensland University of Technology (QUT)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Queensland University of Technology (QUT), Australia}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Technical University of Munich","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Technical University of Munich, Germany}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Potsdam","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Potsdam, Germany}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"Leibniz University Hannover","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Leibniz University Hannover, Germany}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 7\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of Cape Town","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Cape Town, South Africa}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of South Africa","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of South Africa, South Africa}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"CP_paper_74","field":"Quantitative Finance","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","input_length":7973,"university_association":"University of the Free State","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of the Free State, South Africa}\n\\title{Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning}\n\\maketitle\n\n\\begin{abstract}\nThis paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model\nwith a non-martingale asset price process having infinite active jumps.\nIn our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai \\cite{AI}.\nMoreover, the BNS model includes many variables, which makes the deep learning accuracy worse.\nTherefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically. \\\\\n{\\bf Keywords:} Barndorff-Nielsen and Shephard model, Stochastic volatility model, Supervised deep learning, Monte Carlo simulation, Black-Scholes formula\n\\end{abstract}\n\n\\section{Introduction}\\setcounter{equation}{0}\nWe propose a supervised deep learning scheme to compute call option prices for\nthe non-martingale Barndorff-Nielsen and Shephard (BNS) model with infinite active jumps.\nSome numerical experiments will also be conducted to confirm the accuracy of our deep learning scheme.\n\nThe BNS model is a non-Gaussian Ornstein-Uhlenbeck (OU)-type stochastic variability model that has attracted the attention of many researchers\nsince it was undertaken by Barndorff-Nielsen and Shephard \\cite{BNS1} and \\cite{BNS2}.\nNow, we give a mathematical description of the BNS model.\nThroughout this paper, we consider a financial market with maturity $T>0$, composed of one risky asset and one riskless asset with 0 interest rate.\nThe risky asset price at time $t\\in[0,T]$ is described by\n\\begin{equation}\\label{eq-BNS}\nS_t:=S_0\\exp\\l\\{\\int_0^t\\l(\\mu-\\frac{1}{2}\\sigma_s^2\\r)ds+\\int_0^t\\sigma_sdW_s+\\rho H_{\\lambda t}\\r\\},\n\\end{equation}\nwhere $S_0>0$, $\\rho\\leq0$, $\\mu\\in\\bbR$, $\\lambda>0$, $W=\\{W_t\\}_{0\\leq t\\leq T}$ is a one dimensional standard Brownian motion, and\n$H_\\lambda=\\{H_{\\lambda t}\\}_{0\\leq t\\leq T}$ is a subordinator without drift, that is, a driftless non-decreasing L\\'evy process.\nHere, $\\sigma=\\{\\sigma_t\\}_{0\\leq t\\leq T}$ is the volatility process defined as the square root of a solution $\\sigma^2=\\{\\sigma^2_t\\}_{0\\leq t\\leq T}$\nto the following stochastic differential equation:\n\\begin{equation}\\label{sigma-SDE}\nd\\sigma_t^2 = -\\lambda\\sigma_t^2dt+dH_{\\lambda t}, \\ \\ \\ \\sigma_0^2>0.\n\\end{equation}\nNow, let $N$ be the Poisson random measure of $H_\\lambda$, and define its compensated Poisson random measure as $\\tN(dt,dx):=N(dt,dx)-\\nu(dx)dt$,\nwhere $\\nu$ is the L\\'evy measure of $N$.\nThen, the risky asset price process $S=\\{S_t\\}_{0\\leq t\\leq T}$ is also given as a solution to the following stochastic differential equation:\n\\[\ndS_t = S_{t-}\\l(\\alpha dt+\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r),\n\\]\nwhere\n\\[\n\\alpha:=\\mu+\\int_0^\\infty(e^{\\rho x}-1)\\nu(dx).\n\\]\nFor more details on the BNS model, see Arai et al. \\cite{AIS-BNS}, Nicolato and Venardos \\cite{NV}, and Schoutens \\cite{Scho}.\n\nThe Carr-Madan method, a numerical method based on the fast Fourier transform, has been commonly used to compute option prices in the BNS model.\nHowever, it does not apply to the case where the discounted asset pricing process is not a martingale, that is, $\\alpha\\neq0$.\nIn the non-martingale case, we need to change the underlying probability measure, denoted by $\\bbP$,\ninto an equivalent martingale measure when we compute option prices.\nFrom the incompleteness of the BNS model, the uniqueness of equivalent martingale measures does not hold.\nHence, we need to select an appropriate one. This point will be discussed in more detail later.\nThe problem is that $H_\\lambda$ is no longer a L\\'evy process under any equivalent martingale measure.\nMore precisely, $H_\\lambda$ has neither independent nor stationary increments.\nThus, it is impossible to describe explicitly the characteristic function of $S$ under any equivalent martingale measure.\nAs a result, the Carr-Madan method is unavailable for such cases.\n\nOn the other hand, conducting a Monte Carlo simulation of the BNS model with finite active jumps is not difficult, even in the non-martingale case.\nHere, the BNS model is said to have finite active jumps if L\\'evy measure $\\nu$ is finite.\nA typical example of such a case is the so-called gamma-OU type, in which $\\nu$ is given by\n\\[\n\\nu(dz)=\\lambda abe^{-bz}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$. In this case, $\\nu$ is finite, and the jump parts in (\\ref{eq-BNS}) and (\\ref{sigma-SDE}) are given as a compound Poisson process.\nMeanwhile, there had not been computational methods for the non-martingale BNS model with infinite active jumps before \nArai and Imai \\cite{AI} developed Monte Carlo simulation methods for such a case.\nIn particular, \\cite{AI} treated the IG-OU type, a representative example with infinite active jumps in which $\\nu$ is given as\n\\[\n\\nu(dz)=\\frac{\\lambda a}{2\\sqrt{2\\pi}}z^{-\\frac{3}{2}}(1+b^2z)\\exp\\l\\{-\\frac{1}{2}b^2z\\r\\}dz, \\ \\ \\ z\\in(0,\\infty),\n\\]\nwhere $a>0$ and $b>0$.\nRemark that the method suggested by \\cite{AI} relies on the exact simulation method for $\\sigma^2$\nin the IG-OU case developed by Sabino and Petroni \\cite{SP} and the acceptance\/rejection (A\/R) scheme.\n\nAs mentioned above, a measure change needs to be considered.\n\\cite{AI} selected the minimal martingale measure (MMM) as a representative equivalent martingale measure.\nThe MMM is an equivalent martingale measure appearing in local risk-minimization, which is a well-known optimal hedging strategy for incomplete markets.\nHere, an equivalent martingale measure $\\bbP^*$ is called the MMM if any square integrable $\\bbP$-martingale orthogonal to $M$ is also a $\\bbP^*$-martingale,\nwhere $M=\\{M_t\\}_{0\\leq t\\leq T}$ is the martingale part of $S$, that is, $M$ is given as\n\\[\ndM_t=S_{t-}\\l(\\sigma_t dW_t + \\int_0^\\infty(e^{\\rho x}-1)\\tN(dt,dx)\\r), \\ \\ \\ M_0=0.\n\\]\nUnder Assumption \\ref{ass} below, the Radon-Nikodym density of $\\bbP^*$ is given as\n\\begin{align*}\n\\frac{d\\bbP^*}{d\\bbP} &= \\exp\\bigg\\{-\\int_0^Tu_tdW_t-\\frac{1}{2}\\int_0^Tu_t^2dt+\\int_0^T\\int_0^\\infty\\log(1-\\theta_{t,x})\\tN(dt,dx) \\\\\n                      &  \\hspace{5mm}+\\int_0^T\\int_0^\\infty(\\log(1-\\theta_{t,x})+\\theta_{t,x})\\nu(dx)dt\\bigg\\},\n\\end{align*}\nwhere\n\\[\nu_t:=\\frac{\\alpha\\sigma_t}{\\sigma_t^2+C^\\rho}, \\ \\ \\ \\theta_{t,x}:=\\frac{\\alpha(e^{\\rho x}-1)}{\\sigma_t^2+C^\\rho},\n\\]\nand\n\\begin{equation}\\label{eq-C2}\nC^\\rho:=\\int_0^\\infty(e^{\\rho x}-1)^2\\nu(dx)=2\\rho\\lambda a\\l(\\frac{1}{\\sqrt{b^2-4\\rho}}-\\frac{1}{\\sqrt{b^2-2\\rho}}\\r).\n\\end{equation}\n\n\\begin{ass}\\label{ass}\nThroughout this paper, we assume that\n\\[\n\\frac{b^2}{2}>2\\l(\\frac{1-e^{-\\lambda T}}{\\lambda}\\vee|\\rho|\\r) \\ \\ \\ \\mbox{and} \\ \\ \\ \\frac{\\alpha}{e^{-\\lambda T}\\sigma^2_0+C^\\rho}>-1.\n\\]\n\\end{ass}\n\n\\noindent\nRoughly speaking, Assumption \\ref{ass} ensures that the MMM is well-defined as a probability measure.\nFor more details on Assumption \\ref{ass}, see \\cite{AI}.\nRemark that the European call option at time 0 with a strike price of $K>0$ and maturity of $T>0$ is given by\n\\[\n\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\n\nIn this paper, we develop a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM.\nTo this end, we generate teaching data using the Monte Carlo method developed in \\cite{AI}.\nThe asset price process at time 0 in the non-martingale IG-OU type BNS model includes seven variables:\n$S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, and $\\sigma_0^2$.\nWe need two more variables to compute the call option price: the strike price $K$ and the maturity $T$.\nThe training samples in deep learning will be generated by using quasi-random numbers.\nIn other words, the number of input data is equivalent to the dimension of the quasi-random numbers.\nFrom this point of view, the number of variables in the BNS model is too large to develop a deep learning scheme.\nTherefore, we restrict the range of each variable to around the calibrated value implemented in \\cite{NV}.\nNevertheless, only restricting the range of variables does not improve the accuracy of the deep learning scheme. Therefore, we have added another idea.\nFixing the values of the seven variables in the BNS model and the maturity $T$ and regarding the option prices as a function of the strike price $K$,\nwe can expect that their behavior is similar to the Black-Scholes formula.\nThus, substituting $\\sigma^2_0$, $K$, and $T$ into the Black-Scholes formula, we add its value to the input data.\nThis is a significant feature of our deep learning scheme, dramatically improving its accuracy.\n\nAlthough it is possible to perform option price computation using the Monte Carlo method alone,\nthe trained deep learning model enables us to speed up by approximately 100 times.\nIt takes about 4 seconds to implement one simulation with a regular laptop.\nHowever, the option price computation by the trained deep learning model is instantaneous.\nThis difference in computation time is very significant when many option prices must be computed repeatedly,\nsuch as when performing calibration or computing volatility surface.\nAs a preceding study, we present Arai \\cite{A}, which developed an unsupervised deep learning scheme for the BNS model.\nIn \\cite{A}, the loss function in the neural network was defined by making use of the fact that option prices satisfy\na partial-integral differential equation.\nHowever, the accuracy was insufficient for the non-martingale case with infinite active jumps.\n\nThe rest of this paper is organized as follows: We present the specification of our deep learning scheme in next section.\nSection 3 is devoted to introducing the results of numerical experiments, and this paper concludes in Section 4.\n\n\\section{Deep learning specification}\\setcounter{equation}{0}\nThe objective is to develop a supervised deep learning scheme for call option prices for the IG-OU type BNS model under the MMM.\nRecall that a call option price at time 0 is given as a function of 9 variables: $S_0$, $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$.\nWe denote it by the function $F$, that is,\n\\[\nF(S_0,\\alpha,\\rho,\\lambda,a,b,\\sigma_0^2,K,T):=\\bbE_{\\bbP^*}[(S_T-K)^+].\n\\]\nIn addition, the function $F_{MC}$ represents the computational result of the Monte Carlo method developed by \\cite{AI}.\nIn this section, we introduce the neural network's structure and the results of numerical experiments.\nRemark that all numerical computations in this paper are performed in MATLAB.\n\n\\subsection{Data generating}\nBased on the calibration result in \\cite{NV}, displayed in Table \\ref{tab1}, we generate 100,000($=N$) samples first.\n\n\\begin{table}[h]\n\\begin{center}\\caption{Calibration result in \\cite{NV}}\\label{tab1}\n\\begin{tabular}{cccccc}\\hline \\vspace{-3.5mm} \\\\\n$S_0$  & $\\rho$  & $\\lambda$ & $a$    & $b$   & $\\sigma_0^2$ \\\\ \\hline \\hline\n468.40 & -4.0739 & 2.4958    & 0.0872 & 11.98 & 0.0041       \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nEach value in the second column of Table \\ref{tab1} is denoted by adding a tilde, e.g., $\\wtrho=-$4.0739.\nGenerate an 8-dimensional Sobol sequence with a length of $N$, and denote it by $(\\bfx_1,\\dots,\\bfx_N)$.\nHence, $\\bfx_n$ is in $[0,1]^8$ for each $n=1,\\dots,N$; we can express it as $\\bfx_n=(x^1_n,\\dots,x^8_n)$.\nFor $n=1,\\dots,N$, $x^1_n,\\dots,x^8_n$ are corresponding to variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, $K$, and $T$, respectively.\nHere, we fix the value of $S_0$ to 468.40, denoted by $\\wtS$.\n\nNow, we need to set a range for each variable and transform the values of $x^1_n,\\dots,x^8_n$ accordingly.\nFirst, we set the range of $\\rho$ to $\\d{\\l[\\frac{1}{2}\\wtrho,\\frac{3}{2}\\wtrho\\r]}$, and those of $\\lambda$, $a$, and $\\sigma_0^2$ are also set similarly.\nHence, we transform $x^2_n$, $x^3_n$, $x^4_n$ and $x^6_n$ as follows: \n\\[\n\\rho_n         := \\frac{1}{2}\\wtrho    +\\wtrho x^2_n,    \\ \\ \\ \n\\lambda_n      := \\frac{1}{2}\\wtlambda +\\wtlambda x^3_n, \\ \\ \\ \na_n            := \\frac{1}{2}\\wta      +\\wta x^4_n,      \\ \\mbox{ and } \\ \n(\\sigma^2_0)_n := \\frac{1}{2}\\wtsigma  +\\wtsigma x^6_n.\n\\]\nMoreover, the 7th and 8th components are corresponding to $K$ and $T$, respectively.\nWe set then the range of $K$ to $\\d{\\l[\\frac{1}{2}\\wtS,\\frac{3}{2}\\wtS\\r]}$, that is, we transform $x^7_n$ into $\\d{K_n:=\\frac{1}{2}\\wtS+\\wtS x^7_n}$.\nOn the other hand, we set the extent of $T$ from 0.01 to 1 and convert $x^8_n$ as $T_n:=0.01+0.99x^8_n$.\nAs for the variables $\\alpha$ and $b$, we need to set the lower bounds of their ranges to meet Assumption \\ref{ass}.\nTo this end, for $n=1,\\dots,N$, we define $\\ulb_n$ as\n\\[\n\\ulb_n:=1.05\\times2\\sqrt{\\frac{1-e^{-\\lambda_nT_n}}{\\lambda_n}\\vee|\\rho_n|},\n\\]\nwhich is the lower bound of $b$ in the first condition of Assumption \\ref{ass} multiplied by 1.05.\nWe transform then $x^5_n$ so that the range of $b$ is $\\l[\\ulb_n,\\frac{3}{2}\\wtb\\r]$ as follows:\n\\[\nb_n := \\ulb_n +\\l(\\frac{3}{2}\\wtb-\\ulb_n\\r)x^5_n.\n\\]\nNote that $\\ulb_n<\\frac{3}{2}\\wtb$ holds at any time.\nNext, we define $\\ulalpha_n$, the lower bound of $\\alpha$. To this end, we first define\n\\[\n\\whalpha_n := -\\exp\\{-\\lambda_nT_n\\}(\\sigma^2_0)_n-C^\\rho_n,\n\\]\nwhere $C^\\rho_n$ is defined by replacing $a$, $b$, $\\rho$, and $\\lambda$ in (\\ref{eq-C2}) with $a_n$, $b_n$, $\\rho_n$, and $\\lambda_n$, respectively.\nWe define then $\\ulalpha_n$ as\n\\[\n\\ulalpha_n := 0.95\\whalpha_n{\\bf 1}_{\\{\\whalpha_n\\geq-3\/2\\}}-\\frac{3}{2}{\\bf 1}_{\\{\\whalpha_n<-3\/2\\}}.\n\\]\nand transform $x^1_n$ so that the range of $\\alpha$ is $\\d{\\l[\\ulalpha_n,\\frac{3}{2}\\r]}$ as follows:\n\\[\n\\alpha_n := \\ulalpha_n+\\l(\\frac{3}{2}-\\ulalpha_n\\r)x^1_n.\n\\]\nHere, the absolute value of $\\alpha$ is set so that it is not greater than $3\/2$.\n\nFor $n=1,\\dots,N$, we implement the Monte Carlo method developed by \\cite{AI}\nfor the $n$th sample $(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n)$, and denote by $MC_n$ its result, that is,\n\\[\nMC_n := F_{MC}(\\wtS,\\alpha_n,\\rho_n,\\lambda_n,a_n,b_n,(\\sigma_0^2)_n,K_n,T_n).\n\\]\nWe shall use $MC_n$ as teaching data in our scheme.\nWhen implementing the Monte Carlo method, the number of paths and time step interval are set to 1,000 and 0.01, respectively.\nNote that implementing the Monte Carlo method for 100,000 samples is very time-consuming.\nIt took 178 hours approximately. Therefore, it is very difficult to increase the number of samples.\n\nDue to the large number of variables, it is not easy to construct a deep learning scheme with high accuracy.\nOn the other hand, we can expect that option prices for the BNS model and the Black-Scholes model exhibit similar behavior.\nTherefore, we calculate the Black-Scholes formula with $(\\sigma^2_0)_n$, $K_n$, and $T_n$ for $n=1,\\dots,N$, and add them to the dataset.\nHere, the call option price at time 0 for the Black-Scholes model with volatility $\\sigma$, strike price $K$, and maturity $T$ is given as\n\\[\nBS(\\sigma,K,T) := S_0\\Phi(d_+)-K\\Phi(d_-)K,\n\\]\nwhere the interest rate is 0, $S_0$ is the current asset price, $\\Phi$ denotes the standard normal cumulative distribution function, that is,\n\\[\n\\Phi(x) := \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{y^2}{2}}dy,\n\\]\nand we denote\n\\[\nd_{\\pm} := \\frac{\\log S_0-\\log K}{\\sigma \\sqrt{T}}\\pm\\frac{\\sigma\\sqrt{T}}{2}.\n\\]\nFor $n=1,\\dots,N$, we denote $BS_n := BS(\\sqrt{(\\sigma^2_0)_n},K_n,T_n)$.\n\nIn summary, we generate a dataset of 100,000 samples from an 8-dimensional Sobol sequence.\nEach sample is composed of 10 variables: $\\alpha_n$, $\\rho_n$, $\\lambda_n$, $a_n$, $b_n$, $(\\sigma_0^2)_n$, $K_n$, $T_n$, $BS_n$ and $MC_n$.\nWe randomly divide our dataset into a training dataset of 98,000 samples, a validation dataset of 1,000 samples, and a test dataset of 1,000 samples.\n\n\\subsection{Neural network and learning}\nFor the 98,000 training samples, the first nine variables of each sample are used as input data, and the last variable, $MC_n$, will be used as teaching data.\nRemark that each input data is rescaled so that the range of each variable is in the interval $[-1,1]$, e.g., $\\rho_n$ is rescaled as\n\\[\n\\frac{2}{\\rho_{\\max}-\\rho_{\\min}}\\l(\\rho_n-\\frac{\\rho_{\\max}+\\rho_{\\min}}{2}\\r),\n\\]\nwhere $\\rho_{\\max}$ and $\\rho_{\\min}$ are the maximum and minimum values of $\\rho_n$ among all training samples.\n\nWe construct a neural network composed of six hidden layers with 200 units.\nAs activation functions, we place the ReLU function in the first five layers and the softplus function in the last,\nwhere the ReLU and softplus functions are defined as $\\Relu(x) := x{\\bf 1}_{\\{x\\geq0\\}}$ and $\\softplus(x) := \\log(1+e^x)$, respectively.\n\nWe divide the training dataset into batches of size 128 and set the number of epochs to 4,000.\nThat is, the number of batches is 766, and the size of the 766th batch is 80.\nAdam is used as the gradient descent algorithm. The initial learning rate is 0.006, decreasing by 0.5\\Note that, for each batch, we calculate the loss value and execute the learning.\nHere, we use as the loss function the half mean squared error (MSE) defined as follows:\n\\begin{align*}\n\\MSE := \\frac{1}{2m}\\sum^m_{i=1}(X_i-MC_i)^2,\n\\end{align*}\nwhere $m$ is the bathch size, $X_i$ is the output from the neural network for the $i$th sample in the batch,\nand $MC_i$ is the teaching data of the $i$th sample.\nAs a result, the learning is executed 3,064,000 times in total.\n\nUsing the validation dataset, we additionally calculate the MSE every 50 epochs. Here, we need to take 1,000, the size of the validation dataset, as $m$.\nSince the total number of epochs is 4,000, the MSE of this type is calculated 80 times.\nThe output when the MSE attains its minimum will be used as the final output, which we call the trained deep learning model.\nIn other words, the output at the end of the 4,000 epochs is not necessarily used as the trained deep learning model.\nIn addition, to evaluate the whole of our scheme performance, we calculate the root mean squared error (RMSE), defined as follows, using the test dataset:\n\\begin{align*}\n\\RMSE := \\sqrt{\\frac{1}{m}\\sum^m_i(X_i-MC_i)^2},\n\\end{align*}\nwhere $m=$1,000 is the size of the test dataset, $X_i$ is the result from the trained deep learning model for the $i$th sample in the test dataset,\nand $MC_i$ is the teaching data of the $i$th sample.\nConstructing a neural network described above and executing the learning, we found the value of the RMSE to be 0.7578, which is sufficiently small.\n\n\\section{Numerical results}\\setcounter{equation}{0}\nWe compute call option prices of the IG-OU type BNS model under the MMM using both the trained deep learning model\nand the Monte Carlo method developed in \\cite{AI} and compare the results to confirm that the accuracy of our deep learning scheme is sufficient.\n\nTo this end, we generate 7-dimensional uniform random numbers on $[0,1]$.\nEach element of a random number is corresponding to the variables $\\alpha$, $\\rho$, $\\lambda$, $a$, $b$, $\\sigma_0^2$, and $T$, respectively.\nWe transform the values of random numbers in the same way as in subsection 2.1, and fix the value of $S_0$ at 468.40.\nAs for the strike price $K$, we move its value from $\\d{\\frac{1}{2}S_0=234.2}$ to $\\d{\\frac{3}{2}S_0=702.6}$ at steps of $\\d{\\frac{1}{100}S_0}$.\n\n\\begin{table}[htb]\n\\begin{center}\\caption{Variable sets used in experiments}\\label{tab2}\n\\begin{tabular}{cccccccc}\\hline \\vspace{-3.5mm} \\\\\nVariable set & \\mcc{$\\alpha$} & \\mcc{$\\rho$} & \\mcc{$\\lambda$} & \\mcc{$a$} & \\mcc{$b$} & \\mcc{$\\sigma_0^2$} & \\mcc{$T$} \\\\ \\hline \\hline\n(a)          & 0.49867        & -4.71919     & 1.41919         & 0.10997   & 16.96651  & 0.00386            & 0.31475   \\\\ \\hline\n(b)          & 0.23797        & -4.69071     & 3.23799         & 0.10078   & 15.63037  & 0.00523            & 0.97049   \\\\ \\hline\n(c)          & 1.17240        & -6.72677     & 3.16246         & 0.10582   & 15.79102  & 0.00581            & 0.65818   \\\\ \\hline\n\\end{tabular}\\end{center}\\end{table}\n\n\\noindent\nWe introduce the results for the three sets of variables displayed in Table \\ref{tab2}.\nRemark that experiments for many other sets show similar results.\nFigure \\ref{fig1} displays the call option prices against strike prices.\nNote that panels (a), (b), and (c) correspond to the variable sets (a), (b), and (c) in Table \\ref{tab2}, respectively.\nIn each panel in Figure \\ref{fig1}, the blue, red, and black curves draw option prices derived from the Monte Carlo method,\nfrom the trained deep learning model, and the payoff function $(S_0-K)^+$, respectively.\nIn all panels, the blue and red curves overlap and are indistinguishable.\nFigure \\ref{fig2} shows the differences between the option prices using the trained deep learning model and the Monte Carlo method, that is,\nwe define ``difference\" as\n\\[\n\\mbox{difference}:=\\mbox{price by the trained deep learning model}-\\mbox{price by the Monte Carlo method}.\n\\]\nIn addition, Figure \\ref{fig3} displays the relative errors against strike prices,\nwhere the relative error is defined as the difference in Figure \\ref{fig2} divided by the option price derived from the Monte Carlo method, i.e.,\n\\[\n\\mbox{relative error}:=\\frac{\\mbox{difference}}{\\mbox{price by the Monte Carlo method}\\vee (K\/100)}.\n\\]\nNote that we set the lower bound of the denominator to be $K\/100$ since the relative error becomes too large when the denominator is small.\nIn Panel (a) and (b) of Figure \\ref{fig3}, the relative errors near at-the-money reach around 0.06 but are less than 0.01 otherwise.\nAs for Panel (c), the relative errors fluctuate in out-of-the-money but are still included in the interval $[-0.05,0.04]$.\nOverall, we can say that the trained deep learning performs well.\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midFig.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longFig.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Call option prices from the Monte Carlo method (blue), the trained deep learning model (red),\nand the payoff function $(S_0-K)^+$ (black) vs. strike prices.\nEach panel is corresponding to the variable sets (a), (b), and (c), respectively.}\\label{fig1}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midDiff.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longDiff.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Differences between option prices from the trained deep learning model and the Monte Carlo method, vs. strike prices.}\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[H]\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{shortRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{midRE.pdf}\\subcaption{}\\end{minipage}\n    \\begin{center}\\begin{minipage}{0.5\\hsize}\\includegraphics[width=70mm]{longRE.pdf}\\subcaption{}\\end{minipage}\\end{center}\n\\caption{Relative errors vs. strike prices.}\\label{fig3}\n\\end{figure}\n\n\\section{Concluding remarks}\\setcounter{equation}{0}\nWe have developed a supervised deep learning scheme to compute call option prices for the IG-OU type BNS model under the MMM,\na representative case of the non-martingale BNS model with infinite active jumps.\nIn particular, we generate teaching data by using the Monte Carlo method developed by \\cite{AI}.\nUsing the Black-Scholes formula, we created yet another input variable, which improved the accuracy of our deep learning scheme\nas shown by the results of numerical experiments.\n\nOn the other hand, the generation of teaching data by the Monte Carlo method is time-consuming, so the number of samples in the dataset cannot be increased.\nIn addition, the BNS model has many variables, making it difficult to improve the deep learning accuracy.\nTherefore, we had to restrict the range of variables in this paper.\nDeveloping a deep learning scheme valid with broader ranges of variables is a future challenge.\n\n\\begin{center}{\\bf Acknowledgments}\\end{center}\nTakuji Arai and Yuto Imai gratefully acknowledge the financial support of the MEXT Grant-in-Aid for\nScientific Research (C) No.22K03419 and Early-Career Scientists No.21K13327, respectively.\n\n\\begin{thebibliography}{9999}\n\\bibitem{A} Arai, T. (2023). Deep learning-based option pricing for Barndorff-Nielsen and Shephard model.\nInternational Journal of Financial Engineering, 2350015.\n\\bibitem{AI} Arai, T. and Imai, Y. (2024). Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure,\nMathematics and Computers in Simulation, 218, pp.223-234.\n\\bibitem{AIS-BNS} Arai, T., Imai, Y. and Suzuki, R. (2017). Local risk-minimization for Barndorff-Nielsen and Shephard models,\nFinance \\& Stochastics, 21 , pp.551-592.\n\\bibitem{BNS1} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nModelling by L\\'evy processes for financial econometrics. In L\\'evy processes (pp.283-318). Birkh\\\"auser, Boston, MA.\n\\bibitem{BNS2} Barndorff-Nielsen, O. E., \\& Shephard, N. (2001).\nNon-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial economics.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), pp.167-241.\n\\bibitem{DOP} Di Nunno, G., {\\O}ksendal, B., \\& Proske, F. (2008).\nMalliavin calculus for L\\'evy processes with applications to finance. Berlin, Heidelberg: Springer.\n\\bibitem{G} Glasserman, P. (2004). Monte Carlo methods in financial engineering (Vol. 53, pp. xiv+-596). New York: Springer.\n\\bibitem{NV} Nicolato, E. and Venardos, E. (2003). Option pricing in stochastic volatility models of the Ornstein-\\\"Uhlenbeck type,\nMathematical Finance, 13 , pp.445-466.\n\\bibitem{QDZ} Qu, Y., Dassios, A., \\& Zhao, H. (2021). Exact simulation of Ornstein-Uhlenbeck tempered stable processes.\nJournal of Applied Probability, 58(2), pp.347-371.\n\\bibitem{SP} Sabino, P., \\& Petroni, N. C. (2022).\nFast simulation of tempered stable Ornstein-\\\"Uhlenbeck processes. Computational Statistics, 37(5), pp.2517-2551.\n\\bibitem{Scho} Schoutens, W. (2003). L\\'evy processes in finance: pricing financial derivatives, Wiley.\n\\end{thebibliography}\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
